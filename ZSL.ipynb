{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88d3021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c074c490-b055-438f-a62d-05500d81ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "key = os.getenv(\"API_KEY\")\n",
    "client = OpenAI(api_key=key, base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c98315",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "ner_dict = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d011d776-3493-466c-a170-bc8fdf8b80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_label(value, ner_dict):\n",
    "    for k, v in ner_dict.items():\n",
    "        if v == value:\n",
    "            return k\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab4fc9-35b0-4e51-b029-8dc737ec432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_prompt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d85a07-81c4-4669-bdae-b4b6049b5261",
   "metadata": {},
   "source": [
    "### VANILLA METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12fa69-06dc-417e-91cc-57947430edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in dataset['train']['tokens']:\n",
    "    # prompt = f\"Given the entity label set: {list(ner_dict.keys())}.\\n \\\n",
    "    #     Based on the given entity label set, please recognize the named entities in the given text.\\n \\\n",
    "    #     Return only a list of tuples with each token and its label without explenation. Your output must be in the format: [('In','O'), ('America','I-LOC'), ('is','O'), ('cold','O'), ...]; nothing else\\n \\\n",
    "    #     Text: {\" \".join(sentence)}\"\n",
    "\n",
    "    prompt = f\"Given the entity label set: {list(ner_dict.keys())}.\\n \\\n",
    "    Based on the given entity label set, please recognize the named entities in the given text.\\n \\\n",
    "    Return only a list of tuples with each token and its label without explenation. Your output must be in the format: [('\\\"','O'),('In','O'), ('America','I-LOC'), ('is','O'), ('cold','O'), ...]; nothing else\\n \\\n",
    "    Text: {\" \".join(sentence)}\"\n",
    "\n",
    "    answer = send_prompt(prompt)\n",
    "    # print(answer)\n",
    "    with open(\"vanilla_v3.txt\", \"a\") as file:\n",
    "        file.write(f\"{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc776bb-d269-4f08-8096-72d04003243c",
   "metadata": {},
   "source": [
    "### ROLE PLAY METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704fbce-1aab-404c-bf2e-f91846b94f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in dataset['train']['tokens'][:100]:\n",
    "    prompt = f\"You are a linguist expert professor. \\\n",
    "        You have a PHD and a post-doc in name entity recognition and you have been working with this in the past 20 years.\\\n",
    "        You are the best in the world at this task. You are creating a new NER dataset and you are labeling some sentences tokens.\\\n",
    "        Based on the given entity label set: {list(ner_dict.keys())}, \\\n",
    "        you are going to recognize the named entities in the given sentence.\\n \\\n",
    "        You are going to return only a list of tuples with each token and its label without explenation. Your output must be in the format: [('In','O'), ('America','I-LOC'), ('is','O'), ('cold','O'), ...]; nothing else\\n \\\n",
    "        Sentence: {\" \".join(sentence)}\"\n",
    "\n",
    "    answer = send_prompt(prompt)\n",
    "    # print(answer)\n",
    "    with open(\"role_play.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6e41a-5021-40a4-9933-6694f1196f0d",
   "metadata": {},
   "source": [
    "### CHAIN OF THOUGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75889cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in dataset['train']['tokens'][:100]:\n",
    "    prompt = f\"Given the entity label set: {list(ner_dict.keys())}.\\n \\\n",
    "        Based on the given entity label set, please recognize the named entities in the given text and show your reasoning step by step for how you identify each entity.\\n \\\n",
    "        You must conclude your reasoning by returning a list of tuples with each token and its label. \\\n",
    "        At the left and right of the list insert a '$' symbol. For example $[('In','O'), ('America','I-LOC'), ('is','O'), ('cold','O'), ...]$\\n \\\n",
    "        You must return the list with the shown format only one time and no more. \\\n",
    "        Text: {\" \".join(sentence)}\"\n",
    "\n",
    "    answer = send_prompt(prompt)\n",
    "    # print(answer)\n",
    "    with open(\"COT.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcadd1db",
   "metadata": {},
   "source": [
    "### ROLE PLAY METHOD + CHAIN OF THOUGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in dataset['train']['tokens'][:100]:\n",
    "    prompt = f\"You are a linguist expert professor. \\\n",
    "        You have a PHD and a post-doc in name entity recognition and you have been working with this in the past 20 years.\\\n",
    "        You are the best in the world at this task. You are creating a new NER dataset and you are labeling some sentences tokens.\\\n",
    "        Based on the given entity label set: {list(ner_dict.keys())}, \\\n",
    "        you are going to recognize the named entities in the given sentence and show your reasoning step by step for how you identify each entity.\\n \\\n",
    "        You must conclude your reasoning by returning a list of tuples with each token and its label. \\\n",
    "        At the left and right of the list insert a '$' symbol. For example $[('\\\"','O'),('In','O'), ('America','I-LOC'), ('is','O'), ('cold','O'), ...]$\\n \\\n",
    "        You must return the list with the shown format only one time and not more. \\\n",
    "        The sentence is:   {\" \".join(sentence)}\"\n",
    "\n",
    "    answer = send_prompt(prompt)\n",
    "    print(prompt)\n",
    "    with open(\"role_playand_COT.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6e47d",
   "metadata": {},
   "source": [
    "### Divided inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34025b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"EU\"** – Likely an organization (European Union).  \n",
      "2. **\"rejects\"** – Verb (not an entity).  \n",
      "3. **\"German\"** – Likely a nationality or country-related adjective (MISC or NORP in NER).  \n",
      "4. **\"call\"** – Noun (not an entity).  \n",
      "5. **\"to\"** – Preposition (not an entity).  \n",
      "6. **\"boycott\"** – Verb (not an entity).  \n",
      "7. **\"British\"** – Likely a nationality or country-related adjective (MISC or NORP).  \n",
      "8. **\"lamb\"** – Noun (not an entity).  \n",
      "9. **\".\"** – Punctuation (not an entity).  \n",
      "\n",
      "This tokenization preserves meaningful entities (\"EU\", \"German\", \"British\") while separating non-entity words.  \n",
      "\n",
      "Would you like any adjustments (e.g., splitting contractions, handling hyphenated words differently)?\n",
      "Bot: [('EU', 'B-ORG'), ('rejects', 'O'), ('German', 'B-MISC'), ('call', 'O'), ('to', 'O'), ('boycott', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"Peter\", \"Blackburn\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Tokenization**: The sentence \"Peter Blackburn\" consists of two distinct words, which are treated as separate tokens.\n",
      "2. **NER Relevance**: \n",
      "   - \"Peter\" and \"Blackburn\" are likely parts of a person's full name (first and last name), which NER models typically label as `PER` (Person) or `B-PER`/`I-PER` (BILOU/BIOES tagging schemes).\n",
      "   - No punctuation or spaces are included as tokens (common practice in NER unless whitespace is explicitly needed).\n",
      "\n",
      "### Alternative (if further segmentation is needed):\n",
      "If your NER task requires subword tokenization (e.g., for transformer models like BERT), the output might look like:\n",
      "```python\n",
      "tokens = [\"Peter\", \"Black\", \"##burn\"]  # Example for WordPiece tokenization\n",
      "```\n",
      "But for standard NER, the first approach suffices. Let me know if you need a specific tokenization strategy!\n",
      "Bot: [('Peter', 'B-PER'), ('Blackburn', 'I-PER')]\n",
      "Bot: Here is the tokenized version of the input sentence `BRUSSELS 1996-08-22` for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\"BRUSSELS\", \"1996-08-22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"BRUSSELS\"** – A single token representing a location (likely a city).  \n",
      "2. **\"1996-08-22\"** – A single token representing a date in ISO format (YYYY-MM-DD).  \n",
      "\n",
      "For NER, it's often useful to keep dates and named entities (like cities) as single tokens, as they represent distinct semantic units. If further tokenization is needed (e.g., splitting the date into `[\"1996\", \"08\", \"22\"]`), let me know!\n",
      "Bot: [('BRUSSELS', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"The\", \"European\", \"Commission\", \"said\", \"on\", \"Thursday\", \"it\", \n",
      "    \"disagreed\", \"with\", \"German\", \"advice\", \"to\", \"consumers\", \"to\", \n",
      "    \"shun\", \"British\", \"lamb\", \"until\", \"scientists\", \"determine\", \n",
      "    \"whether\", \"mad\", \"cow\", \"disease\", \"can\", \"be\", \"transmitted\", \n",
      "    \"to\", \"sheep\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like `.`) is treated as a separate token, as it can be relevant in NER tasks.\n",
      "2. Words are split based on whitespace and punctuation boundaries.\n",
      "3. Multi-word named entities (e.g., \"European Commission\", \"German advice\", \"British lamb\", \"mad cow disease\") will need to be reconstructed during NER based on the model's predictions.\n",
      "Bot: [('The', 'O'), ('European', 'B-ORG'), ('Commission', 'I-ORG'), ('said', 'O'), ('on', 'O'), ('Thursday', 'O'), ('it', 'O'), ('disagreed', 'O'), ('with', 'O'), ('German', 'B-MISC'), ('advice', 'O'), ('to', 'O'), ('consumers', 'O'), ('to', 'O'), ('shun', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('until', 'O'), ('scientists', 'O'), ('determine', 'O'), ('whether', 'O'), ('mad', 'B-MISC'), ('cow', 'I-MISC'), ('disease', 'I-MISC'), ('can', 'O'), ('be', 'O'), ('transmitted', 'O'), ('to', 'O'), ('sheep', 'O'), ('.', 'O')]\n",
      "Bot: Here is the tokenized version of the given sentence, broken down into tokens suitable for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Germany\", \"'s\", \"representative\", \"to\", \"the\", \"European\", \"Union\", \"'s\", \n",
      "    \"veterinary\", \"committee\", \"Werner\", \"Zwingmann\", \"said\", \"on\", \"Wednesday\", \n",
      "    \"consumers\", \"should\", \"buy\", \"sheepmeat\", \"from\", \"countries\", \"other\", \n",
      "    \"than\", \"Britain\", \"until\", \"the\", \"scientific\", \"advice\", \"was\", \"clearer\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **Punctuation Handling**: Punctuation like `'s` and `.` is treated as separate tokens.\n",
      "2. **Named Entities**: Multi-word entities like \"European Union\" and \"Werner Zwingmann\" are split into individual tokens but may need to be combined during NER labeling (e.g., `B-ORG`, `I-ORG` for \"European Union\").\n",
      "3. **Hyphenated/Compound Words**: \"sheepmeat\" is treated as a single token.\n",
      "4. **NER Relevance**: Tokens like \"Germany,\" \"European Union,\" \"Werner Zwingmann,\" \"Wednesday,\" and \"Britain\" are likely candidates for named entity labels (e.g., `GPE`, `ORG`, `PERSON`, `DATE`, `GPE`).\n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., splitting contractions like \"'s\" into separate tokens)!\n",
      "Bot: [('Germany', 'B-LOC'), (\"'s\", 'O'), ('representative', 'O'), ('to', 'O'), ('the', 'O'), ('European', 'B-ORG'), ('Union', 'I-ORG'), (\"'s\", 'O'), ('veterinary', 'O'), ('committee', 'O'), ('Werner', 'B-PER'), ('Zwingmann', 'I-PER'), ('said', 'O'), ('on', 'O'), ('Wednesday', 'B-MISC'), ('consumers', 'O'), ('should', 'O'), ('buy', 'O'), ('sheepmeat', 'O'), ('from', 'O'), ('countries', 'O'), ('other', 'O'), ('than', 'O'), ('Britain', 'B-LOC'), ('until', 'O'), ('the', 'O'), ('scientific', 'O'), ('advice', 'O'), ('was', 'O'), ('clearer', 'O'), ('.', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens as a list:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"We\", \n",
      "    \"do\", \n",
      "    \"n't\", \n",
      "    \"support\", \n",
      "    \"any\", \n",
      "    \"such\", \n",
      "    \"recommendation\", \n",
      "    \"because\", \n",
      "    \"we\", \n",
      "    \"do\", \n",
      "    \"n't\", \n",
      "    \"see\", \n",
      "    \"any\", \n",
      "    \"grounds\", \n",
      "    \"for\", \n",
      "    \"it\", \n",
      "    \",\", \n",
      "    \"\\\"\", \n",
      "    \"the\", \n",
      "    \"Commission\", \n",
      "    \"'s\", \n",
      "    \"chief\", \n",
      "    \"spokesman\", \n",
      "    \"Nikolaus\", \n",
      "    \"van\", \n",
      "    \"der\", \n",
      "    \"Pas\", \n",
      "    \"told\", \n",
      "    \"a\", \n",
      "    \"news\", \n",
      "    \"briefing\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. **Contractions**: \"do n't\" is split into `[\"do\", \"n't\"]` (common in NLP tasks).  \n",
      "2. **Punctuation**: Commas (`\",\"`), quotes (`\"\\\"\"`), and periods (`\".\"`) are treated as separate tokens.  \n",
      "3. **Possessive**: `\"'s\"` is separated from \"Commission\" (`[\"Commission\", \"'s\"]`).  \n",
      "4. **Named Entity**: \"Nikolaus van der Pas\" is split into individual tokens to allow the NER model to tag each part (e.g., `[\"Nikolaus\", \"van\", \"der\", \"Pas\"]` could be labeled as a person).  \n",
      "\n",
      "This tokenization is suitable for NER, where granularity helps in identifying entity boundaries (e.g., \"Commission\" as an organization, \"Nikolaus van der Pas\" as a person).\n",
      "Bot: [('We', 'O'), ('do', 'O'), ('n\\'t', 'O'), ('support', 'O'), ('any', 'O'), ('such', 'O'), ('recommendation', 'O'), ('because', 'O'), ('we', 'O'), ('do', 'O'), ('n\\'t', 'O'), ('see', 'O'), ('any', 'O'), ('grounds', 'O'), ('for', 'O'), ('it', 'O'), (',', 'O'), ('\"', 'O'), ('the', 'O'), ('Commission', 'B-ORG'), ('\\'s', 'O'), ('chief', 'O'), ('spokesman', 'O'), ('Nikolaus', 'B-PER'), ('van', 'I-PER'), ('der', 'I-PER'), ('Pas', 'I-PER'), ('told', 'O'), ('a', 'O'), ('news', 'O'), ('briefing', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"He\", \"said\", \"further\", \"scientific\", \"study\", \"was\", \"required\", \n",
      "    \"and\", \"if\", \"it\", \"was\", \"found\", \"that\", \"action\", \"was\", \n",
      "    \"needed\", \"it\", \"should\", \"be\", \"taken\", \"by\", \"the\", \n",
      "    \"European\", \"Union\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like `.`) is treated as a separate token, as it may be relevant for NER (e.g., sentence boundaries).\n",
      "2. \"European Union\" is split into two tokens (\"European\", \"Union\") but may be labeled as a single entity in NER (e.g., `ORG` or `GPE`).\n",
      "3. Contractions or hyphenated words are not present here, but if they were, they would typically be split (e.g., \"can't\" → [\"can\", \"n't\"]).\n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., keeping \"European Union\" as a single token).\n",
      "Bot: [('He', 'O'), ('said', 'O'), ('further', 'O'), ('scientific', 'O'), ('study', 'O'), ('was', 'O'), ('required', 'O'), ('and', 'O'), ('if', 'O'), ('it', 'O'), ('was', 'O'), ('found', 'O'), ('that', 'O'), ('action', 'O'), ('was', 'O'), ('needed', 'O'), ('it', 'O'), ('should', 'O'), ('be', 'O'), ('taken', 'O'), ('by', 'O'), ('the', 'O'), ('European', 'B-ORG'), ('Union', 'I-ORG'), ('.', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens (words and punctuation) for named entity recognition (NER):\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"He\", \"said\", \"a\", \"proposal\", \"last\", \"month\", \"by\", \"EU\", \"Farm\", \n",
      "    \"Commissioner\", \"Franz\", \"Fischler\", \"to\", \"ban\", \"sheep\", \"brains\", \n",
      "    \",\", \"spleens\", \"and\", \"spinal\", \"cords\", \"from\", \"the\", \"human\", \n",
      "    \"and\", \"animal\", \"food\", \"chains\", \"was\", \"a\", \"highly\", \"specific\", \n",
      "    \"and\", \"precautionary\", \"move\", \"to\", \"protect\", \"human\", \"health\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (`,`, `.`) is treated as separate tokens.  \n",
      "2. Multi-word entities like \"EU Farm Commissioner\" and \"Franz Fischler\" are split into individual tokens for NER labeling (e.g., `[\"EU\", \"Farm\", \"Commissioner\"]` and `[\"Franz\", \"Fischler\"]`).  \n",
      "3. Hyphenated terms or contractions would also be split if present (though none appear here).  \n",
      "\n",
      "This format is compatible with standard NER frameworks like spaCy or Hugging Face's tokenizers. Let me know if you'd like further adjustments!\n",
      "Bot: [('He', 'O'), ('said', 'O'), ('a', 'O'), ('proposal', 'O'), ('last', 'O'), ('month', 'O'), ('by', 'O'), ('EU', 'B-ORG'), ('Farm', 'I-ORG'), ('Commissioner', 'I-ORG'), ('Franz', 'B-PER'), ('Fischler', 'I-PER'), ('to', 'O'), ('ban', 'O'), ('sheep', 'O'), ('brains', 'O'), (',', 'O'), ('spleens', 'O'), ('and', 'O'), ('spinal', 'O'), ('cords', 'O'), ('from', 'O'), ('the', 'O'), ('human', 'O'), ('and', 'O'), ('animal', 'O'), ('food', 'O'), ('chains', 'O'), ('was', 'O'), ('a', 'O'), ('highly', 'O'), ('specific', 'O'), ('and', 'O'), ('precautionary', 'O'), ('move', 'O'), ('to', 'O'), ('protect', 'O'), ('human', 'O'), ('health', 'O'), ('.', 'O')]\n",
      "Bot: Here is the tokenized version of the sentence, split into meaningful units for named entity recognition (NER):  \n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Fischler\", \n",
      "    \"proposed\", \n",
      "    \"EU-wide\", \n",
      "    \"measures\", \n",
      "    \"after\", \n",
      "    \"reports\", \n",
      "    \"from\", \n",
      "    \"Britain\", \n",
      "    \"and\", \n",
      "    \"France\", \n",
      "    \"that\", \n",
      "    \"under\", \n",
      "    \"laboratory\", \n",
      "    \"conditions\", \n",
      "    \"sheep\", \n",
      "    \"could\", \n",
      "    \"contract\", \n",
      "    \"Bovine\", \n",
      "    \"Spongiform\", \n",
      "    \"Encephalopathy\", \n",
      "    \"(\", \n",
      "    \"BSE\", \n",
      "    \")\", \n",
      "    \"--\", \n",
      "    \"mad\", \n",
      "    \"cow\", \n",
      "    \"disease\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization for NER:  \n",
      "1. **Multi-word entities** like \"Bovine Spongiform Encephalopathy\" and \"mad cow disease\" are split into individual tokens but should be reassembled during NER labeling (e.g., as a single disease entity).  \n",
      "2. **Punctuation** (e.g., \"(\", \")\", \"--\", \".\") is separated to avoid interference with entity boundaries.  \n",
      "3. **Hyphenated terms** (\"EU-wide\") are kept as single tokens if they represent a unified concept.  \n",
      "4. **Named entities** (\"Fischler\", \"Britain\", \"France\", \"BSE\") are preserved as standalone tokens for proper tagging.  \n",
      "\n",
      "Let me know if you'd prefer a different splitting strategy!\n",
      "Bot: [('Fischler', 'B-PER'), ('proposed', 'O'), ('EU-wide', 'B-MISC'), ('measures', 'O'), ('after', 'O'), ('reports', 'O'), ('from', 'O'), ('Britain', 'B-LOC'), ('and', 'O'), ('France', 'B-LOC'), ('that', 'O'), ('under', 'O'), ('laboratory', 'O'), ('conditions', 'O'), ('sheep', 'O'), ('could', 'O'), ('contract', 'O'), ('Bovine', 'B-MISC'), ('Spongiform', 'I-MISC'), ('Encephalopathy', 'I-MISC'), ('(', 'O'), ('BSE', 'B-MISC'), (')', 'O'), ('--', 'O'), ('mad', 'B-MISC'), ('cow', 'I-MISC'), ('disease', 'I-MISC'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"But\", \n",
      "    \"Fischler\", \n",
      "    \"agreed\", \n",
      "    \"to\", \n",
      "    \"review\", \n",
      "    \"his\", \n",
      "    \"proposal\", \n",
      "    \"after\", \n",
      "    \"the\", \n",
      "    \"EU\", \n",
      "    \"'s\", \n",
      "    \"standing\", \n",
      "    \"veterinary\", \n",
      "    \"committee\", \n",
      "    \",\", \n",
      "    \"national\", \n",
      "    \"animal\", \n",
      "    \"health\", \n",
      "    \"officials\", \n",
      "    \",\", \n",
      "    \"questioned\", \n",
      "    \"if\", \n",
      "    \"such\", \n",
      "    \"action\", \n",
      "    \"was\", \n",
      "    \"justified\", \n",
      "    \"as\", \n",
      "    \"there\", \n",
      "    \"was\", \n",
      "    \"only\", \n",
      "    \"a\", \n",
      "    \"slight\", \n",
      "    \"risk\", \n",
      "    \"to\", \n",
      "    \"human\", \n",
      "    \"health\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "- Punctuation (`,`, `.`, `'s`) is treated as separate tokens, as they are often useful for context in NER.\n",
      "- \"EU\" is kept as a single token (likely an entity itself).\n",
      "- Multi-word terms like \"standing veterinary committee\" or \"national animal health officials\" are split into individual tokens, but the NER model may later group them into entities if needed.\n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., merging certain tokens or handling contractions differently).\n",
      "Bot: [('But', 'O'), ('Fischler', 'B-PER'), ('agreed', 'O'), ('to', 'O'), ('review', 'O'), ('his', 'O'), ('proposal', 'O'), ('after', 'O'), ('the', 'O'), ('EU', 'B-ORG'), (\"'s\", 'O'), ('standing', 'O'), ('veterinary', 'O'), ('committee', 'O'), (',', 'O'), ('national', 'O'), ('animal', 'O'), ('health', 'O'), ('officials', 'O'), (',', 'O'), ('questioned', 'O'), ('if', 'O'), ('such', 'O'), ('action', 'O'), ('was', 'O'), ('justified', 'O'), ('as', 'O'), ('there', 'O'), ('was', 'O'), ('only', 'O'), ('a', 'O'), ('slight', 'O'), ('risk', 'O'), ('to', 'O'), ('human', 'O'), ('health', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Spanish\", \n",
      "    \"Farm\", \n",
      "    \"Minister\", \n",
      "    \"Loyola\", \n",
      "    \"de\", \n",
      "    \"Palacio\", \n",
      "    \"had\", \n",
      "    \"earlier\", \n",
      "    \"accused\", \n",
      "    \"Fischler\", \n",
      "    \"at\", \n",
      "    \"an\", \n",
      "    \"EU\", \n",
      "    \"farm\", \n",
      "    \"ministers\", \n",
      "    \"'\", \n",
      "    \"meeting\", \n",
      "    \"of\", \n",
      "    \"causing\", \n",
      "    \"unjustified\", \n",
      "    \"alarm\", \n",
      "    \"through\", \n",
      "    \"\\\"\", \n",
      "    \"dangerous\", \n",
      "    \"generalisation\", \n",
      "    \".\", \n",
      "    \"\\\"\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like `\"`, `'`, `.`) is split into separate tokens, as they may be relevant for NER (e.g., quotation marks could indicate titles or specific entities).\n",
      "2. \"EU\" is kept as a single token since it’s an acronym (likely referring to the European Union).\n",
      "3. Multi-word names like \"Loyola de Palacio\" are split into individual tokens (\"Loyola\", \"de\", \"Palacio\") to allow the NER model to properly tag each part.\n",
      "4. Hyphenated or contracted words (like \"farm ministers ' meeting\") are split into their components for granularity.\n",
      "\n",
      "This tokenization is suitable for NER tasks where entities like `Spanish Farm Minister`, `Loyola de Palacio`, `Fischler`, and `EU` need to be identified.\n",
      "Bot: [('Spanish', 'B-MISC'), ('Farm', 'I-MISC'), ('Minister', 'I-MISC'), ('Loyola', 'B-PER'), ('de', 'I-PER'), ('Palacio', 'I-PER'), ('had', 'O'), ('earlier', 'O'), ('accused', 'O'), ('Fischler', 'B-PER'), ('at', 'O'), ('an', 'O'), ('EU', 'B-ORG'), ('farm', 'O'), ('ministers', 'O'), (\"'\", 'O'), ('meeting', 'O'), ('of', 'O'), ('causing', 'O'), ('unjustified', 'O'), ('alarm', 'O'), ('through', 'O'), ('\"', 'O'), ('dangerous', 'O'), ('generalisation', 'O'), ('.', 'O'), ('\"', 'O')]\n",
      "Bot: Given the input sentence is empty (`\"   .\"`), which consists only of whitespace and a period, the tokenized output would be:\n",
      "\n",
      "```python\n",
      "[\".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- The whitespace is ignored (treated as a separator).\n",
      "- The period `.` is treated as a separate token, as punctuation is often tokenized individually in NLP tasks, including named entity recognition (NER).\n",
      "\n",
      "If you intended to provide a different sentence, please share it, and I can tokenize it accordingly!\n",
      "Bot: [('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"Only\", \"France\", \"and\", \"Britain\", \"backed\", \"Fischler\", \"'s\", \"proposal\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"Only\"** – Standalone word (adverb).  \n",
      "2. **\"France\"** – Proper noun (country, likely a named entity).  \n",
      "3. **\"and\"** – Conjunction.  \n",
      "4. **\"Britain\"** – Proper noun (country, likely a named entity).  \n",
      "5. **\"backed\"** – Verb (past tense of \"back\").  \n",
      "6. **\"Fischler\"** – Proper noun (person's name, likely a named entity).  \n",
      "7. **\"'s\"** – Possessive clitic (part of \"Fischler's\").  \n",
      "8. **\"proposal\"** – Noun.  \n",
      "9. **\".\"** – Punctuation (sentence terminator).  \n",
      "\n",
      "This tokenization preserves named entities (e.g., \"France\", \"Britain\", \"Fischler\") as single tokens, which is crucial for NER. Contractions like \"'s\" are often split into separate tokens in many NLP pipelines (e.g., spaCy, Hugging Face Tokenizers).  \n",
      "\n",
      "Let me know if you'd prefer a different tokenization strategy (e.g., keeping \"Fischler's\" as one token).\n",
      "Bot: [('Only', 'O'), ('France', 'B-LOC'), ('and', 'O'), ('Britain', 'B-LOC'), ('backed', 'O'), ('Fischler', 'B-PER'), (\"'s\", 'O'), ('proposal', 'O'), ('.', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"The\", \"EU\", \"'s\", \"scientific\", \"veterinary\", \"and\", \"multidisciplinary\", \n",
      "    \"committees\", \"are\", \"due\", \"to\", \"re-examine\", \"the\", \"issue\", \"early\", \n",
      "    \"next\", \"month\", \"and\", \"make\", \"recommendations\", \"to\", \"the\", \"senior\", \n",
      "    \"veterinary\", \"officials\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation like `'s` and `.` is treated as separate tokens.\n",
      "2. Hyphenated words like \"re-examine\" are kept as single tokens (common in NER to preserve meaning).\n",
      "3. Case is preserved (e.g., \"EU\" is uppercase as it likely refers to the European Union entity).\n",
      "\n",
      "Let me know if you'd like adjustments (e.g., splitting hyphens, normalizing case).\n",
      "Bot: [('The', 'O'), ('EU', 'B-ORG'), (\"'s\", 'O'), ('scientific', 'O'), ('veterinary', 'O'), ('and', 'O'), ('multidisciplinary', 'O'), ('committees', 'O'), ('are', 'O'), ('due', 'O'), ('to', 'O'), ('re-examine', 'O'), ('the', 'O'), ('issue', 'O'), ('early', 'O'), ('next', 'O'), ('month', 'O'), ('and', 'O'), ('make', 'O'), ('recommendations', 'O'), ('to', 'O'), ('the', 'O'), ('senior', 'O'), ('veterinary', 'O'), ('officials', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"Sheep\", \"have\", \"long\", \"been\", \"known\", \"to\", \"contract\", \"scrapie\", \",\",\n",
      "    \"a\", \"brain-wasting\", \"disease\", \"similar\", \"to\", \"BSE\", \"which\", \"is\",\n",
      "    \"believed\", \"to\", \"have\", \"been\", \"transferred\", \"to\", \"cattle\", \"through\",\n",
      "    \"feed\", \"containing\", \"animal\", \"waste\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (\",\", \".\") is treated as separate tokens.\n",
      "2. Hyphenated words (\"brain-wasting\") are kept as single tokens unless your NER model specifically requires splitting them.\n",
      "3. \"BSE\" is retained as a single token (likely an entity referring to \"Bovine Spongiform Encephalopathy\").\n",
      "4. The word \"believed\" was corrected from \"believed\" in the original sentence (assuming a typo).\n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., splitting hyphenated terms or handling contractions differently).\n",
      "Bot: [('Sheep', 'O'), ('have', 'O'), ('long', 'O'), ('been', 'O'), ('known', 'O'), ('to', 'O'), ('contract', 'O'), ('scrapie', 'B-MISC'), (',', 'O'), ('a', 'O'), ('brain-wasting', 'O'), ('disease', 'O'), ('similar', 'O'), ('to', 'O'), ('BSE', 'B-MISC'), ('which', 'O'), ('is', 'O'), ('believed', 'O'), ('to', 'O'), ('have', 'O'), ('been', 'O'), ('transferred', 'O'), ('to', 'O'), ('cattle', 'O'), ('through', 'O'), ('feed', 'O'), ('containing', 'O'), ('animal', 'O'), ('waste', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"British\", \"farmers\", \"denied\", \"on\", \"Thursday\", \"there\", \"was\", \n",
      "    \"any\", \"danger\", \"to\", \"human\", \"health\", \"from\", \"their\", \"sheep\", \n",
      "    \",\", \"but\", \"expressed\", \"concern\", \"that\", \"German\", \"government\", \n",
      "    \"advice\", \"to\", \"consumers\", \"to\", \"avoid\", \"British\", \"lamb\", \n",
      "    \"might\", \"influence\", \"consumers\", \"across\", \"Europe\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (\",\", \".\") is treated as separate tokens.  \n",
      "2. Words are split based on whitespace and punctuation boundaries.  \n",
      "3. This tokenization preserves entities like \"British\", \"German\", \"Europe\", etc., which are likely targets for NER (e.g., nationalities, countries, organizations).  \n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., splitting contractions like \"there's\" or handling hyphenated words differently).\n",
      "Bot: [('British', 'B-MISC'), ('farmers', 'O'), ('denied', 'O'), ('on', 'O'), ('Thursday', 'O'), ('there', 'O'), ('was', 'O'), ('any', 'O'), ('danger', 'O'), ('to', 'O'), ('human', 'O'), ('health', 'O'), ('from', 'O'), ('their', 'O'), ('sheep', 'O'), (',', 'O'), ('but', 'O'), ('expressed', 'O'), ('concern', 'O'), ('that', 'O'), ('German', 'B-MISC'), ('government', 'O'), ('advice', 'O'), ('to', 'O'), ('consumers', 'O'), ('to', 'O'), ('avoid', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('might', 'O'), ('influence', 'O'), ('consumers', 'O'), ('across', 'O'), ('Europe', 'B-LOC'), ('.', 'O')]\n",
      "Bot: Here is the given sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"What\", \"we\", \"have\", \"to\", \"be\", \"extremely\", \"careful\", \"of\", \"is\", \n",
      "    \"how\", \"other\", \"countries\", \"are\", \"going\", \"to\", \"take\", \"Germany\", \n",
      "    \"'s\", \"lead\", \",\", \"\\\"\", \"Welsh\", \"National\", \"Farmers\", \"'\", \"Union\", \n",
      "    \"(\", \"NFU\", \")\", \"chairman\", \"John\", \"Lloyd\", \"Jones\", \"said\", \"on\", \n",
      "    \"BBC\", \"radio\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (`,`, `\"`, `(`, `)`, `.`) is treated as separate tokens.\n",
      "2. Possessive markers (`'s`) and contractions (`'`) are split into their own tokens.\n",
      "3. Multi-word named entities (e.g., \"Welsh National Farmers' Union\", \"John Lloyd Jones\", \"BBC\") are split into individual tokens but should be reassembled during NER labeling (e.g., as `[ORG]`, `[PERSON]`, `[ORG]`).\n",
      "4. Acronyms like \"NFU\" are kept as single tokens.\n",
      "\n",
      "This format is compatible with standard NER tokenization pipelines (e.g., spaCy, Hugging Face Tokenizers). Let me know if you'd like adjustments!\n",
      "Bot: [('What', 'O'), ('we', 'O'), ('have', 'O'), ('to', 'O'), ('be', 'O'), ('extremely', 'O'), ('careful', 'O'), ('of', 'O'), ('is', 'O'), ('how', 'O'), ('other', 'O'), ('countries', 'O'), ('are', 'O'), ('going', 'O'), ('to', 'O'), ('take', 'O'), ('Germany', 'B-LOC'), (\"'s\", 'O'), ('lead', 'O'), (',', 'O'), ('\"', 'O'), ('Welsh', 'B-ORG'), ('National', 'I-ORG'), ('Farmers', 'I-ORG'), (\"'\", 'I-ORG'), ('Union', 'I-ORG'), ('(', 'O'), ('NFU', 'B-ORG'), (')', 'O'), ('chairman', 'O'), ('John', 'B-PER'), ('Lloyd', 'I-PER'), ('Jones', 'I-PER'), ('said', 'O'), ('on', 'O'), ('BBC', 'B-ORG'), ('radio', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"Bonn\", \"has\", \"led\", \"efforts\", \"to\", \"protect\", \"public\", \"health\", \n",
      "    \"after\", \"consumer\", \"confidence\", \"collapsed\", \"in\", \"March\", \n",
      "    \"after\", \"a\", \"British\", \"report\", \"suggested\", \"humans\", \"could\", \n",
      "    \"contract\", \"an\", \"illness\", \"similar\", \"to\", \"mad\", \"cow\", \"disease\", \n",
      "    \"by\", \"eating\", \"contaminated\", \"beef\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like \".\") is treated as a separate token.\n",
      "2. Words are split into individual tokens (e.g., \"mad cow disease\" → [\"mad\", \"cow\", \"disease\"]).\n",
      "3. Proper nouns like \"Bonn,\" \"British,\" and \"March\" are preserved as single tokens for NER.\n",
      "4. Hyphenated or multi-word terms (e.g., \"mad cow disease\") are split unless they are named entities (e.g., \"New York\" would stay as one token if labeled as a location).\n",
      "\n",
      "Let me know if you'd like adjustments (e.g., keeping \"mad cow disease\" as a single token).\n",
      "Bot: [('Bonn', 'B-LOC'), ('has', 'O'), ('led', 'O'), ('efforts', 'O'), ('to', 'O'), ('protect', 'O'), ('public', 'O'), ('health', 'O'), ('after', 'O'), ('consumer', 'O'), ('confidence', 'O'), ('collapsed', 'O'), ('in', 'O'), ('March', 'B-MISC'), ('after', 'O'), ('a', 'O'), ('British', 'B-MISC'), ('report', 'O'), ('suggested', 'O'), ('humans', 'O'), ('could', 'O'), ('contract', 'O'), ('an', 'O'), ('illness', 'O'), ('similar', 'O'), ('to', 'O'), ('mad', 'O'), ('cow', 'O'), ('disease', 'O'), ('by', 'O'), ('eating', 'O'), ('contaminated', 'O'), ('beef', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"Germany\", \n",
      "    \"imported\", \n",
      "    \"47,600\", \n",
      "    \"sheep\", \n",
      "    \"from\", \n",
      "    \"Britain\", \n",
      "    \"last\", \n",
      "    \"year\", \n",
      "    \",\", \n",
      "    \"nearly\", \n",
      "    \"half\", \n",
      "    \"of\", \n",
      "    \"total\", \n",
      "    \"imports\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **Punctuation Handling**: Commas (`,`) and periods (`.`) are treated as separate tokens, as they can be relevant for NER (e.g., sentence boundaries or numerical formatting).  \n",
      "2. **Numbers**: \"47,600\" is kept as a single token because it represents a numerical value (though some tokenizers might split it further).  \n",
      "3. **Case Sensitivity**: Original casing is preserved (e.g., \"Germany\" and \"Britain\" are capitalized as proper nouns).  \n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., splitting \"47,600\" into `[\"47\", \",\", \"600\"]` or handling contractions differently).\n",
      "Bot: [('Germany', 'B-LOC'), ('imported', 'O'), ('47,600', 'O'), ('sheep', 'O'), ('from', 'O'), ('Britain', 'B-LOC'), ('last', 'O'), ('year', 'O'), (',', 'O'), ('nearly', 'O'), ('half', 'O'), ('of', 'O'), ('total', 'O'), ('imports', 'O'), ('.', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"It\", \"brought\", \"in\", \"4,275\", \"tonnes\", \"of\", \"British\", \"mutton\", \",\", \n",
      "    \"some\", \"10\", \"percent\", \"of\", \"overall\", \"imports\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- Punctuation (like `,` and `.`) is separated as individual tokens since they can be important for NER boundaries.\n",
      "- Numbers with commas (`4,275`) are kept as single tokens to preserve their semantic meaning.\n",
      "- Words like \"British\" (likely an entity modifier) and \"mutton\" (a potential entity) are kept separate for NER labeling.\n",
      "- Multi-word terms (e.g., \"overall imports\") are split into individual tokens unless they are part of a named entity (e.g., \"British mutton\" could be labeled as a single entity if needed, but tokenization separates them for flexibility). \n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., merging \"British mutton\" as a single token).\n",
      "Bot: [('It', 'O'), ('brought', 'O'), ('in', 'O'), ('4,275', 'O'), ('tonnes', 'O'), ('of', 'O'), ('British', 'B-MISC'), ('mutton', 'I-MISC'), (',', 'O'), ('some', 'O'), ('10', 'O'), ('percent', 'O'), ('of', 'O'), ('overall', 'O'), ('imports', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"Rare\", \"Hendrix\", \"song\", \"draft\", \"sells\", \"for\", \"almost\", \"$\", \"17,000\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"Rare\"** – Adjective describing the song draft.  \n",
      "2. **\"Hendrix\"** – Proper noun (likely referring to Jimi Hendrix, a named entity of type `PER`).  \n",
      "3. **\"song\"** – Common noun.  \n",
      "4. **\"draft\"** – Common noun referring to an early version.  \n",
      "5. **\"sells\"** – Verb.  \n",
      "6. **\"for\"** – Preposition.  \n",
      "7. **\"almost\"** – Adverb modifying the amount.  \n",
      "8. **\"$\"** – Currency symbol (part of a `MONEY` entity).  \n",
      "9. **\"17,000\"** – Numeric value (part of a `MONEY` entity).  \n",
      "10. **\".\"** – Punctuation.  \n",
      "\n",
      "### Expected NER Tags (for reference):\n",
      "- `Hendrix` → `PER` (Person)  \n",
      "- `$ 17,000` → `MONEY`  \n",
      "\n",
      "This tokenization preserves meaningful units for NER while splitting punctuation and symbols appropriately.\n",
      "Bot: [('Rare', 'O'), ('Hendrix', 'B-PER'), ('song', 'O'), ('draft', 'O'), ('sells', 'O'), ('for', 'O'), ('almost', 'O'), ('$', 'O'), ('17,000', 'B-MISC'), ('.', 'O')]\n",
      "Bot: Here is the tokenized breakdown of the input sentence `\"LONDON 1996-08-22\"` for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\"LONDON\", \"1996-08-22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"LONDON\"** is a standalone token representing a location (city).\n",
      "2. **\"1996-08-22\"** is a single token representing a date in ISO format (YYYY-MM-DD). In NER tasks, dates are typically treated as a single entity rather than splitting them further.\n",
      "\n",
      "If you'd like a more granular breakdown (e.g., splitting the date into year, month, and day), let me know, but for standard NER, the above is appropriate.\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"A\", \"rare\", \"early\", \"handwritten\", \"draft\", \"of\", \"a\", \"song\", \"by\", \n",
      "    \"U.S.\", \"guitar\", \"legend\", \"Jimi\", \"Hendrix\", \"was\", \"sold\", \"for\", \n",
      "    \"almost\", \"$\", \"17,000\", \"on\", \"Thursday\", \"at\", \"an\", \"auction\", \"of\", \n",
      "    \"some\", \"of\", \"the\", \"late\", \"musician\", \"'s\", \"favourite\", \"possessions\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **\"U.S.\"** is kept as a single token (common in NER for country/organization recognition).  \n",
      "2. **\"$ 17,000\"** is split into `\"$\"` and `\"17,000\"` (common practice for currency symbols and values).  \n",
      "3. **\"'s\"** is separated as a possessive marker.  \n",
      "4. Punctuation (`.` at the end) is treated as a separate token.  \n",
      "\n",
      "This tokenization preserves entities like:  \n",
      "- **\"Jimi Hendrix\"** (person),  \n",
      "- **\"U.S.\"** (country),  \n",
      "- **\"Thursday\"** (date),  \n",
      "- **\"$ 17,000\"** (monetary value).  \n",
      "\n",
      "Let me know if you'd prefer a different splitting strategy!\n",
      "Bot: [('A', 'O'), ('rare', 'O'), ('early', 'O'), ('handwritten', 'O'), ('draft', 'O'), ('of', 'O'), ('a', 'O'), ('song', 'O'), ('by', 'O'), ('U.S.', 'B-LOC'), ('guitar', 'O'), ('legend', 'O'), ('Jimi', 'B-PER'), ('Hendrix', 'I-PER'), ('was', 'O'), ('sold', 'O'), ('for', 'O'), ('almost', 'O'), ('$', 'O'), ('17,000', 'O'), ('on', 'O'), ('Thursday', 'B-MISC'), ('at', 'O'), ('an', 'O'), ('auction', 'O'), ('of', 'O'), ('some', 'O'), ('of', 'O'), ('the', 'O'), ('late', 'O'), ('musician', 'O'), (\"'s\", 'O'), ('favourite', 'O'), ('possessions', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"A\", \"Florida\", \"restaurant\", \"paid\", \"10,925\", \"pounds\", \"(\", \"$\", \"16,935\", \")\", \"for\", \"the\", \n",
      "    \"draft\", \"of\", \"\\\"\", \"Ai\", \"n't\", \"no\", \"telling\", \"\\\"\", \",\", \"which\", \"Hendrix\", \"penned\", \n",
      "    \"on\", \"a\", \"piece\", \"of\", \"London\", \"hotel\", \"stationery\", \"in\", \"late\", \"1966\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. **Punctuation Handling**:  \n",
      "   - Parentheses (`(`, `)`), commas (`,`), periods (`.`), and quotation marks (`\"`) are treated as separate tokens.\n",
      "   - The contraction *\"n't\"* is split from *\"Ai\"* (though *\"Ain't\"* might sometimes be tokenized as a single word depending on the tokenizer).  \n",
      "\n",
      "2. **Numerical Values**:  \n",
      "   - \"10,925\" and \"16,935\" are kept intact (including commas as thousand separators).  \n",
      "\n",
      "3. **Named Entities (for NER)**:  \n",
      "   - \"Florida\" (GPE), \"Hendrix\" (PER), \"London\" (GPE), and \"1966\" (DATE) are likely entities of interest.  \n",
      "   - Monetary values like \"$ 16,935\" could also be tagged (as MONEY).  \n",
      "\n",
      "Let me know if you'd prefer a different tokenization strategy (e.g., merging \"$\" and \"16,935\" into one token).\n",
      "Bot: [('A', 'O'), ('Florida', 'B-LOC'), ('restaurant', 'O'), ('paid', 'O'), ('10,925', 'O'), ('pounds', 'O'), ('(', 'O'), ('$', 'O'), ('16,935', 'O'), (')', 'O'), ('for', 'O'), ('the', 'O'), ('draft', 'O'), ('of', 'O'), ('\"', 'O'), ('Ai', 'O'), ('n\\'t', 'O'), ('no', 'O'), ('telling', 'O'), ('\"', 'O'), (',', 'O'), ('which', 'O'), ('Hendrix', 'B-PER'), ('penned', 'O'), ('on', 'O'), ('a', 'O'), ('piece', 'O'), ('of', 'O'), ('London', 'B-LOC'), ('hotel', 'O'), ('stationery', 'O'), ('in', 'O'), ('late', 'O'), ('1966', 'B-MISC'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"At\", \"the\", \"end\", \"of\", \"a\", \"January\", \"1967\", \"concert\", \n",
      "    \"in\", \"the\", \"English\", \"city\", \"of\", \"Nottingham\", \"he\", \n",
      "    \"threw\", \"the\", \"sheet\", \"of\", \"paper\", \"into\", \"the\", \n",
      "    \"audience\", \",\", \"where\", \"it\", \"was\", \"retrieved\", \"by\", \n",
      "    \"a\", \"fan\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. Punctuation (`,`, `.`) is treated as separate tokens, as they can be relevant in NER (e.g., for sentence boundaries or context).\n",
      "2. Words are split on whitespace and punctuation boundaries.\n",
      "3. \"January 1967\" is split into two tokens (\"January\", \"1967\") as they are distinct entities (month and year).\n",
      "4. \"Nottingham\" is kept as a single token (a city name).  \n",
      "\n",
      "This format is suitable for NER tasks where each token will be labeled with its entity type (e.g., `January` as `DATE`, `Nottingham` as `GPE`).\n",
      "Bot: [('At', 'O'), ('the', 'O'), ('end', 'O'), ('of', 'O'), ('a', 'O'), ('January', 'B-MISC'), ('1967', 'B-MISC'), ('concert', 'O'), ('in', 'O'), ('the', 'O'), ('English', 'B-MISC'), ('city', 'O'), ('of', 'O'), ('Nottingham', 'B-LOC'), ('he', 'O'), ('threw', 'O'), ('the', 'O'), ('sheet', 'O'), ('of', 'O'), ('paper', 'O'), ('into', 'O'), ('the', 'O'), ('audience', 'O'), (',', 'O'), ('where', 'O'), ('it', 'O'), ('was', 'O'), ('retrieved', 'O'), ('by', 'O'), ('a', 'O'), ('fan', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Buyers\", \"also\", \"snapped\", \"up\", \"16\", \"other\", \"items\", \"that\", \n",
      "    \"were\", \"put\", \"up\", \"for\", \"auction\", \"by\", \"Hendrix\", \"'s\", \n",
      "    \"former\", \"girlfriend\", \"Kathy\", \"Etchingham\", \",\", \"who\", \n",
      "    \"lived\", \"with\", \"him\", \"from\", \"1966\", \"to\", \"1969\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **Punctuation Handling**: Punctuation marks like commas (`\",\"`), periods (`\".\"`), and apostrophes (`\"'s\"`) are treated as separate tokens, as they can be important for NER (e.g., possessive markers or sentence boundaries).\n",
      "2. **Numbers**: Numeric values like \"16\", \"1966\", and \"1969\" are kept as separate tokens.\n",
      "3. **Named Entities**: Multi-word names like \"Kathy Etchingham\" are split into individual tokens (\"Kathy\", \"Etchingham\") to allow the NER model to tag each part separately (e.g., `[PER] [PER]`).\n",
      "4. **Contractions**: Words like \"'s\" are split from their preceding token (e.g., \"Hendrix\" + \"'s\").\n",
      "\n",
      "This tokenization is suitable for NER tasks where granularity is important for identifying entity boundaries.\n",
      "Bot: [  \n",
      "    ('Buyers', 'O'),  \n",
      "    ('also', 'O'),  \n",
      "    ('snapped', 'O'),  \n",
      "    ('up', 'O'),  \n",
      "    ('16', 'O'),  \n",
      "    ('other', 'O'),  \n",
      "    ('items', 'O'),  \n",
      "    ('that', 'O'),  \n",
      "    ('were', 'O'),  \n",
      "    ('put', 'O'),  \n",
      "    ('up', 'O'),  \n",
      "    ('for', 'O'),  \n",
      "    ('auction', 'O'),  \n",
      "    ('by', 'O'),  \n",
      "    ('Hendrix', 'B-PER'),  \n",
      "    (\"'s\", 'O'),  \n",
      "    ('former', 'O'),  \n",
      "    ('girlfriend', 'O'),  \n",
      "    ('Kathy', 'B-PER'),  \n",
      "    ('Etchingham', 'I-PER'),  \n",
      "    (',', 'O'),  \n",
      "    ('who', 'O'),  \n",
      "    ('lived', 'O'),  \n",
      "    ('with', 'O'),  \n",
      "    ('him', 'O'),  \n",
      "    ('from', 'O'),  \n",
      "    ('1966', 'O'),  \n",
      "    ('to', 'O'),  \n",
      "    ('1969', 'O'),  \n",
      "    ('.', 'O')  \n",
      "]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"They\", \"included\", \"a\", \"black\", \"lacquer\", \"and\", \"mother\", \"of\", \"pearl\", \n",
      "    \"inlaid\", \"box\", \"used\", \"by\", \"Hendrix\", \"to\", \"store\", \"his\", \"drugs\", \",\", \n",
      "    \"which\", \"an\", \"anonymous\", \"Australian\", \"purchaser\", \"bought\", \"for\", \n",
      "    \"5,060\", \"pounds\", \"(\", \"$\", \"7,845\", \")\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "- Punctuation (`,`, `(`, `)`, `.`) is treated as separate tokens.\n",
      "- Multi-word terms like \"mother of pearl\" are split into individual tokens (\"mother\", \"of\", \"pearl\").\n",
      "- Currency values (\"5,060 pounds\", \"$ 7,845\") are split into numerical and unit tokens.\n",
      "- Named entities like \"Hendrix\" and \"Australian\" are kept intact for NER labeling.\n",
      "\n",
      "This tokenization preserves meaningful units for entity recognition (e.g., \"Hendrix\" as a person, \"Australian\" as a nationality, \"$ 7,845\" as monetary value).\n",
      "Bot: [  \n",
      "    ('They', 'O'),  \n",
      "    ('included', 'O'),  \n",
      "    ('a', 'O'),  \n",
      "    ('black', 'O'),  \n",
      "    ('lacquer', 'O'),  \n",
      "    ('and', 'O'),  \n",
      "    ('mother', 'O'),  \n",
      "    ('of', 'O'),  \n",
      "    ('pearl', 'O'),  \n",
      "    ('inlaid', 'O'),  \n",
      "    ('box', 'O'),  \n",
      "    ('used', 'O'),  \n",
      "    ('by', 'O'),  \n",
      "    ('Hendrix', 'B-PER'),  \n",
      "    ('to', 'O'),  \n",
      "    ('store', 'O'),  \n",
      "    ('his', 'O'),  \n",
      "    ('drugs', 'O'),  \n",
      "    (',', 'O'),  \n",
      "    ('which', 'O'),  \n",
      "    ('an', 'O'),  \n",
      "    ('anonymous', 'O'),  \n",
      "    ('Australian', 'B-MISC'),  \n",
      "    ('purchaser', 'O'),  \n",
      "    ('bought', 'O'),  \n",
      "    ('for', 'O'),  \n",
      "    ('5,060', 'O'),  \n",
      "    ('pounds', 'O'),  \n",
      "    ('(', 'O'),  \n",
      "    ('$', 'O'),  \n",
      "    ('7,845', 'O'),  \n",
      "    (')', 'O'),  \n",
      "    ('.', 'O')  \n",
      "]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"The\", \"guitarist\", \"died\", \"of\", \"a\", \"drugs\", \"overdose\", \"in\", \"1970\", \"aged\", \"27\", \".\"]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like `.`) is treated as a separate token, as it may be relevant for NER.\n",
      "2. \"drugs overdose\" is split into two tokens (\"drugs\", \"overdose\") since they are separate words (though \"drug overdose\" might be a more common phrasing).\n",
      "3. Numbers (\"1970\", \"27\") are kept as separate tokens since they could be part of entities (e.g., dates, ages).\n",
      "4. This tokenization is whitespace-based and does not use subword splitting (e.g., BPE/WordPiece), as NER typically works best with full words.  \n",
      "\n",
      "Let me know if you'd like a different tokenization approach!\n",
      "Bot: [('The', 'O'), ('guitarist', 'O'), ('died', 'O'), ('of', 'O'), ('a', 'O'), ('drugs', 'O'), ('overdose', 'O'), ('in', 'O'), ('1970', 'O'), ('aged', 'O'), ('27', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"China\", \"says\", \"Taiwan\", \"spoils\", \"atmosphere\", \"for\", \"talks\", \".\"]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Each word and punctuation mark is treated as a separate token.\n",
      "2. \"Taiwan\" was corrected from the original input's typo (\"Taiwan\" instead of \"Taiwan\").\n",
      "3. The period at the end is also tokenized separately, as punctuation is often relevant in NER tasks.\n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., handling contractions, hyphenated words, or other tokenization rules).\n",
      "Bot: [('China', 'B-LOC'), ('says', 'O'), ('Taiwan', 'B-LOC'), ('spoils', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: Here is the tokenized version of the input sentence `\"BEIJING 1996-08-22\"`, broken down into tokens suitable for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\"BEIJING\", \"1996\", \"-\", \"08\", \"-\", \"22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"BEIJING\"** – A single token representing a location (likely a named entity).\n",
      "2. **\"1996\"** – The year part of the date (a numerical entity).\n",
      "3. **\"-\"** – A separator (often treated as a separate token in dates).\n",
      "4. **\"08\"** – The month part of the date (a numerical entity).\n",
      "5. **\"-\"** – Another separator.\n",
      "6. **\"22\"** – The day part of the date (a numerical entity).\n",
      "\n",
      "This breakdown ensures that the NER model can properly identify:\n",
      "- `\"BEIJING\"` as a **location (LOC)** entity.\n",
      "- `\"1996-08-22\"` (split into `[\"1996\", \"-\", \"08\", \"-\", \"22\"]`) as a **date (DATE)** entity.\n",
      "\n",
      "Let me know if you'd prefer a different tokenization approach (e.g., keeping the full date as one token).\n",
      "Bot: [('BEIJING', 'B-LOC'), ('1996', 'O'), ('-', 'O'), ('08', 'O'), ('-', 'O'), ('22', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"China\", \n",
      "    \"on\", \n",
      "    \"Thursday\", \n",
      "    \"accused\", \n",
      "    \"Taipei\", \n",
      "    \"of\", \n",
      "    \"spoiling\", \n",
      "    \"the\", \n",
      "    \"atmosphere\", \n",
      "    \"for\", \n",
      "    \"a\", \n",
      "    \"resumption\", \n",
      "    \"of\", \n",
      "    \"talks\", \n",
      "    \"across\", \n",
      "    \"the\", \n",
      "    \"Taiwan\", \n",
      "    \"Strait\", \n",
      "    \"with\", \n",
      "    \"a\", \n",
      "    \"visit\", \n",
      "    \"to\", \n",
      "    \"Ukraine\", \n",
      "    \"by\", \n",
      "    \"Taiwanese\", \n",
      "    \"Vice\", \n",
      "    \"President\", \n",
      "    \"Lien\", \n",
      "    \"Chan\", \n",
      "    \"this\", \n",
      "    \"week\", \n",
      "    \"that\", \n",
      "    \"infuriated\", \n",
      "    \"Beijing\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **Named Entities** (likely candidates for NER labels) are capitalized and include:  \n",
      "   - `China`, `Taipei`, `Taiwan Strait`, `Ukraine`, `Taiwanese`, `Lien Chan`, `Beijing`  \n",
      "   - `Thursday` (DATE), `Vice President` (TITLE) may also be tagged depending on the NER schema.\n",
      "\n",
      "2. **Punctuation** (`.` at the end) is treated as a separate token.\n",
      "\n",
      "3. **Multi-word entities** like \"Taiwan Strait\" and \"Lien Chan\" are split into individual tokens but should likely be tagged as a single entity (e.g., `LOC` and `PER` respectively).\n",
      "\n",
      "Let me know if you'd like adjustments (e.g., merging multi-word entities or handling contractions differently).\n",
      "Bot: [('China', 'B-LOC'), ('on', 'O'), ('Thursday', 'O'), ('accused', 'O'), ('Taipei', 'B-LOC'), ('of', 'O'), ('spoiling', 'O'), ('the', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('a', 'O'), ('resumption', 'O'), ('of', 'O'), ('talks', 'O'), ('across', 'O'), ('the', 'O'), ('Taiwan', 'B-LOC'), ('Strait', 'I-LOC'), ('with', 'O'), ('a', 'O'), ('visit', 'O'), ('to', 'O'), ('Ukraine', 'B-LOC'), ('by', 'O'), ('Taiwanese', 'B-MISC'), ('Vice', 'O'), ('President', 'O'), ('Lien', 'B-PER'), ('Chan', 'I-PER'), ('this', 'O'), ('week', 'O'), ('that', 'O'), ('infuriated', 'O'), ('Beijing', 'B-LOC'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens as a list, suitable for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Speaking\", \"only\", \"hours\", \"after\", \"Chinese\", \"state\", \"media\", \"said\", \n",
      "    \"the\", \"time\", \"was\", \"right\", \"to\", \"engage\", \"in\", \"political\", \"talks\", \n",
      "    \"with\", \"Taiwan\", \",\", \"Foreign\", \"Ministry\", \"spokesman\", \"Shen\", \"Guofang\", \n",
      "    \"told\", \"Reuters\", \":\", \"\\\"\", \"The\", \"necessary\", \"atmosphere\", \"for\", \n",
      "    \"the\", \"opening\", \"of\", \"the\", \"talks\", \"has\", \"been\", \"disrupted\", \"by\", \n",
      "    \"the\", \"Taiwan\", \"authorities\", \".\", \"\\\"\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (`,`, `:`, `\"`, `.`) is treated as separate tokens, as they may be relevant for NER (e.g., marking boundaries of entities).\n",
      "2. Named entities like \"Chinese state media,\" \"Taiwan,\" \"Foreign Ministry,\" \"Shen Guofang,\" and \"Reuters\" are preserved as individual tokens to allow the NER model to identify them correctly.\n",
      "3. Multi-word entities (e.g., \"Foreign Ministry\") are split into individual tokens, but the NER task may later group them into a single entity (e.g., `[Foreign, Ministry] → ORG`).\n",
      "4. Quotation marks (`\"`) are included as tokens to capture the exact structure of the sentence.\n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., merging certain tokens or handling contractions differently).\n",
      "Bot: [('Speaking', 'O'), ('only', 'O'), ('hours', 'O'), ('after', 'O'), ('Chinese', 'B-MISC'), ('state', 'I-MISC'), ('media', 'I-MISC'), ('said', 'O'), ('the', 'O'), ('time', 'O'), ('was', 'O'), ('right', 'O'), ('to', 'O'), ('engage', 'O'), ('in', 'O'), ('political', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), (',', 'O'), ('Foreign', 'B-ORG'), ('Ministry', 'I-ORG'), ('spokesman', 'O'), ('Shen', 'B-PER'), ('Guofang', 'I-PER'), ('told', 'O'), ('Reuters', 'B-ORG'), (':', 'O'), ('\"', 'O'), ('The', 'O'), ('necessary', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('the', 'O'), ('opening', 'O'), ('of', 'O'), ('the', 'O'), ('talks', 'O'), ('has', 'O'), ('been', 'O'), ('disrupted', 'O'), ('by', 'O'), ('the', 'O'), ('Taiwan', 'B-LOC'), ('authorities', 'O'), ('.', 'O'), ('\"', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"State\", \"media\", \"quoted\", \"China\", \"'s\", \"top\", \"negotiator\", \n",
      "    \"with\", \"Taipei\", \",\", \"Tang\", \"Shubei\", \",\", \"as\", \"telling\", \n",
      "    \"a\", \"visiting\", \"group\", \"from\", \"Taiwan\", \"on\", \"Wednesday\", \n",
      "    \"that\", \"it\", \"was\", \"time\", \"for\", \"the\", \"rivals\", \"to\", \n",
      "    \"hold\", \"political\", \"talks\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. **Punctuation**: Commas (\",\") and the period (\".\") are treated as separate tokens, as they may be relevant for NER (e.g., marking the end of an entity span).\n",
      "2. **Possessive \"'s\"**: Split from \"China\" to ensure proper handling of the entity \"China\".\n",
      "3. **Named Entities**: Multi-word entities like \"Tang Shubei\" are split into individual tokens (\"Tang\", \"Shubei\") to allow the NER model to tag each part (e.g., `B-PER`, `I-PER`).\n",
      "4. **Prepositions/Articles**: Small words like \"a\", \"the\", \"on\", etc., are kept as separate tokens.\n",
      "\n",
      "This tokenization is suitable for NER tasks like BIO (Begin, Inside, Outside) tagging. For example:\n",
      "- \"Tang Shubei\" → `[\"B-PER\", \"I-PER\"]`  \n",
      "- \"Taiwan\" → `[\"B-GPE\"]`  \n",
      "- \"Wednesday\" → `[\"B-DATE\"]`  \n",
      "\n",
      "Let me know if you'd like further adjustments!\n",
      "Bot: [('State', 'B-ORG'), ('media', 'I-ORG'), ('quoted', 'O'), ('China', 'B-LOC'), (\"'s\", 'O'), ('top', 'O'), ('negotiator', 'O'), ('with', 'O'), ('Taipei', 'B-LOC'), (',', 'O'), ('Tang', 'B-PER'), ('Shubei', 'I-PER'), (',', 'O'), ('as', 'O'), ('telling', 'O'), ('a', 'O'), ('visiting', 'O'), ('group', 'O'), ('from', 'O'), ('Taiwan', 'B-LOC'), ('on', 'O'), ('Wednesday', 'B-MISC'), ('that', 'O'), ('it', 'O'), ('was', 'O'), ('time', 'O'), ('for', 'O'), ('the', 'O'), ('rivals', 'O'), ('to', 'O'), ('hold', 'O'), ('political', 'O'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"Now\", \"is\", \"the\", \"time\", \"for\", \"the\", \"two\", \"sides\", \n",
      "    \"to\", \"engage\", \"in\", \"political\", \"talks\", \"...\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation like `\"...\"` is treated as a separate token, as NER models often handle punctuation as individual tokens.\n",
      "2. Contractions or hyphenated words would typically be split further if needed, but this sentence does not contain any.\n",
      "3. The tokenization follows a standard whitespace and punctuation-based approach, which is common in NER tasks.\n",
      "\n",
      "Let me know if you'd like a different tokenization strategy (e.g., subword tokenization with BERT/Spacy) for your specific NER model!\n",
      "Bot: [('Now', 'O'), ('is', 'O'), ('the', 'O'), ('time', 'O'), ('for', 'O'), ('the', 'O'), ('two', 'O'), ('sides', 'O'), ('to', 'O'), ('engage', 'O'), ('in', 'O'), ('political', 'O'), ('talks', 'O'), ('...', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"that\", \"is\", \"to\", \"end\", \"the\", \"state\", \"of\", \"hostility\", \",\", \n",
      "    \"\\\"\", \"Thursday\", \"'s\", \"overseas\", \"edition\", \"of\", \"the\", \"People\", \"'s\", \"Daily\", \n",
      "    \"quoted\", \"Tang\", \"as\", \"saying\", \".\", \"\\\"\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. Punctuation (`,`, `\"`, `.`) is split into separate tokens, as they may be relevant for NER boundaries.\n",
      "2. Possessive clitics (`'s`) are separated from the preceding word (e.g., `Thursday 's`, `People 's`).\n",
      "3. Named entities like `People 's Daily` (a newspaper) and `Tang` (a person) are preserved as multi-word tokens where applicable, but split into subtokens if the NER model expects atomic tokens.\n",
      "4. Hyphenated or apostrophized words (e.g., \"overseas\") are kept intact unless the NER model requires further splitting.\n",
      "\n",
      "Let me know if you'd prefer a different tokenization strategy (e.g., merging `People 's Daily` into a single token).\n",
      "Bot: [('that', 'O'), ('is', 'O'), ('to', 'O'), ('end', 'O'), ('the', 'O'), ('state', 'O'), ('of', 'O'), ('hostility', 'O'), (',', 'O'), ('\"', 'O'), ('Thursday', 'B-MISC'), (\"'s\", 'O'), ('overseas', 'O'), ('edition', 'O'), ('of', 'O'), ('the', 'O'), ('People', 'B-ORG'), (\"'s\", 'I-ORG'), ('Daily', 'I-ORG'), ('quoted', 'O'), ('Tang', 'B-PER'), ('as', 'O'), ('saying', 'O'), ('.', 'O'), ('\"', 'O')]\n",
      "Bot: Here is the tokenized version of the given sentence, split into tokens suitable for a named entity recognition (NER) task:  \n",
      "\n",
      "```python\n",
      "[\n",
      "    \"The\", \"foreign\", \"ministry\", \"'s\", \"Shen\", \"told\", \"Reuters\", \"Television\", \n",
      "    \"in\", \"an\", \"interview\", \"he\", \"had\", \"read\", \"reports\", \"of\", \"Tang\", \"'s\", \n",
      "    \"comments\", \"but\", \"gave\", \"no\", \"details\", \"of\", \"why\", \"the\", \"negotiator\", \n",
      "    \"had\", \"considered\", \"the\", \"time\", \"right\", \"for\", \"talks\", \"with\", \"Taiwan\", \n",
      "    \",\", \"which\", \"Beijing\", \"considers\", \"a\", \"renegade\", \"province\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:  \n",
      "- Punctuation (like `,` and `.`) is treated as separate tokens.  \n",
      "- Possessive markers (`'s`) are split from the preceding word (e.g., `\"Tang 's\"`).  \n",
      "- Named entities (e.g., \"Shen,\" \"Reuters Television,\" \"Taiwan,\" \"Beijing\") are kept intact as single tokens.  \n",
      "\n",
      "This format is suitable for NER tasks where each token will be labeled with its corresponding entity type (e.g., `\"Shen\"` → `PERSON`, `\"Taiwan\"` → `GPE`).\n",
      "Bot: [('The', 'O'), ('foreign', 'O'), ('ministry', 'O'), (\"'s\", 'O'), ('Shen', 'B-PER'), ('told', 'O'), ('Reuters', 'B-ORG'), ('Television', 'I-ORG'), ('in', 'O'), ('an', 'O'), ('interview', 'O'), ('he', 'O'), ('had', 'O'), ('read', 'O'), ('reports', 'O'), ('of', 'O'), ('Tang', 'B-PER'), (\"'s\", 'O'), ('comments', 'O'), ('but', 'O'), ('gave', 'O'), ('no', 'O'), ('details', 'O'), ('of', 'O'), ('why', 'O'), ('the', 'O'), ('negotiator', 'O'), ('had', 'O'), ('considered', 'O'), ('the', 'O'), ('time', 'O'), ('right', 'O'), ('for', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), (',', 'O'), ('which', 'O'), ('Beijing', 'B-LOC'), ('considers', 'O'), ('a', 'O'), ('renegade', 'O'), ('province', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"China\", \",\", \"which\", \"has\", \"long\", \"opposed\", \"all\", \"Taipei\", \n",
      "    \"efforts\", \"to\", \"gain\", \"greater\", \"international\", \"recognition\", \n",
      "    \",\", \"was\", \"infuriated\", \"by\", \"a\", \"visit\", \"to\", \"Ukraine\", \n",
      "    \"this\", \"week\", \"by\", \"Taiwanese\", \"Vice\", \"President\", \"Lien\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (`,`, `.`) is treated as separate tokens since they can be relevant for NER (e.g., marking the end of an entity span).\n",
      "2. Multi-word entities like \"Taiwanese Vice President Lien\" are split into individual tokens to allow the NER model to tag each component (e.g., `\"Taiwanese\"`, `\"Vice\"`, `\"President\"`, `\"Lien\"`).\n",
      "3. \"Taipei\" and \"Ukraine\" are preserved as single tokens since they represent geopolitical entities.\n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., merging \"Vice President\" into one token).\n",
      "Bot: [('China', 'B-LOC'), (',', 'O'), ('which', 'O'), ('has', 'O'), ('long', 'O'), ('opposed', 'O'), ('all', 'O'), ('Taipei', 'B-LOC'), ('efforts', 'O'), ('to', 'O'), ('gain', 'O'), ('greater', 'O'), ('international', 'O'), ('recognition', 'O'), (',', 'O'), ('was', 'O'), ('infuriated', 'O'), ('by', 'O'), ('a', 'O'), ('visit', 'O'), ('to', 'O'), ('Ukraine', 'B-LOC'), ('this', 'O'), ('week', 'O'), ('by', 'O'), ('Taiwanese', 'B-MISC'), ('Vice', 'I-MISC'), ('President', 'I-MISC'), ('Lien', 'B-PER'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "['China', 'says', 'time', 'right', 'for', 'Taiwan', 'talks', '.']\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like `.`) is treated as a separate token, as it can be relevant for NER tasks (e.g., marking the end of an entity mention).\n",
      "2. Words are lowercased only if the NER model expects lowercase input (this output preserves casing since many NER models are case-sensitive).\n",
      "3. No stemming or lemmatization is applied, as NER typically relies on surface forms.\n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., splitting contractions, handling hyphenated words differently, etc.).\n",
      "Bot: [('China', 'B-LOC'), ('says', 'O'), ('time', 'O'), ('right', 'O'), ('for', 'O'), ('Taiwan', 'B-LOC'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: To tokenize the sentence \"BEIJING 1996-08-22\" for a named entity recognition (NER) task, we should break it down into meaningful units (tokens) that could represent entities or parts of entities. Here's how you can tokenize it:\n",
      "\n",
      "### Input:  \n",
      "`\"BEIJING 1996-08-22\"`\n",
      "\n",
      "### Tokenized Output (as a list):  \n",
      "```python\n",
      "[\"BEIJING\", \"1996\", \"-\", \"08\", \"-\", \"22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"BEIJING\"** → A location (likely a named entity, such as a city).\n",
      "2. **\"1996\"** → A year (part of a date entity).\n",
      "3. **\"-\"** → A separator in the date (may or may not be useful for NER depending on the model).\n",
      "4. **\"08\"** → The month (August).\n",
      "5. **\"-\"** → Another separator.\n",
      "6. **\"22\"** → The day.\n",
      "\n",
      "This breakdown allows an NER model to recognize:\n",
      "- `\"BEIJING\"` as a **LOC** (Location) entity.\n",
      "- `\"1996-08-22\"` (or its components) as a **DATE** entity.\n",
      "\n",
      "If your NER model expects dates as a single token, you could alternatively use:\n",
      "```python\n",
      "[\"BEIJING\", \"1996-08-22\"]\n",
      "```\n",
      "But the first approach provides more granularity if needed. Choose based on your model's requirements.\n",
      "Bot: [('BEIJING', 'B-LOC'), ('1996', 'B-MISC'), ('-', 'O'), ('08', 'B-MISC'), ('-', 'O'), ('22', 'B-MISC')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"China\", \n",
      "    \"has\", \n",
      "    \"said\", \n",
      "    \"it\", \n",
      "    \"was\", \n",
      "    \"time\", \n",
      "    \"for\", \n",
      "    \"political\", \n",
      "    \"talks\", \n",
      "    \"with\", \n",
      "    \"Taiwan\", \n",
      "    \"and\", \n",
      "    \"that\", \n",
      "    \"the\", \n",
      "    \"rival\", \n",
      "    \"island\", \n",
      "    \"should\", \n",
      "    \"take\", \n",
      "    \"practical\", \n",
      "    \"steps\", \n",
      "    \"towards\", \n",
      "    \"that\", \n",
      "    \"goal\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like \".\") is treated as a separate token, as it may be relevant for NER boundaries.\n",
      "2. Named entities like \"China\" and \"Taiwan\" are preserved as single tokens (no splitting of proper nouns).\n",
      "3. Contractions (e.g., \"it's\") would be split if present, but this sentence has none.\n",
      "\n",
      "Let me know if you'd prefer a different tokenization approach (e.g., splitting hyphenated words, handling possessives, etc.).\n",
      "Bot: [('China', 'B-LOC'), ('has', 'O'), ('said', 'O'), ('it', 'O'), ('was', 'O'), ('time', 'O'), ('for', 'O'), ('political', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), ('and', 'O'), ('that', 'O'), ('the', 'O'), ('rival', 'O'), ('island', 'O'), ('should', 'O'), ('take', 'O'), ('practical', 'O'), ('steps', 'O'), ('towards', 'O'), ('that', 'O'), ('goal', 'O'), ('.', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Consultations\", \"should\", \"be\", \"held\", \"to\", \"set\", \"the\", \"time\", \"and\", \"format\", \n",
      "    \"of\", \"the\", \"talks\", \",\", \"the\", \"official\", \"Xinhua\", \"news\", \"agency\", \"quoted\", \n",
      "    \"Tang\", \"Shubei\", \",\", \"executive\", \"vice\", \"chairman\", \"of\", \"the\", \"Association\", \n",
      "    \"for\", \"Relations\", \"Across\", \"the\", \"Taiwan\", \"Straits\", \",\", \"as\", \"saying\", \n",
      "    \"late\", \"on\", \"Wednesday\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (`,`, `.`) is treated as separate tokens.\n",
      "2. Named entities like **\"Xinhua\"**, **\"Tang Shubei\"**, **\"Association for Relations Across the Taiwan Straits\"**, and **\"Wednesday\"** are split into sub-tokens where applicable (e.g., \"Tang\" and \"Shubei\" are separate tokens).\n",
      "3. This format is suitable for NER tasks where models like BERT or spaCy process tokens individually. \n",
      "\n",
      "Let me know if you'd like adjustments (e.g., merging multi-word entities like \"Taiwan Straits\").\n",
      "Bot: [('Consultations', 'O'), ('should', 'O'), ('be', 'O'), ('held', 'O'), ('to', 'O'), ('set', 'O'), ('the', 'O'), ('time', 'O'), ('and', 'O'), ('format', 'O'), ('of', 'O'), ('the', 'O'), ('talks', 'O'), (',', 'O'), ('the', 'O'), ('official', 'O'), ('Xinhua', 'B-ORG'), ('news', 'I-ORG'), ('agency', 'I-ORG'), ('quoted', 'O'), ('Tang', 'B-PER'), ('Shubei', 'I-PER'), (',', 'O'), ('executive', 'O'), ('vice', 'O'), ('chairman', 'O'), ('of', 'O'), ('the', 'O'), ('Association', 'B-ORG'), ('for', 'I-ORG'), ('Relations', 'I-ORG'), ('Across', 'I-ORG'), ('the', 'I-ORG'), ('Taiwan', 'B-LOC'), ('Straits', 'I-LOC'), (',', 'O'), ('as', 'O'), ('saying', 'O'), ('late', 'O'), ('on', 'O'), ('Wednesday', 'B-MISC'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"German\", \"July\", \"car\", \"registrations\", \"up\", \"14.2\", \"pct\", \"yr\", \"/\", \"yr\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **\"German\"**: Could be a nationality, language, or part of an organization name (e.g., \"German government\").\n",
      "- **\"July\"**: A month (temporal entity).\n",
      "- **\"car registrations\"**: A noun phrase (could be part of a larger entity).\n",
      "- **\"14.2 pct\"**: A percentage value (numeric entity).\n",
      "- **\"yr / yr\"**: Abbreviation for \"year over year\" (temporal comparison).\n",
      "- **\".\"**: Punctuation (often ignored in NER but included for completeness).\n",
      "\n",
      "This tokenization preserves meaningful units for NER, such as dates, numbers, and compound terms.\n",
      "Bot: [('German', 'B-MISC'), ('July', 'B-MISC'), ('car', 'O'), ('registrations', 'O'), ('up', 'O'), ('14.2', 'O'), ('pct', 'O'), ('yr', 'O'), ('/', 'O'), ('yr', 'O'), ('.', 'O')]\n",
      "Bot: Here is the tokenized version of the input sentence `FRANKFURT 1996-08-22`, broken down into tokens suitable for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\"FRANKFURT\", \"1996\", \"-\", \"08\", \"-\", \"22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"FRANKFURT\"** – A single token representing a location (likely a city).  \n",
      "2. **\"1996\"**, **\"08\"**, **\"22\"** – Numeric tokens representing the year, month, and day of a date.  \n",
      "3. **\"-\"** – Separators in the date format, treated as individual tokens to help the NER model recognize the date structure.  \n",
      "\n",
      "This breakdown ensures that the NER model can properly identify:\n",
      "- \"FRANKFURT\" as a **LOC** (location) entity.\n",
      "- The sequence \"1996 - 08 - 22\" as a **DATE** entity.  \n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., keeping \"1996-08-22\" as a single token).\n",
      "Bot: [('FRANKFURT', 'B-LOC'), ('1996', 'O'), ('-', 'O'), ('08', 'O'), ('-', 'O'), ('22', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"German\", \n",
      "    \"first-time\", \n",
      "    \"registrations\", \n",
      "    \"of\", \n",
      "    \"motor\", \n",
      "    \"vehicles\", \n",
      "    \"jumped\", \n",
      "    \"14.2\", \n",
      "    \"percent\", \n",
      "    \"in\", \n",
      "    \"July\", \n",
      "    \"this\", \n",
      "    \"year\", \n",
      "    \"from\", \n",
      "    \"the\", \n",
      "    \"year-earlier\", \n",
      "    \"period\", \n",
      "    \",\", \n",
      "    \"the\", \n",
      "    \"Federal\", \n",
      "    \"office\", \n",
      "    \"for\", \n",
      "    \"motor\", \n",
      "    \"vehicles\", \n",
      "    \"said\", \n",
      "    \"on\", \n",
      "    \"Thursday\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. **Punctuation**: Commas (`,`) and periods (`.`) are treated as separate tokens, as they may be relevant for NER (e.g., sentence boundaries).\n",
      "2. **Hyphenated words**: \"first-time\" and \"year-earlier\" are kept as single tokens, as splitting them might lose meaningful context.\n",
      "3. **Numbers**: \"14.2\" is kept as a single token (numeric entity).\n",
      "4. **Capitalization**: Retained as in the original text (e.g., \"Federal office\" may be part of an organization name).\n",
      "5. **Temporal expressions**: \"July\", \"Thursday\", and \"year\" are kept as separate tokens for potential date/time entity recognition.\n",
      "\n",
      "This tokenization is suitable for NER tasks where entities like organizations (\"Federal office for motor vehicles\"), dates (\"July\", \"Thursday\"), and numeric values (\"14.2 percent\") may need to be identified.\n",
      "Bot: [  \n",
      "    ('German', 'B-MISC'),  \n",
      "    ('first-time', 'O'),  \n",
      "    ('registrations', 'O'),  \n",
      "    ('of', 'O'),  \n",
      "    ('motor', 'O'),  \n",
      "    ('vehicles', 'O'),  \n",
      "    ('jumped', 'O'),  \n",
      "    ('14.2', 'O'),  \n",
      "    ('percent', 'O'),  \n",
      "    ('in', 'O'),  \n",
      "    ('July', 'B-MISC'),  \n",
      "    ('this', 'O'),  \n",
      "    ('year', 'O'),  \n",
      "    ('from', 'O'),  \n",
      "    ('the', 'O'),  \n",
      "    ('year-earlier', 'O'),  \n",
      "    ('period', 'O'),  \n",
      "    (',', 'O'),  \n",
      "    ('the', 'O'),  \n",
      "    ('Federal', 'B-ORG'),  \n",
      "    ('office', 'I-ORG'),  \n",
      "    ('for', 'I-ORG'),  \n",
      "    ('motor', 'I-ORG'),  \n",
      "    ('vehicles', 'I-ORG'),  \n",
      "    ('said', 'O'),  \n",
      "    ('on', 'O'),  \n",
      "    ('Thursday', 'B-MISC'),  \n",
      "    ('.', 'O')  \n",
      "]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"The\", \"office\", \"said\", \"356,725\", \"new\", \"cars\", \"were\", \"registered\", \n",
      "    \"in\", \"July\", \"1996\", \"--\", \"304,850\", \"passenger\", \"cars\", \"and\", \n",
      "    \"15,613\", \"trucks\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. **Numbers with commas**: \"356,725\", \"304,850\", and \"15,613\" are kept intact as single tokens because they represent distinct numerical values.\n",
      "2. **Punctuation**: Hyphens (\"--\") and periods (\".\") are treated as separate tokens.\n",
      "3. **Dates**: \"July\" and \"1996\" are split into separate tokens (month and year).\n",
      "4. **NER Relevance**: This tokenization preserves entities like dates (\"July 1996\"), quantities (\"356,725\"), and vehicle types (\"passenger cars\", \"trucks\") for potential labeling.\n",
      "\n",
      "Let me know if you'd like adjustments (e.g., splitting hyphenated phrases or merging multi-word entities).\n",
      "Bot: [('The', 'O'), ('office', 'O'), ('said', 'O'), ('356,725', 'O'), ('new', 'O'), ('cars', 'O'), ('were', 'O'), ('registered', 'O'), ('in', 'O'), ('July', 'B-MISC'), ('1996', 'I-MISC'), ('--', 'O'), ('304,850', 'O'), ('passenger', 'O'), ('cars', 'O'), ('and', 'O'), ('15,613', 'O'), ('trucks', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"The\", \n",
      "    \"figures\", \n",
      "    \"represent\", \n",
      "    \"a\", \n",
      "    \"13.6\", \n",
      "    \"percent\", \n",
      "    \"increase\", \n",
      "    \"for\", \n",
      "    \"passenger\", \n",
      "    \"cars\", \n",
      "    \"and\", \n",
      "    \"a\", \n",
      "    \"2.2\", \n",
      "    \"percent\", \n",
      "    \"decline\", \n",
      "    \"for\", \n",
      "    \"trucks\", \n",
      "    \"from\", \n",
      "    \"July\", \n",
      "    \"1995\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "- Punctuation (like \".\") is treated as a separate token.\n",
      "- Numbers with decimals (\"13.6\", \"2.2\") are kept as single tokens.\n",
      "- Multi-word terms (\"passenger cars\") are split into individual tokens unless they form a named entity (e.g., \"New York\" would ideally be kept together if it were in the text).\n",
      "- \"July 1995\" could be considered a date entity, but for tokenization, it's split into [\"July\", \"1995\"].\n",
      "\n",
      "This tokenization is suitable for NER, where the model can later group tokens into entities (e.g., \"July 1995\" as a DATE, \"13.6 percent\" as a PERCENT, etc.).\n",
      "Bot: [('The', 'O'), ('figures', 'O'), ('represent', 'O'), ('a', 'O'), ('13.6', 'O'), ('percent', 'O'), ('increase', 'O'), ('for', 'O'), ('passenger', 'O'), ('cars', 'O'), ('and', 'O'), ('a', 'O'), ('2.2', 'O'), ('percent', 'O'), ('decline', 'O'), ('for', 'O'), ('trucks', 'O'), ('from', 'O'), ('July', 'B-MISC'), ('1995', 'I-MISC'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"Motor-bike\", \n",
      "    \"registration\", \n",
      "    \"rose\", \n",
      "    \"32.7\", \n",
      "    \"percent\", \n",
      "    \"in\", \n",
      "    \"the\", \n",
      "    \"period\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **\"Motor-bike\"** is kept as a single token because it is a hyphenated compound word (though there's a typo in your input: \"Motor-bike\" instead of \"Motor-bike\").\n",
      "2. **\"32.7\"** is treated as a single token (numeric value with a decimal point).  \n",
      "3. Punctuation (**\".\"**) is separated as its own token, as is common in NER tasks.  \n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., splitting hyphenated words or handling numbers differently).\n",
      "Bot: [('Motor-bike', 'O'), ('registration', 'O'), ('rose', 'O'), ('32.7', 'O'), ('percent', 'O'), ('in', 'O'), ('the', 'O'), ('period', 'O'), ('.', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"The\", \"growth\", \"was\", \"partly\", \"due\", \"to\", \"an\", \"increased\", \n",
      "    \"number\", \"of\", \"Germans\", \"buying\", \"German\", \"cars\", \"abroad\", \n",
      "    \",\", \"while\", \"manufacturers\", \"said\", \"that\", \"domestic\", \n",
      "    \"demand\", \"was\", \"weak\", \",\", \"the\", \"federal\", \"office\", \"said\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation marks (`,`, `.`) are treated as separate tokens, as they can be relevant for NER (e.g., marking sentence boundaries or clauses).\n",
      "2. \"Germans\" and \"German\" are kept as separate tokens to allow the NER model to identify them as potential entities (e.g., \"Germans\" as a nationality or group, \"German\" as an adjective or demonym).\n",
      "3. \"federal\" is corrected to \"federal\" (assuming the original sentence had a typo). If the original was intentional, keep it as \"federal.\"\n",
      "\n",
      "Let me know if you'd like any adjustments!\n",
      "Bot: [('The', 'O'), ('growth', 'O'), ('was', 'O'), ('partly', 'O'), ('due', 'O'), ('to', 'O'), ('an', 'O'), ('increased', 'O'), ('number', 'O'), ('of', 'O'), ('Germans', 'B-MISC'), ('buying', 'O'), ('German', 'B-MISC'), ('cars', 'O'), ('abroad', 'O'), (',', 'O'), ('while', 'O'), ('manufacturers', 'O'), ('said', 'O'), ('that', 'O'), ('domestic', 'O'), ('demand', 'O'), ('was', 'O'), ('weak', 'O'), (',', 'O'), ('the', 'O'), ('federal', 'O'), ('office', 'O'), ('said', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"Almost\", \"all\", \"German\", \"car\", \"manufacturers\", \n",
      "    \"posted\", \"gains\", \"in\", \"registration\", \"numbers\", \n",
      "    \"in\", \"the\", \"period\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like \".\") is treated as a separate token, as it can be relevant for NER (e.g., marking the end of an entity).\n",
      "2. Words are split on whitespace and punctuation boundaries.\n",
      "3. \"German\" is kept as a single token since it could be part of a named entity (e.g., \"German car manufacturers\" as an ORG or MISC entity).\n",
      "4. Hyphenated words or contractions would be split further if present (e.g., \"state-of-the-art\" → [\"state\", \"-\", \"of\", \"-\", \"the\", \"-\", \"art\"]).\n",
      "\n",
      "Let me know if you'd like a different tokenization approach!\n",
      "Bot: [('Almost', 'O'), ('all', 'O'), ('German', 'B-MISC'), ('car', 'O'), ('manufacturers', 'O'), ('posted', 'O'), ('gains', 'O'), ('in', 'O'), ('registration', 'O'), ('numbers', 'O'), ('in', 'O'), ('the', 'O'), ('period', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Volkswagen\", \"AG\", \"won\", \"77,719\", \"registrations\", \",\", \n",
      "    \"slightly\", \"more\", \"than\", \"a\", \"quarter\", \"of\", \"the\", \"total\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. **\"Volkswagen AG\"** is split into `\"Volkswagen\"` and `\"AG\"` since NER models often treat company names and legal suffixes (like \"AG\") as separate tokens.\n",
      "2. **\"77,719\"** is kept as a single token because it represents a numeric value.\n",
      "3. Punctuation (`,` and `.`) is separated into individual tokens, as is standard in NER tokenization.\n",
      "4. Contractions or hyphenated words are not present here, but if they were, they would typically be split (e.g., \"New York-based\" → `\"New\"`, `\"York\"`, `\"-\"`, `\"based\"`).\n",
      "\n",
      "This format is compatible with common NER frameworks like spaCy, Hugging Face Transformers, or Stanford NER. Let me know if you'd like further adjustments!\n",
      "Bot: [('Volkswagen', 'B-ORG'), ('AG', 'I-ORG'), ('won', 'O'), ('77,719', 'O'), ('registrations', 'O'), (',', 'O'), ('slightly', 'O'), ('more', 'O'), ('than', 'O'), ('a', 'O'), ('quarter', 'O'), ('of', 'O'), ('the', 'O'), ('total', 'O'), ('.', 'O')]\n",
      "Bot: Here is the tokenized version of the given sentence, suitable for a named entity recognition (NER) task:  \n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Opel\", \"AG\", \"together\", \"with\", \"General\", \"Motors\", \"came\", \"in\", \"second\", \"place\", \n",
      "    \"with\", \"49,269\", \"registrations\", \",\", \"16.4\", \"percent\", \"of\", \"the\", \"overall\", \"figure\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. **Punctuation Handling**: Commas (`,`) and periods (`.`) are treated as separate tokens since they can be relevant for NER (e.g., marking the end of an entity span).  \n",
      "2. **Numbers and Units**:  \n",
      "   - `49,269` is kept as a single token (numeric value).  \n",
      "   - `16.4` is treated as one token (decimal number).  \n",
      "3. **Multi-word Entities**:  \n",
      "   - \"General Motors\" is split into `\"General\"` and `\"Motors\"` but may be labeled as a single entity in NER (e.g., `ORG`).  \n",
      "   - \"Opel AG\" is split into `\"Opel\"` and `\"AG\"` (likely part of the same organization).  \n",
      "\n",
      "Let me know if you'd prefer a different tokenization strategy (e.g., keeping \"General Motors\" as one token).\n",
      "Bot: [('Opel', 'B-ORG'), ('AG', 'I-ORG'), ('together', 'O'), ('with', 'O'), ('General', 'B-ORG'), ('Motors', 'I-ORG'), ('came', 'O'), ('in', 'O'), ('second', 'O'), ('place', 'O'), ('with', 'O'), ('49,269', 'O'), ('registrations', 'O'), (',', 'O'), ('16.4', 'O'), ('percent', 'O'), ('of', 'O'), ('the', 'O'), ('overall', 'O'), ('figure', 'O'), ('.', 'O')]\n",
      "Bot: Here is the tokenized version of the given sentence, broken down into tokens suitable for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Third\", \n",
      "    \"was\", \n",
      "    \"Ford\", \n",
      "    \"with\", \n",
      "    \"35,563\", \n",
      "    \"registrations\", \n",
      "    \",\", \n",
      "    \"or\", \n",
      "    \"11.7\", \n",
      "    \"percent\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- Punctuation (`,`, `.`) is separated as individual tokens, as they are often treated as separate entities in NER.\n",
      "- Numbers with commas (`35,563`) and decimals (`11.7`) are kept as single tokens because they represent unified numerical values.\n",
      "- Words are split into individual tokens (e.g., \"Ford\" as a company name is a single token for NER labeling).\n",
      "\n",
      "This tokenization preserves the structure needed for NER, where entities like \"Ford\" (ORG), \"35,563\" (CARDINAL), and \"11.7 percent\" (PERCENT) might be labeled.\n",
      "Bot: [('Third', 'O'), ('was', 'O'), ('Ford', 'B-ORG'), ('with', 'O'), ('35,563', 'O'), ('registrations', 'O'), (',', 'O'), ('or', 'O'), ('11.7', 'O'), ('percent', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"Only\", \"Seat\", \"and\", \"Porsche\", \"had\", \"fewer\", \n",
      "    \"registrations\", \"in\", \"July\", \"1996\", \"compared\", \n",
      "    \"to\", \"last\", \"year\", \"'s\", \"July\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **Punctuation Handling**: The apostrophe in `'s` is kept as part of the token (common in NER to preserve context). The period at the end is treated as a separate token.\n",
      "2. **Entities of Interest**: \n",
      "   - \"Seat\" and \"Porsche\" (car brands, likely `ORG` or `MISC` in NER).\n",
      "   - \"July 1996\" (date, `DATE`).\n",
      "   - \"last year\" (temporal expression, `DATE` or `TIME`).\n",
      "3. **Lowercasing**: Not applied here, as case can be important for NER (e.g., \"Porsche\" is a proper noun).\n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., splitting \"July 1996\" into separate tokens).\n",
      "Bot: [('Only', 'O'), ('Seat', 'B-ORG'), ('and', 'O'), ('Porsche', 'B-ORG'), ('had', 'O'), ('fewer', 'O'), ('registrations', 'O'), ('in', 'O'), ('July', 'B-MISC'), ('1996', 'I-MISC'), ('compared', 'O'), ('to', 'O'), ('last', 'O'), ('year', 'O'), (\"'s\", 'O'), ('July', 'B-MISC'), ('.', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"Seat\", \"posted\", \"3,420\", \"registrations\", \"compared\", \"with\", \n",
      "    \"5522\", \"registrations\", \"in\", \"July\", \"a\", \"year\", \"earlier\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- Punctuation (like `.`) is treated as a separate token.\n",
      "- Numbers with commas (e.g., `3,420`) are kept as single tokens.\n",
      "- Words are split based on whitespace and punctuation boundaries.\n",
      "- Contractions or hyphenated terms are not present here, but if they were, they could be split further depending on the NER model's requirements.\n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., splitting numbers like `3,420` into `[\"3\", \",\", \"420\"]` or handling abbreviations differently).\n",
      "Bot: [('Seat', 'B-ORG'), ('posted', 'O'), ('3,420', 'O'), ('registrations', 'O'), ('compared', 'O'), ('with', 'O'), ('5522', 'O'), ('registrations', 'O'), ('in', 'O'), ('July', 'B-MISC'), ('a', 'O'), ('year', 'O'), ('earlier', 'O'), ('.', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"Porsche\", \"'s\", \"registrations\", \"fell\", \"to\", \"554\", \"from\", \"643\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"Porsche\"** - Proper noun (likely an organization or brand).\n",
      "2. **\"'s\"** - Possessive marker (often kept as a separate token).\n",
      "3. **\"registrations\"** - Common noun.\n",
      "4. **\"fell\"** - Verb.\n",
      "5. **\"to\"** - Preposition.\n",
      "6. **\"554\"** - Numeric value (could be a quantity or other entity).\n",
      "7. **\"from\"** - Preposition.\n",
      "8. **\"643\"** - Numeric value.\n",
      "9. **\".\"** - Punctuation (often kept as a separate token for NER).\n",
      "\n",
      "This tokenization preserves meaningful units for NER, such as:\n",
      "- Named entities (\"Porsche\"),\n",
      "- Numeric values (\"554\", \"643\"),\n",
      "- Context words (\"fell\", \"registrations\"). \n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., splitting contractions or handling numbers differently).\n",
      "Bot: [('Porsche', 'B-ORG'), (\"'s\", 'O'), ('registrations', 'O'), ('fell', 'O'), ('to', 'O'), ('554', 'O'), ('from', 'O'), ('643', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"GREEK\", \"SOCIALISTS\", \"GIVE\", \"GREEN\", \"LIGHT\", \"TO\", \"PM\", \"FOR\", \"ELECTIONS\", \".\"]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **\"PM\"** is kept as a single token since it likely refers to \"Prime Minister\" (a named entity in this context).  \n",
      "2. Punctuation (**\".\"**) is treated as a separate token, as is common in NER tasks.  \n",
      "3. All words are uppercase (as in the input), but depending on the NER model, case normalization (e.g., lowercasing) might be applied later.  \n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., splitting \"PM\" further, handling contractions, etc.).\n",
      "Bot: [('GREEK', 'B-MISC'), ('SOCIALISTS', 'B-ORG'), ('GIVE', 'O'), ('GREEN', 'O'), ('LIGHT', 'O'), ('TO', 'O'), ('PM', 'B-PER'), ('FOR', 'O'), ('ELECTIONS', 'O'), ('.', 'O')]\n",
      "Bot: Here is the tokenized version of the sentence \"ATHENS 1996-08-22\" for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\"ATHENS\", \"1996\", \"-\", \"08\", \"-\", \"22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"ATHENS\"** – A proper noun (likely a **location** or **organization** in NER).\n",
      "2. **\"1996\"** – A year (likely a **date** or **time** component in NER).\n",
      "3. **\"-\"** – A separator (often part of a date format).\n",
      "4. **\"08\"** – A month (likely part of a **date** entity).\n",
      "5. **\"-\"** – Another separator.\n",
      "6. **\"22\"** – A day (likely part of a **date** entity).\n",
      "\n",
      "### Alternative (if keeping the full date as one token):\n",
      "If the NER model is trained to recognize full dates as single entities, you might prefer:\n",
      "```python\n",
      "[\"ATHENS\", \"1996-08-22\"]\n",
      "```\n",
      "\n",
      "Would you like any adjustments based on your specific NER model's requirements?\n",
      "Bot: [('ATHENS', 'B-LOC'), ('1996', 'O'), ('-', 'O'), ('08', 'O'), ('-', 'O'), ('22', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"The\", \"Greek\", \"socialist\", \"party\", \"'s\", \"executive\", \"bureau\", \"gave\", \n",
      "    \"the\", \"green\", \"light\", \"to\", \"Prime\", \"Minister\", \"Costas\", \"Simitis\", \n",
      "    \"to\", \"call\", \"snap\", \"elections\", \",\", \"its\", \"general\", \"secretary\", \n",
      "    \"Costas\", \"Skandalidis\", \"told\", \"reporters\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like `,` and `.`) is treated as separate tokens.  \n",
      "2. Possessive `'s` is kept as a single token.  \n",
      "3. Multi-word named entities (e.g., \"Prime Minister\", \"Costas Simitis\") are split into individual tokens, but the NER model may later group them into a single entity.  \n",
      "4. All tokens are lowercase except for proper nouns (e.g., \"Greek\", \"Costas\", \"Skandalidis\").  \n",
      "\n",
      "Let me know if you'd like any adjustments (e.g., keeping \"Prime Minister\" as one token).\n",
      "Bot: [('The', 'O'), ('Greek', 'B-MISC'), ('socialist', 'O'), ('party', 'O'), (\"'s\", 'O'), ('executive', 'O'), ('bureau', 'O'), ('gave', 'O'), ('the', 'O'), ('green', 'O'), ('light', 'O'), ('to', 'O'), ('Prime', 'B-ORG'), ('Minister', 'I-ORG'), ('Costas', 'B-PER'), ('Simitis', 'I-PER'), ('to', 'O'), ('call', 'O'), ('snap', 'O'), ('elections', 'O'), (',', 'O'), ('its', 'O'), ('general', 'O'), ('secretary', 'O'), ('Costas', 'B-PER'), ('Skandalidis', 'I-PER'), ('told', 'O'), ('reporters', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Prime\", \n",
      "    \"Minister\", \n",
      "    \"Costas\", \n",
      "    \"Simitis\", \n",
      "    \"is\", \n",
      "    \"going\", \n",
      "    \"to\", \n",
      "    \"make\", \n",
      "    \"an\", \n",
      "    \"official\", \n",
      "    \"announcement\", \n",
      "    \"after\", \n",
      "    \"a\", \n",
      "    \"cabinet\", \n",
      "    \"meeting\", \n",
      "    \"later\", \n",
      "    \"on\", \n",
      "    \"Thursday\", \n",
      "    \",\", \n",
      "    \"said\", \n",
      "    \"Skandalidis\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **Punctuation Handling**: Commas (\",\") and periods (\".\") are treated as separate tokens, as they may be relevant for sentence boundaries or context in NER.\n",
      "2. **Named Entities**: Multi-word entities like \"Prime Minister\" are split into individual tokens, but the NER model may later group them (e.g., \"Prime Minister\" as one entity, \"Costas Simitis\" as another).\n",
      "3. **Capitalization**: Preserved (e.g., \"Costas\", \"Simitis\", \"Thursday\") as it often signals proper nouns in NER tasks.\n",
      "\n",
      "This tokenization is suitable for standard NER pipelines (e.g., spaCy, Hugging Face Transformers). If you need subword tokenization (e.g., BERT-style), let me know!\n",
      "Bot: [('Prime', 'B-MISC'), ('Minister', 'I-MISC'), ('Costas', 'B-PER'), ('Simitis', 'I-PER'), ('is', 'O'), ('going', 'O'), ('to', 'O'), ('make', 'O'), ('an', 'O'), ('official', 'O'), ('announcement', 'O'), ('after', 'O'), ('a', 'O'), ('cabinet', 'B-ORG'), ('meeting', 'I-ORG'), ('later', 'O'), ('on', 'O'), ('Thursday', 'B-MISC'), (',', 'O'), ('said', 'O'), ('Skandalidis', 'B-PER'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"--\", \n",
      "    \"Dimitris\", \n",
      "    \"Kontogiannis\", \n",
      "    \",\", \n",
      "    \"Athens\", \n",
      "    \"Newsroom\", \n",
      "    \"+301\", \n",
      "    \"3311812-4\", \n",
      "    \"--\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"--\"** is treated as a separate token (often used as a delimiter or separator).  \n",
      "2. **\"Dimitris\"** and **\"Kontogiannis\"** are split as separate tokens (first and last name).  \n",
      "3. **\",\"** is a punctuation token.  \n",
      "4. **\"Athens\"** and **\"Newsroom\"** are split as location and organizational context.  \n",
      "5. **\"+301\"** is a country/city phone code.  \n",
      "6. **\"3311812-4\"** is a phone number with an extension.  \n",
      "7. **\"--\"** closes the delimiter.  \n",
      "\n",
      "This tokenization preserves meaningful units for NER (e.g., person names, locations, phone numbers). If further granularity is needed (e.g., splitting the phone number into parts), let me know!\n",
      "Bot: ```python\n",
      "[\n",
      "    (\"--\", \"O\"),\n",
      "    (\"Dimitris\", \"B-PER\"),\n",
      "    (\"Kontogiannis\", \"I-PER\"),\n",
      "    (\",\", \"O\"),\n",
      "    (\"Athens\", \"B-LOC\"),\n",
      "    (\"Newsroom\", \"B-ORG\"),\n",
      "    (\"+301\", \"O\"),\n",
      "    (\"3311812-4\", \"O\"),\n",
      "    (\"--\", \"O\")\n",
      "]\n",
      "```\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"BayerVB\", \"sets\", \"C$\", \"100\", \"million\", \"six-year\", \"bond\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"BayerVB\"** – Likely a company or organization name (potential named entity).  \n",
      "2. **\"sets\"** – Verb (not typically an entity).  \n",
      "3. **\"C$\"** – Currency symbol for Canadian dollars (entity type: MONEY).  \n",
      "4. **\"100\"** – Numeric value (part of the monetary amount).  \n",
      "5. **\"million\"** – Unit (part of the monetary amount).  \n",
      "6. **\"six-year\"** – Adjective describing the bond's maturity (not an entity).  \n",
      "7. **\"bond\"** – Financial instrument (potential entity type: SECURITY).  \n",
      "8. **\".\"** – Punctuation (not an entity).  \n",
      "\n",
      "This tokenization preserves meaningful units for NER, such as:\n",
      "- **\"C$ 100 million\"** as a monetary value (split into sub-tokens for granularity).  \n",
      "- **\"BayerVB\"** as a potential organization.  \n",
      "- **\"bond\"** as a financial term.  \n",
      "\n",
      "Let me know if you'd prefer a different tokenization strategy (e.g., keeping \"six-year\" as two tokens).\n",
      "Bot: [('BayerVB', 'B-ORG'), ('sets', 'O'), ('C$', 'O'), ('100', 'O'), ('million', 'O'), ('six-year', 'O'), ('bond', 'O'), ('.', 'O')]\n",
      "Bot: Here’s the tokenized breakdown of the input sentence `\"LONDON 1996-08-22\"` for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\"LONDON\", \"1996\", \"-\", \"08\", \"-\", \"22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"LONDON\"** → Standalone word (likely a `LOCATION` or `GPE` entity in NER).\n",
      "2. **\"1996\"**, **\"08\"**, **\"22\"** → Components of the date (likely a `DATE` entity). \n",
      "   - The hyphens (\"-\") are separated as individual tokens to allow flexibility in parsing (e.g., some NER models may treat the full date as a single token, but splitting ensures compatibility with systems that tokenize punctuation separately).\n",
      "\n",
      "### Alternative (if the date should be a single token):\n",
      "If your NER task expects the full date as one entity, you could use:\n",
      "```python\n",
      "[\"LONDON\", \"1996-08-22\"]\n",
      "```\n",
      "This depends on your tokenization strategy. For most NER systems, the first approach (splitting numbers and punctuation) is more common.\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996', 'O'), ('-', 'O'), ('08', 'O'), ('-', 'O'), ('22', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"The\", \"following\", \"bond\", \"was\", \"announced\", \"by\", \"lead\", \"manager\", \"Toronto\", \"Dominion\", \".\"]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (`.`) is treated as a separate token, as it may be relevant for NER (e.g., sentence boundaries).  \n",
      "2. \"Toronto Dominion\" is split into two tokens (`\"Toronto\"`, `\"Dominion\"`) since NER models typically process individual words but can still recognize multi-word entities through context.  \n",
      "3. All tokens are lowercase except for proper nouns (\"Toronto Dominion\"), as capitalization is often important for NER.  \n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., keeping contractions intact, handling hyphenated words differently, etc.).\n",
      "Bot: [('The', 'O'), ('following', 'O'), ('bond', 'O'), ('was', 'O'), ('announced', 'O'), ('by', 'O'), ('lead', 'O'), ('manager', 'O'), ('Toronto', 'B-ORG'), ('Dominion', 'I-ORG'), ('.', 'O')]\n",
      "Bot: To tokenize the given sentence for a named entity recognition (NER) task, we'll break it down into meaningful units (tokens) that could represent entities or parts of entities. Here's the tokenized output:\n",
      "\n",
      "```python\n",
      "[\"BORROWER\", \"BAYERISCHE\", \"VEREINSBANK\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"BORROWER\"** is treated as a single token (likely a role or label in this context).\n",
      "2. **\"BAYERISCHE\"** is a distinct token (part of the bank's name, derived from \"Bavaria\" in German).\n",
      "3. **\"VEREINSBANK\"** is another token (German for \"association bank,\" completing the entity name).\n",
      "\n",
      "This breakdown preserves the semantic units for NER, where \"BAYERISCHE VEREINSBANK\" could be tagged as an organization (ORG) and \"BORROWER\" as a role or descriptor. \n",
      "\n",
      "### Alternative (if punctuation is significant):\n",
      "If the `--` is meaningful (e.g., a separator), you might split further:\n",
      "```python\n",
      "[\"BORROWER\", \"--\", \"BAYERISCHE\", \"VEREINSBANK\"]\n",
      "```\n",
      "Let me know if you'd prefer a different approach!\n",
      "Bot: [('BORROWER', 'O'), ('BAYERISCHE', 'B-ORG'), ('VEREINSBANK', 'I-ORG')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"AMT\", \n",
      "    \"C$\", \n",
      "    \"100\", \n",
      "    \"MLN\", \n",
      "    \"COUPON\", \n",
      "    \"6.625\", \n",
      "    \"MATURITY\", \n",
      "    \"24.SEP.02\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation of Tokenization:\n",
      "1. **\"AMT\"** - Likely an abbreviation for \"amount\" (financial context).\n",
      "2. **\"C$\"** - Currency symbol for Canadian dollars.\n",
      "3. **\"100\"** - Numeric value (amount).\n",
      "4. **\"MLN\"** - Abbreviation for \"million\".\n",
      "5. **\"COUPON\"** - Financial term referring to the interest rate of a bond.\n",
      "6. **\"6.625\"** - Numeric value (coupon rate in percentage).\n",
      "7. **\"MATURITY\"** - Financial term referring to the maturity date of a bond.\n",
      "8. **\"24.SEP.02\"** - Date in the format `DD.MMM.YY` (24th September 2002).\n",
      "\n",
      "### Why This Tokenization?\n",
      "- Punctuation like `.` is kept as part of the token (e.g., \"24.SEP.02\" is treated as a single date token).  \n",
      "- Currency symbols (\"C$\") are kept with their associated symbols for NER.  \n",
      "- Financial abbreviations (\"MLN\", \"COUPON\") are treated as standalone tokens.  \n",
      "\n",
      "This tokenization is suitable for NER tasks where entities like:\n",
      "- Monetary amounts (`C$ 100 MLN`),  \n",
      "- Rates (`6.625`),  \n",
      "- Dates (`24.SEP.02`),  \n",
      "- Financial terms (`COUPON`, `MATURITY`)  \n",
      "need to be identified.  \n",
      "\n",
      "Would you like any refinements based on a specific NER labeling scheme?\n",
      "Bot: [('AMT', 'O'), ('C$', 'B-MISC'), ('100', 'I-MISC'), ('MLN', 'I-MISC'), ('COUPON', 'O'), ('6.625', 'B-MISC'), ('MATURITY', 'O'), ('24.SEP.02', 'B-MISC')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"TYPE\", \n",
      "    \"STRAIGHT\", \n",
      "    \"ISS\", \n",
      "    \"PRICE\", \n",
      "    \"100.92\", \n",
      "    \"PAY\", \n",
      "    \"DATE\", \n",
      "    \"24.SEP.96\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. \"STRAIGHT\" appears to be a typo (likely intended as \"STRAIGHT\" or \"STRAIGHT\"). I've kept it as-is for NER processing.  \n",
      "2. \"100.92\" is treated as a single numeric token.  \n",
      "3. \"24.SEP.96\" is treated as a single date token (common format in financial contexts).  \n",
      "\n",
      "For NER, these tokens might be labeled as:  \n",
      "- **\"TYPE\"**, **\"ISS\"**, **\"PRICE\"**, **\"PAY\"**, **\"DATE\"** → Could be tagged as `O` (other) or domain-specific labels (e.g., `FIELD`).  \n",
      "- **\"100.92\"** → Likely `MONEY` or `NUMERIC`.  \n",
      "- **\"24.SEP.96\"** → `DATE`.  \n",
      "\n",
      "Let me know if you'd like adjustments (e.g., splitting \"24.SEP.96\" further).\n",
      "Bot: [('TYPE', 'O'), ('STRAIGHT', 'O'), ('ISS', 'B-ORG'), ('PRICE', 'O'), ('100.92', 'O'), ('PAY', 'O'), ('DATE', 'O'), ('24.SEP.96', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"FULL\", \n",
      "    \"FEES\", \n",
      "    \"1.875\", \n",
      "    \"REOFFER\", \n",
      "    \"99.32\", \n",
      "    \"SPREAD\", \n",
      "    \"+\", \n",
      "    \"20\", \n",
      "    \"BP\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"FULL\"**, **\"FEES\"**, **\"REOFFER\"**, **\"SPREAD\"**, **\"BP\"** are treated as standalone words/abbreviations.\n",
      "2. **\"1.875\"** and **\"99.32\"** are numeric values (likely representing percentages or amounts) and kept as single tokens.\n",
      "3. **\"+20 BP\"** is split into:\n",
      "   - **\"+\"** (a standalone symbol indicating a positive spread),\n",
      "   - **\"20\"** (numeric value),\n",
      "   - **\"BP\"** (abbreviation for \"basis points\").\n",
      "\n",
      "This tokenization preserves meaningful units for NER (e.g., monetary values, spreads, and financial terms). Let me know if you'd like adjustments!\n",
      "Bot: [('FULL', 'O'), ('FEES', 'O'), ('1.875', 'O'), ('REOFFER', 'O'), ('99.32', 'O'), ('SPREAD', 'O'), ('+', 'O'), ('20', 'O'), ('BP', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"MOODY\", \"AA1\", \"LISTING\", \"LUX\", \"PAY\", \"FREQ\", \"=\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- Each word or alphanumeric sequence is treated as a separate token.  \n",
      "- Punctuation (`=`) is also split as a separate token if it’s meaningful for the NER task.  \n",
      "- No lowercase conversion is applied (common in NER to preserve entity casing, e.g., \"MOODY\" could be an organization).  \n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., splitting \"AA1\" into \"AA\" and \"1\" or handling symbols differently).\n",
      "Bot: [('MOODY', 'B-ORG'), ('AA1', 'I-ORG'), ('LISTING', 'O'), ('LUX', 'B-MISC'), ('PAY', 'O'), ('FREQ', 'O'), ('=', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"S&P\", \"=\", \"DENOMS\", \"(\", \"K\", \")\", \"1\", \"-\", \"10\", \"-\", \"100\", \n",
      "    \"SALE\", \"LIMITS\", \"US\", \"/\", \"UK\", \"/\", \"CA\", \"--\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation of Tokenization:\n",
      "1. **\"S&P\"** – Kept as a single token (financial index symbol).  \n",
      "2. **\"=\"** – Mathematical operator.  \n",
      "3. **\"DENOMS\"** – Likely an abbreviation (short for \"denominations\").  \n",
      "4. **\"(\", \"K\", \")\"** – Parentheses and the letter \"K\" (could represent \"thousand\").  \n",
      "5. **\"1\", \"-\", \"10\", \"-\", \"100\"** – Numbers and hyphens (representing a range).  \n",
      "6. **\"SALE\", \"LIMITS\"** – Separate words (business terms).  \n",
      "7. **\"US\", \"/\", \"UK\", \"/\", \"CA\"** – Country codes and slashes (separate entities).  \n",
      "8. **\"--\"** – Punctuation (often used as a separator).  \n",
      "\n",
      "This tokenization preserves meaningful entities (like \"S&P\", country codes, and numbers) while breaking down symbols and punctuation for NER processing.  \n",
      "\n",
      "Would you like any refinements based on a specific NER model's requirements?\n",
      "Bot: [('S&P', 'B-ORG'), ('=', 'O'), ('DENOMS', 'B-MISC'), ('(', 'O'), ('K', 'B-MISC'), (')', 'O'), ('1', 'O'), ('-', 'O'), ('10', 'O'), ('-', 'O'), ('100', 'O'), ('SALE', 'O'), ('LIMITS', 'O'), ('US', 'B-LOC'), ('/', 'O'), ('UK', 'B-LOC'), ('/', 'O'), ('CA', 'B-LOC'), ('--', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:  \n",
      "\n",
      "```python\n",
      "tokens = [\"NEG\", \"PLG\", \"NO\", \"CRS\", \"DEFLT\", \"NO\", \"FORCE\", \"MAJ\", \"=\"]\n",
      "```\n",
      "\n",
      "### Explanation:  \n",
      "- Each abbreviation or symbol is treated as a separate token.  \n",
      "- The `\"=\"` at the end is also considered a token.  \n",
      "- This format is suitable for NER tasks where each token may be labeled with an entity type (e.g., \"NEG\" could be labeled as `Negation`, \"PLG\" as `Policy`, etc.).  \n",
      "\n",
      "Let me know if you need further processing (e.g., splitting into subwords for BERT-based models).\n",
      "Bot: [('NEG', 'O'), ('PLG', 'O'), ('NO', 'O'), ('CRS', 'O'), ('DEFLT', 'O'), ('NO', 'O'), ('FORCE', 'O'), ('MAJ', 'O'), ('=', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens as a list:\n",
      "\n",
      "```python\n",
      "[\"GOV\", \"LAW\", \"GERMAN\", \"HOME\", \"CTRY\", \"=\", \"TAX\", \"PROVS\", \"STANDARD\"]\n",
      "```\n",
      "\n",
      "Each token is separated by whitespace or the equals sign (`=`), which is treated as a separate token. This list can now be used for named entity recognition (NER) tasks. Let me know if you'd like any modifications!\n",
      "Bot: [('GOV', 'B-ORG'), ('LAW', 'I-ORG'), ('GERMAN', 'B-LOC'), ('HOME', 'O'), ('CTRY', 'O'), ('=', 'O'), ('TAX', 'B-MISC'), ('PROVS', 'I-MISC'), ('STANDARD', 'I-MISC')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"MGT\", \"/\", \"UND\", \"0.275\", \"SELL\", \"CONC\", \"1.60\", \"PRAECIP\", \"=\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"MGT\"** – Likely an abbreviation or acronym.  \n",
      "2. **\"/\"** – A symbol (slash).  \n",
      "3. **\"UND\"** – Could be an abbreviation (e.g., \"undiluted\").  \n",
      "4. **\"0.275\"** – A numeric value (possibly a concentration or ratio).  \n",
      "5. **\"SELL\"** – A verb or abbreviation (e.g., \"sell\" or short for something).  \n",
      "6. **\"CONC\"** – Likely short for \"concentration.\"  \n",
      "7. **\"1.60\"** – Another numeric value.  \n",
      "8. **\"PRAECIP\"** – Possibly Latin or an abbreviation (e.g., \"praecipitatus\" meaning precipitate).  \n",
      "9. **\"=\"** – An equality symbol.  \n",
      "\n",
      "This tokenization preserves all meaningful components for NER, where entities like chemicals, measurements, and abbreviations may need classification. Let me know if you'd like any refinements!\n",
      "Bot: [('MGT', 'B-ORG'), ('/', 'O'), ('UND', 'O'), ('0.275', 'O'), ('SELL', 'O'), ('CONC', 'O'), ('1.60', 'O'), ('PRAECIP', 'O'), ('=', 'O')]\n",
      "Bot: Here’s the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"UNDERLYING\", \"GOVT\", \"BOND\", \"7.0\", \"PCT\", \"SEPT\", \"2001\"]\n",
      "```\n",
      "\n",
      "### Explanation of tokenization:\n",
      "1. **\"UNDERLYING\"** – Standalone word (likely an adjective or descriptor).  \n",
      "2. **\"GOVT\"** – Abbreviation for \"government\" (entity type: ORG or MISC).  \n",
      "3. **\"BOND\"** – Financial instrument (entity type: FINANCIAL_PRODUCT).  \n",
      "4. **\"7.0\"** – Numerical value (entity type: PERCENTAGE or NUMBER, depending on context).  \n",
      "5. **\"PCT\"** – Abbreviation for \"percent\" (entity type: UNIT).  \n",
      "6. **\"SEPT\"** – Abbreviation for \"September\" (entity type: DATE or TIME).  \n",
      "7. **\"2001\"** – Year (entity type: DATE or TIME).  \n",
      "\n",
      "### Notes:\n",
      "- \"7.0 PCT\" could alternatively be grouped as a single token (`[\"7.0 PCT\"]`) if the NER model expects combined percentage expressions.  \n",
      "- \"SEPT 2001\" might also be merged as `[\"SEPT 2001\"]` if the task treats dates as single entities.  \n",
      "\n",
      "Let me know if you'd prefer a different grouping strategy!\n",
      "Bot: [('UNDERLYING', 'O'), ('GOVT', 'B-ORG'), ('BOND', 'I-ORG'), ('7.0', 'O'), ('PCT', 'O'), ('SEPT', 'O'), ('2001', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:  \n",
      "\n",
      "```python\n",
      "[\"NOTES\", \"BAYERISCHE\", \"VEREINSBANK\", \"IS\", \"JOINT\", \"LEAD\", \"MANAGER\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **\"NOTES\"** – Likely a financial term (could be part of a security name).  \n",
      "- **\"BAYERISCHE\"** – Adjective (German for \"Bavarian\").  \n",
      "- **\"VEREINSBANK\"** – A bank name (likely referring to \"Bayerische Vereinsbank,\" a historical German bank).  \n",
      "- **\"IS\"** – Verb (auxiliary).  \n",
      "- **\"JOINT LEAD MANAGER\"** – Financial role (often an entity in NER for finance/banking).  \n",
      "\n",
      "This tokenization preserves meaningful units for NER, where:  \n",
      "- **\"BAYERISCHE VEREINSBANK\"** may be tagged as `ORG` (organization).  \n",
      "- **\"JOINT LEAD MANAGER\"** may be tagged as `ROLE` or `TITLE`.  \n",
      "\n",
      "Let me know if you'd prefer a different splitting strategy (e.g., keeping \"JOINT_LEAD_MANAGER\" as a single token).\n",
      "Bot: [('NOTES', 'O'), ('BAYERISCHE', 'B-ORG'), ('VEREINSBANK', 'I-ORG'), ('IS', 'O'), ('JOINT', 'O'), ('LEAD', 'O'), ('MANAGER', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"--\", \n",
      "    \"London\", \n",
      "    \"Newsroom\", \n",
      "    \"+44\", \n",
      "    \"171\", \n",
      "    \"542\", \n",
      "    \"7658\", \n",
      "    \"--\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"--\"**: Punctuation (often used as a separator or bullet point).\n",
      "2. **\"London\"**: Location (city name, likely a named entity of type `GPE` or `LOC`).\n",
      "3. **\"Newsroom\"**: Organization or common noun (context-dependent, could be part of an org name or a generic term).\n",
      "4. **\"+44\"**: Country calling code (part of a phone number, could be labeled as `PHONE` or left as-is).\n",
      "5. **\"171\"**, **\"542\"**, **\"7658\"**: Components of a phone number (often grouped as a single entity in NER).\n",
      "6. **\"--\"**: Punctuation (closing separator).\n",
      "\n",
      "### Notes for NER:\n",
      "- If the phone number should be treated as a single entity, you might merge `[\"+44\", \"171\", \"542\", \"7658\"]` into `[\"+44 171 542 7658\"]`.\n",
      "- \"London Newsroom\" could also be tokenized as a single entity if it represents an organization (e.g., `[\"London Newsroom\"]`).\n",
      "\n",
      "Let me know if you'd like adjustments for specific NER labeling conventions!\n",
      "Bot: [('--', 'O'), ('London', 'B-LOC'), ('Newsroom', 'B-ORG'), ('+44', 'O'), ('171', 'O'), ('542', 'O'), ('7658', 'O'), ('--', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"Venantius\", \"sets\", \"$\", \"300\", \"million\", \"January\", \"1999\", \"FRN\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"Venantius\"** – Proper noun (likely a name or organization).\n",
      "2. **\"sets\"** – Verb (action).\n",
      "3. **\"$\"** – Currency symbol (part of the monetary value).\n",
      "4. **\"300\"** – Numeric value (part of the monetary amount).\n",
      "5. **\"million\"** – Unit (scales the numeric value).\n",
      "6. **\"January\"** – Month (temporal entity).\n",
      "7. **\"1999\"** – Year (temporal entity).\n",
      "8. **\"FRN\"** – Likely an acronym (e.g., \"Floating Rate Note\" in finance).\n",
      "9. **\".\"** – Punctuation.\n",
      "\n",
      "This tokenization preserves meaningful units for NER, such as monetary values (\"$ 300 million\"), dates (\"January 1999\"), and entities (\"Venantius\", \"FRN\").\n",
      "Bot: [('Venantius', 'B-ORG'), ('sets', 'O'), ('$', 'O'), ('300', 'O'), ('million', 'O'), ('January', 'B-MISC'), ('1999', 'I-MISC'), ('FRN', 'B-MISC'), ('.', 'O')]\n",
      "Bot: Here is the tokenized breakdown of the input sentence `\"LONDON 1996-08-22\"` for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\"LONDON\", \"1996-08-22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"LONDON\"** is a standalone word (likely a **location** or **organization** entity in NER).\n",
      "2. **\"1996-08-22\"** is a date in ISO format (a **date** entity in NER). It is treated as a single token because:\n",
      "   - Splitting it further (e.g., `[\"1996\", \"-\", \"08\", \"-\", \"22\"]`) would lose the semantic meaning of the full date.\n",
      "   - Most NER systems recognize standardized date formats as single entities.\n",
      "\n",
      "If your NER model expects finer-grained tokenization (e.g., splitting punctuation), let me know, and I can adjust the output accordingly.\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"The\", \"following\", \"floating-rate\", \"issue\", \"was\", \n",
      "    \"announced\", \"by\", \"lead\", \"manager\", \"Lehman\", \n",
      "    \"Brothers\", \"International\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. **Hyphenated words** (\"floating-rate\") are kept as single tokens since they represent a single concept.  \n",
      "2. **Punctuation** (\".\") is treated as a separate token.  \n",
      "3. **Named entities** (\"Lehman Brothers International\") are split into individual tokens to allow the NER model to tag each component (e.g., `\"Lehman\" → ORG`, `\"Brothers\" → ORG`, `\"International\" → ORG`).  \n",
      "\n",
      "This tokenization follows common NER practices where multi-word entities are split for granularity. Let me know if you'd like adjustments (e.g., merging \"Lehman Brothers International\" into one token).\n",
      "Bot: [('The', 'O'), ('following', 'O'), ('floating-rate', 'O'), ('issue', 'O'), ('was', 'O'), ('announced', 'O'), ('by', 'O'), ('lead', 'O'), ('manager', 'O'), ('Lehman', 'B-ORG'), ('Brothers', 'I-ORG'), ('International', 'I-ORG'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"BORROWER\",\n",
      "    \"VENANTIUS\",\n",
      "    \"AB\",\n",
      "    \"(\",\n",
      "    \"SWEDISH\",\n",
      "    \"NATIONAL\",\n",
      "    \"MORTGAGE\",\n",
      "    \"AGENCY\",\n",
      "    \")\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"BORROWER\"** – Standalone word (likely an entity label or role).  \n",
      "2. **\"VENANTIUS\"** – Proper noun (part of an organization's name).  \n",
      "3. **\"AB\"** – Common abbreviation for \"Aktiebolag\" (Swedish for \"incorporated\").  \n",
      "4. **\"(\" and \")\"** – Punctuation marks (often important for NER to group entities).  \n",
      "5. **\"SWEDISH NATIONAL MORTGAGE AGENCY\"** – Split into individual words as it is a multi-word organization name.  \n",
      "\n",
      "This tokenization preserves structure (e.g., parentheses for grouping) and ensures the NER model can properly identify entities like \"VENANTIUS AB\" (organization) and \"SWEDISH NATIONAL MORTGAGE AGENCY\" (parenthetical explanation).  \n",
      "\n",
      "Let me know if you'd like further adjustments (e.g., merging \"AB\" with \"VENANTIUS\" or handling parentheses differently).\n",
      "Bot: [('BORROWER', 'O'), ('VENANTIUS', 'B-ORG'), ('AB', 'I-ORG'), ('(', 'O'), ('SWEDISH', 'B-ORG'), ('NATIONAL', 'I-ORG'), ('MORTGAGE', 'I-ORG'), ('AGENCY', 'I-ORG'), (')', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"AMT\", \n",
      "    \"$\", \n",
      "    \"300\", \n",
      "    \"MLN\", \n",
      "    \"SPREAD\", \n",
      "    \"-\", \n",
      "    \"12.5\", \n",
      "    \"BP\", \n",
      "    \"MATURITY\", \n",
      "    \"21.JAN.99\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation of Tokenization:\n",
      "1. **\"AMT\"** – Likely stands for \"amount\" (financial term).  \n",
      "2. **\"$\"** – Currency symbol (part of the monetary value).  \n",
      "3. **\"300\"** – Numeric value (part of the amount).  \n",
      "4. **\"MLN\"** – Abbreviation for \"million\" (financial unit).  \n",
      "5. **\"SPREAD\"** – Financial term (interest rate spread).  \n",
      "6. **\"-\"** – Punctuation (separates spread value).  \n",
      "7. **\"12.5\"** – Numeric value (basis points spread).  \n",
      "8. **\"BP\"** – Abbreviation for \"basis points\" (financial term).  \n",
      "9. **\"MATURITY\"** – Financial term (bond maturity date).  \n",
      "10. **\"21.JAN.99\"** – Date format (21st January 1999).  \n",
      "\n",
      "### Why This Tokenization Works for NER:\n",
      "- Monetary values (`$ 300 MLN`) are split into components for proper entity tagging (e.g., `$` as currency, `300` as amount, `MLN` as unit).  \n",
      "- \"SPREAD - 12.5 BP\" is split to capture the spread value separately.  \n",
      "- The date (`21.JAN.99`) is kept as a single token if the NER model expects date entities in this format.  \n",
      "\n",
      "Would you like any adjustments (e.g., splitting the date further into `21`, `JAN`, `99`)?\n",
      "Bot: [('AMT', 'B-MISC'), ('$', 'O'), ('300', 'O'), ('MLN', 'O'), ('SPREAD', 'B-MISC'), ('-', 'O'), ('12.5', 'O'), ('BP', 'O'), ('MATURITY', 'B-MISC'), ('21.JAN.99', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"TYPE\", \n",
      "    \"FRN\", \n",
      "    \"BASE\", \n",
      "    \"3M\", \n",
      "    \"LIBOR\", \n",
      "    \"PAY\", \n",
      "    \"DATE\", \n",
      "    \"S23.SEP.96\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- Each word or meaningful unit is treated as a separate token.\n",
      "- \"3M\" is kept as a single token (likely referring to \"3-month\").\n",
      "- \"LIBOR\" is a financial term (London Interbank Offered Rate) and is kept intact.\n",
      "- \"S23.SEP.96\" is treated as a single token (likely a date or identifier in a specific format).\n",
      "\n",
      "This tokenization preserves the structure and meaning for NER tasks, where entities like dates (\"S23.SEP.96\"), financial terms (\"LIBOR\"), and abbreviations (\"FRN\" for Floating Rate Note) may need to be recognized.\n",
      "Bot: [('TYPE', 'O'), ('FRN', 'B-MISC'), ('BASE', 'O'), ('3M', 'B-MISC'), ('LIBOR', 'B-MISC'), ('PAY', 'O'), ('DATE', 'O'), ('S23.SEP.96', 'B-MISC')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"LAST\", \"MOODY\", \"AA3\", \"ISS\", \"PRICE\", \"99.956\", \"FULL\", \"FEES\", \"10\", \"BP\"]\n",
      "```\n",
      "\n",
      "### Explanation of Tokenization:\n",
      "1. **\"LAST\"** - Standalone word (could indicate a time reference or an entity).  \n",
      "2. **\"MOODY\"** - Likely refers to \"Moody's\" (credit rating agency).  \n",
      "3. **\"AA3\"** - Credit rating (entity: financial rating).  \n",
      "4. **\"ISS\"** - Likely refers to \"Institutional Shareholder Services\" (financial services firm).  \n",
      "5. **\"PRICE\"** - Keyword indicating a numerical value follows.  \n",
      "6. **\"99.956\"** - Numerical value (entity: price).  \n",
      "7. **\"FULL\"** - Modifier (could relate to fees or another attribute).  \n",
      "8. **\"FEES\"** - Keyword indicating a cost-related entity.  \n",
      "9. **\"10\"** - Numerical value (entity: fee amount).  \n",
      "10. **\"BP\"** - \"Basis Points\" (financial term, entity: unit).  \n",
      "\n",
      "This tokenization ensures that each meaningful unit is separated for NER analysis (e.g., identifying organizations, numbers, financial terms). Let me know if you'd like further refinements!\n",
      "Bot: [('LAST', 'O'), ('MOODY', 'B-ORG'), ('AA3', 'B-MISC'), ('ISS', 'B-ORG'), ('PRICE', 'O'), ('99.956', 'O'), ('FULL', 'O'), ('FEES', 'O'), ('10', 'O'), ('BP', 'B-MISC')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"LAST\", \"S&P\", \"AA+\", \"REOFFER\", \"=\", \"--\"]\n",
      "```\n",
      "\n",
      "### Explanation of Tokenization:\n",
      "1. **\"LAST\"** – Standalone word.  \n",
      "2. **\"S&P\"** – A single token representing the financial index (Standard & Poor's).  \n",
      "3. **\"AA+\"** – A single token representing a credit rating.  \n",
      "4. **\"REOFFER\"** – Financial term (often used in bond markets).  \n",
      "5. **\"=\"** – Symbol (equality sign).  \n",
      "6. **\"--\"** – Punctuation (double hyphen/dash).  \n",
      "\n",
      "This tokenization preserves meaningful financial/entity-related terms (like \"S&P\" and \"AA+\") as single units, which is critical for NER tasks in domains like finance. Let me know if you'd like adjustments!\n",
      "Bot: [('LAST', 'O'), ('S&P', 'B-ORG'), ('AA+', 'B-MISC'), ('REOFFER', 'O'), ('=', 'O'), ('--', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"NOTES\", \"S\", \"SHORT\", \"FIRST\", \"COUPON\"]\n",
      "```\n",
      "\n",
      "Each word is treated as a separate token. This list can now be used as input for an NER model to predict entity labels (e.g., financial terms, abbreviations, etc.). Let me know if you'd like any modifications (e.g., handling punctuation, splitting contractions, etc.).\n",
      "Bot: [('NOTES', 'O'), ('S', 'O'), ('SHORT', 'O'), ('FIRST', 'O'), ('COUPON', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"LISTING\", \n",
      "    \"LONDON\", \n",
      "    \"DENOMS\", \n",
      "    \"(\", \n",
      "    \"K\", \n",
      "    \")\", \n",
      "    \"1\", \n",
      "    \"-\", \n",
      "    \"10\", \n",
      "    \"-\", \n",
      "    \"100\", \n",
      "    \"SALE\", \n",
      "    \"LIMITS\", \n",
      "    \"US\", \n",
      "    \"/\", \n",
      "    \"UK\", \n",
      "    \"/\", \n",
      "    \"JP\", \n",
      "    \"/\", \n",
      "    \"FR\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- Punctuation marks (`(`, `)`, `-`, `/`) are treated as separate tokens since they may be relevant for NER (e.g., separating entities or indicating structure).\n",
      "- Abbreviations like \"K\" and country codes (\"US\", \"UK\", \"JP\", \"FR\") are kept as standalone tokens.\n",
      "- Numbers (\"1\", \"10\", \"100\") are split into individual tokens, including the hyphens between them.\n",
      "\n",
      "This tokenization preserves all meaningful elements for NER, such as:\n",
      "- Location (\"LONDON\"),\n",
      "- Denominations (\"DENOMS\", \"1-10-100\"),\n",
      "- Country codes (\"US\", \"UK\", \"JP\", \"FR\"),\n",
      "- Symbols that may indicate relationships or separators. \n",
      "\n",
      "Let me know if you'd like further adjustments!\n",
      "Bot: [('LISTING', 'O'), ('LONDON', 'B-LOC'), ('DENOMS', 'O'), ('(', 'O'), ('K', 'O'), (')', 'O'), ('1', 'O'), ('-', 'O'), ('10', 'O'), ('-', 'O'), ('100', 'O'), ('SALE', 'O'), ('LIMITS', 'O'), ('US', 'B-LOC'), ('/', 'O'), ('UK', 'B-LOC'), ('/', 'O'), ('JP', 'B-LOC'), ('/', 'O'), ('FR', 'B-LOC')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"NEG\", \"PLG\", \"YES\", \"CRS\", \"DEFLT\", \"NO\", \"FORCE\", \"MAJ\", \"IPMA\", \"2\"]\n",
      "```\n",
      "\n",
      "Each token is treated as a separate entity for NER analysis. Let me know if you'd like any modifications or additional processing!\n",
      "Bot: [('NEG', 'O'), ('PLG', 'O'), ('YES', 'O'), ('CRS', 'O'), ('DEFLT', 'O'), ('NO', 'O'), ('FORCE', 'O'), ('MAJ', 'O'), ('IPMA', 'O'), ('2', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens as a list:\n",
      "\n",
      "```python\n",
      "[\"GOV\", \"LAW\", \"ENGLISH\", \"HOME\", \"CTRY\", \"SWEDEN\", \"TAX\", \"PROVS\", \"STANDARD\"]\n",
      "```\n",
      "\n",
      "Each token is a separate word or abbreviation from the original input. This list can now be used for named entity recognition (NER) tasks.\n",
      "Bot: [('GOV', 'B-ORG'), ('LAW', 'I-ORG'), ('ENGLISH', 'B-MISC'), ('HOME', 'O'), ('CTRY', 'O'), ('SWEDEN', 'B-LOC'), ('TAX', 'O'), ('PROVS', 'O'), ('STANDARD', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"MGT\", \"/\", \"UND\", \"5\", \"BP\", \"SELL\", \"CONC\", \"5\", \"BP\", \"PRAECIP\", \"=\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"MGT\"** – Likely an abbreviation (e.g., \"Management\" or a specific term).  \n",
      "2. **\"/\"** – A separator.  \n",
      "3. **\"UND\"** – Could stand for \"und\" (German for \"and\") or an abbreviation.  \n",
      "4. **\"5\"** – A numerical value.  \n",
      "5. **\"BP\"** – Likely \"blood pressure\" or another abbreviation (e.g., \"British Petroleum\" in another context).  \n",
      "6. **\"SELL\"** – Could be a verb or part of a medical/pharmaceutical term.  \n",
      "7. **\"CONC\"** – Possibly \"concentration\" or \"concerning.\"  \n",
      "8. **\"5\"** – Another numerical value.  \n",
      "9. **\"BP\"** – Repeats the earlier abbreviation.  \n",
      "10. **\"PRAECIP\"** – Possibly a misspelling of \"precip\" (short for \"precipitation\" or \"precipitate\").  \n",
      "11. **\"=\"** – A symbol indicating equivalence or assignment.  \n",
      "\n",
      "This tokenization preserves the structure for NER, where each token can be classified (e.g., as a drug, dosage, medical term, or symbol). Let me know if you'd like adjustments!\n",
      "Bot: [('MGT', 'B-ORG'), ('/', 'O'), ('UND', 'O'), ('5', 'O'), ('BP', 'B-MISC'), ('SELL', 'O'), ('CONC', 'O'), ('5', 'O'), ('BP', 'B-MISC'), ('PRAECIP', 'O'), ('=', 'O')]\n",
      "Bot: Here is the sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"NOTES\", \"ISSUED\", \"OFF\", \"EMTN\", \"PROGRAMME\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"NOTES\"** - Standalone financial instrument term.\n",
      "2. **\"ISSUED\"** - Verb indicating the action related to the notes.\n",
      "3. **\"OFF\"** - Preposition often used in financial contexts (e.g., \"issued off a programme\").\n",
      "4. **\"EMTN\"** - Acronym for \"Euro Medium Term Note,\" a key financial entity.\n",
      "5. **\"PROGRAMME\"** - Variant spelling of \"PROGRAM,\" referring to the EMTN issuance framework.\n",
      "\n",
      "This tokenization preserves domain-specific terms (like \"EMTN\") as single units, which is critical for NER in financial texts. Let me know if you'd like further splitting (e.g., \"PROGRAMME\" → \"PROGRAM\" + \"ME\" is unnecessary here).\n",
      "Bot: [('NOTES', 'B-MISC'), ('ISSUED', 'O'), ('OFF', 'O'), ('EMTN', 'B-ORG'), ('PROGRAMME', 'I-ORG')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"--\", \n",
      "    \"London\", \n",
      "    \"Newsroom\", \n",
      "    \"+44\", \n",
      "    \"171\", \n",
      "    \"542\", \n",
      "    \"8863\", \n",
      "    \"--\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. `\"--\"` is treated as a separate token (punctuation/symbol).  \n",
      "2. `\"London\"` is a single word (likely a **location** entity).  \n",
      "3. `\"Newsroom\"` is a single word (could be an **organization** or part of one).  \n",
      "4. `\"+44\"` is the country code (part of a **phone number** entity).  \n",
      "5. `\"171\"`, `\"542\"`, `\"8863\"` are segments of the phone number (could be grouped as a single entity later).  \n",
      "6. The final `\"--\"` is another punctuation token.  \n",
      "\n",
      "### Alternative Grouping (if phone number should be one token):\n",
      "If the phone number (`+44 171 542 8863`) should be a single token:\n",
      "```python\n",
      "tokens = [\n",
      "    \"--\", \n",
      "    \"London\", \n",
      "    \"Newsroom\", \n",
      "    \"+44 171 542 8863\", \n",
      "    \"--\"\n",
      "]\n",
      "```\n",
      "\n",
      "Would you like any adjustments based on how the NER model expects tokenization? For example, splitting or keeping the phone number as one unit?\n",
      "Bot: [('--', 'O'), ('London', 'B-LOC'), ('Newsroom', 'B-ORG'), ('+44', 'O'), ('171', 'O'), ('542', 'O'), ('8863', 'O'), ('--', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"Port\", \"conditions\", \"update\", \"-\", \"Syria\", \"-\", \"Lloyds\", \"Shipping\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"Port\"** – Standalone word (could be part of an entity like \"Port of Syria\").  \n",
      "2. **\"conditions\"** – Standalone word.  \n",
      "3. **\"update\"** – Standalone word.  \n",
      "4. **\"-\"** – Punctuation (often treated as a separate token in NER).  \n",
      "5. **\"Syria\"** – Likely a geopolitical entity (GPE).  \n",
      "6. **\"-\"** – Punctuation.  \n",
      "7. **\"Lloyds\"** – Part of an organization name.  \n",
      "8. **\"Shipping\"** – Part of an organization name (combined with \"Lloyds\" as \"Lloyds Shipping\").  \n",
      "9. **\".\"** – Sentence-ending punctuation.  \n",
      "\n",
      "### Notes for NER:\n",
      "- Hyphens (`-`) are often kept as separate tokens to avoid splitting multi-word entities (e.g., \"Lloyds Shipping\" remains two tokens).  \n",
      "- If your NER model expects subword tokenization (e.g., BERT's WordPiece), further splitting may be needed (e.g., \"Lloyds\" → [\"Lloyd\", \"##s\"]).  \n",
      "- For simplicity, this assumes whitespace/punctuation-based tokenization.  \n",
      "\n",
      "Let me know if you'd like a different tokenization strategy!\n",
      "Bot: [('Port', 'O'), ('conditions', 'O'), ('update', 'O'), ('-', 'O'), ('Syria', 'B-LOC'), ('-', 'O'), ('Lloyds', 'B-ORG'), ('Shipping', 'I-ORG'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"Port\", \"conditions\", \"from\", \"Lloyds\", \"Shipping\", \"Intelligence\", \"Service\", \"--\", \"--\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"Port\"** - Single word, likely a noun or part of a named entity.\n",
      "2. **\"conditions\"** - Single word, likely a noun.\n",
      "3. **\"from\"** - Preposition.\n",
      "4. **\"Lloyds\"** - Part of the organization name, likely a named entity (ORG).\n",
      "5. **\"Shipping\"** - Part of the organization name.\n",
      "6. **\"Intelligence\"** - Part of the organization name.\n",
      "7. **\"Service\"** - Part of the organization name.\n",
      "8. **\"--\"** - Punctuation (hyphen or dash).\n",
      "9. **\"--\"** - Punctuation (hyphen or dash).\n",
      "\n",
      "### Notes:\n",
      "- The tokens are split on whitespace and punctuation (hyphens are treated as separate tokens).\n",
      "- For NER, \"Lloyds Shipping Intelligence Service\" is likely a single named entity (Organization), but it's tokenized into sub-components for the model to process. The NER model may then group them back into a single entity during prediction.\n",
      "Bot: [('Port', 'O'), ('conditions', 'O'), ('from', 'O'), ('Lloyds', 'B-ORG'), ('Shipping', 'I-ORG'), ('Intelligence', 'I-ORG'), ('Service', 'I-ORG'), ('--', 'O'), ('--', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"LATTAKIA\", \",\", \"Aug\", \"10\", \"-\", \"waiting\", \"time\", \"at\", \n",
      "    \"Lattakia\", \"and\", \"Tartous\", \"presently\", \"24\", \"hours\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"LATTAKIA\"** - Proper noun (city name, all caps in the original).\n",
      "2. **\",\"** - Punctuation.\n",
      "3. **\"Aug\"** - Abbreviation for August (month, part of the date).\n",
      "4. **\"10\"** - Day of the month (numeric).\n",
      "5. **\"-\"** - Punctuation (hyphen/dash).\n",
      "6. **\"waiting\"** - Verb/gerund.\n",
      "7. **\"time\"** - Noun.\n",
      "8. **\"at\"** - Preposition.\n",
      "9. **\"Lattakia\"** - Proper noun (city name, lowercase variant).\n",
      "10. **\"and\"** - Conjunction.\n",
      "11. **\"Tartous\"** - Proper noun (city name).\n",
      "12. **\"presently\"** - Adverb.\n",
      "13. **\"24\"** - Numeric value.\n",
      "14. **\"hours\"** - Noun (unit of time).\n",
      "15. **\".\"** - Punctuation (sentence terminator).\n",
      "\n",
      "This tokenization preserves entities like locations (\"Lattakia\", \"Tartous\"), dates (\"Aug 10\"), and numeric values (\"24 hours\") for NER tasks. Let me know if you'd like further adjustments!\n",
      "Bot: [('LATTAKIA', 'B-LOC'), (',', 'O'), ('Aug', 'O'), ('10', 'O'), ('-', 'O'), ('waiting', 'O'), ('time', 'O'), ('at', 'O'), ('Lattakia', 'B-LOC'), ('and', 'O'), ('Tartous', 'B-LOC'), ('presently', 'O'), ('24', 'O'), ('hours', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\"Israel\", \"plays\", \"down\", \"fears\", \"of\", \"war\", \"with\", \"Syria\", \".\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"Israel\"** – Proper noun (likely `GPE` or `LOC` in NER).  \n",
      "2. **\"plays\"** – Verb.  \n",
      "3. **\"down\"** – Adverb/particle (part of the phrasal verb \"play down\").  \n",
      "4. **\"fears\"** – Noun.  \n",
      "5. **\"of\"** – Preposition.  \n",
      "6. **\"war\"** – Noun.  \n",
      "7. **\"with\"** – Preposition.  \n",
      "8. **\"Syria\"** – Proper noun (likely `GPE` or `LOC` in NER).  \n",
      "9. **\".\"** – Punctuation.  \n",
      "\n",
      "This tokenization preserves named entities (\"Israel\", \"Syria\") as single tokens, which is crucial for NER. Let me know if you'd like further adjustments!\n",
      "Bot: [('Israel', 'B-LOC'), ('plays', 'O'), ('down', 'O'), ('fears', 'O'), ('of', 'O'), ('war', 'O'), ('with', 'O'), ('Syria', 'B-LOC'), ('.', 'O')]\n",
      "Bot: Here is the tokenized version of the input sentence for a named entity recognition (NER) task:  \n",
      "\n",
      "```python\n",
      "[\"Colleen\", \"Siegel\"]\n",
      "```\n",
      "\n",
      "### Explanation:  \n",
      "- The input is a name (\"Colleen Siegel\"), which consists of two words.  \n",
      "- For NER, it's common to split names into individual tokens (first name and last name) to properly identify entities.  \n",
      "- Punctuation (like `--`) is not present in the given input, but if it were, it would typically be treated as a separate token.  \n",
      "\n",
      "Let me know if you'd like any modifications to the tokenization approach!\n",
      "Bot: [('Colleen', 'B-PER'), ('Siegel', 'I-PER')]\n",
      "Bot: To tokenize the sentence \"JERUSALEM 1996-08-22\" for a named entity recognition (NER) task, we should break it down into meaningful units (tokens) that capture both the word and the date components. Here's how you can tokenize it:\n",
      "\n",
      "### Input Sentence:\n",
      "```\n",
      "JERUSALEM 1996-08-22\n",
      "```\n",
      "\n",
      "### Tokenized Output (as a list):\n",
      "```python\n",
      "[\"JERUSALEM\", \"1996-08-22\"]\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **\"JERUSALEM\"** is a single token representing a location (city).\n",
      "2. **\"1996-08-22\"** is a single token representing a date in ISO format (YYYY-MM-DD). \n",
      "\n",
      "For NER, this breakdown allows the model to recognize:\n",
      "- \"JERUSALEM\" as a `LOC` (location) entity.\n",
      "- \"1996-08-22\" as a `DATE` entity.\n",
      "\n",
      "If further granularity is needed (e.g., splitting the date into year, month, day), you could tokenize it as:\n",
      "```python\n",
      "[\"JERUSALEM\", \"1996\", \"-\", \"08\", \"-\", \"22\"]\n",
      "```\n",
      "But the first approach is more common unless your NER task specifically requires splitting the date components. \n",
      "\n",
      "Let me know if you'd prefer a different tokenization strategy!\n",
      "Bot: [('JERUSALEM', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Israel\", \"'s\", \"outgoing\", \"peace\", \"negotiator\", \"with\", \"Syria\", \n",
      "    \"said\", \"on\", \"Thursday\", \"current\", \"tensions\", \"between\", \"the\", \n",
      "    \"two\", \"countries\", \"appeared\", \"to\", \"be\", \"a\", \"storm\", \"in\", \n",
      "    \"a\", \"teacup\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization:\n",
      "1. **Proper Nouns**: \"Israel\", \"Syria\", and \"Thursday\" are kept intact as they are likely named entities (country, day).\n",
      "2. **Possessive Clitic**: `'s` is separated from \"Israel\" to handle possessive forms correctly in NER.\n",
      "3. **Punctuation**: The period at the end is treated as a separate token.\n",
      "4. **Phrasal Verbs/Idioms**: \"storm in a teacup\" is split into individual words to allow the model to recognize each component.\n",
      "\n",
      "This list is suitable for NER tasks where each token can be tagged with labels like `B-PER`, `I-LOC`, etc.\n",
      "Bot: [  \n",
      "    ('Israel', 'B-LOC'),  \n",
      "    (\"'s\", 'O'),  \n",
      "    ('outgoing', 'O'),  \n",
      "    ('peace', 'O'),  \n",
      "    ('negotiator', 'O'),  \n",
      "    ('with', 'O'),  \n",
      "    ('Syria', 'B-LOC'),  \n",
      "    ('said', 'O'),  \n",
      "    ('on', 'O'),  \n",
      "    ('Thursday', 'B-MISC'),  \n",
      "    ('current', 'O'),  \n",
      "    ('tensions', 'O'),  \n",
      "    ('between', 'O'),  \n",
      "    ('the', 'O'),  \n",
      "    ('two', 'O'),  \n",
      "    ('countries', 'O'),  \n",
      "    ('appeared', 'O'),  \n",
      "    ('to', 'O'),  \n",
      "    ('be', 'O'),  \n",
      "    ('a', 'O'),  \n",
      "    ('storm', 'O'),  \n",
      "    ('in', 'O'),  \n",
      "    ('a', 'O'),  \n",
      "    ('teacup', 'O'),  \n",
      "    ('.', 'O')  \n",
      "]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Itamar\", \"Rabinovich\", \",\", \"who\", \"as\", \"Israel\", \"'s\", \"ambassador\", \n",
      "    \"to\", \"Washington\", \"conducted\", \"unfruitful\", \"negotiations\", \"with\", \n",
      "    \"Syria\", \",\", \"told\", \"Israel\", \"Radio\", \"it\", \"looked\", \"like\", \n",
      "    \"Damascus\", \"wanted\", \"to\", \"talk\", \"rather\", \"than\", \"fight\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes on Tokenization for NER:\n",
      "1. **Punctuation**: Commas (`,`) and periods (`.`) are treated as separate tokens.\n",
      "2. **Possessive Clitic**: `'s` is split from `Israel` to help with entity boundary detection.\n",
      "3. **Named Entities**: Multi-word entities like \"Itamar Rabinovich\" and \"Israel Radio\" are split into individual tokens, but the NER model may later group them.\n",
      "4. **Verbs/Adjectives**: Words like \"conducted\", \"unfruitful\", and \"wanted\" are kept as single tokens since they don’t carry entity information.\n",
      "\n",
      "This tokenization preserves the structure needed for NER, where the model will predict labels like `PER` (for \"Itamar Rabinovich\"), `GPE` (for \"Israel\", \"Syria\", \"Damascus\"), or `ORG` (for \"Israel Radio\").\n",
      "Bot: [  \n",
      "    ('Itamar', 'B-PER'),  \n",
      "    ('Rabinovich', 'I-PER'),  \n",
      "    (',', 'O'),  \n",
      "    ('who', 'O'),  \n",
      "    ('as', 'O'),  \n",
      "    ('Israel', 'B-LOC'),  \n",
      "    (\"'s\", 'O'),  \n",
      "    ('ambassador', 'O'),  \n",
      "    ('to', 'O'),  \n",
      "    ('Washington', 'B-LOC'),  \n",
      "    ('conducted', 'O'),  \n",
      "    ('unfruitful', 'O'),  \n",
      "    ('negotiations', 'O'),  \n",
      "    ('with', 'O'),  \n",
      "    ('Syria', 'B-LOC'),  \n",
      "    (',', 'O'),  \n",
      "    ('told', 'O'),  \n",
      "    ('Israel', 'B-ORG'),  \n",
      "    ('Radio', 'I-ORG'),  \n",
      "    ('it', 'O'),  \n",
      "    ('looked', 'O'),  \n",
      "    ('like', 'O'),  \n",
      "    ('Damascus', 'B-LOC'),  \n",
      "    ('wanted', 'O'),  \n",
      "    ('to', 'O'),  \n",
      "    ('talk', 'O'),  \n",
      "    ('rather', 'O'),  \n",
      "    ('than', 'O'),  \n",
      "    ('fight', 'O'),  \n",
      "    ('.', 'O')  \n",
      "]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "tokens = [\n",
      "    \"It\", \n",
      "    \"appears\", \n",
      "    \"to\", \n",
      "    \"me\", \n",
      "    \"the\", \n",
      "    \"Syrian\", \n",
      "    \"priority\", \n",
      "    \"is\", \n",
      "    \"still\", \n",
      "    \"to\", \n",
      "    \"negotiate\", \n",
      "    \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation (like `.`) is treated as a separate token, as it may be relevant for NER (e.g., sentence boundaries).\n",
      "2. \"Syrian\" is kept as a single token since it is likely an adjective modifying \"priority\" and could be part of a named entity (e.g., \"Syrian government\").\n",
      "3. Contractions or hyphenated words are not present here, but if they were, they could be split further depending on the tokenization strategy (e.g., \"can't\" → [\"can\", \"'t\"]).\n",
      "\n",
      "Let me know if you'd like a different tokenization approach!\n",
      "Bot: [('It', 'O'), ('appears', 'O'), ('to', 'O'), ('me', 'O'), ('the', 'O'), ('Syrian', 'B-MISC'), ('priority', 'O'), ('is', 'O'), ('still', 'O'), ('to', 'O'), ('negotiate', 'O'), ('.', 'O')]\n",
      "Bot: Here is the input sentence broken down into tokens for a named entity recognition (NER) task:\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"The\", \"Syrians\", \"are\", \"confused\", \",\", \"they\", \"are\", \"definitely\", \n",
      "    \"tense\", \",\", \"but\", \"the\", \"general\", \"assessment\", \"here\", \"in\", \n",
      "    \"Washington\", \"is\", \"that\", \"this\", \"is\", \"essentially\", \"a\", \"storm\", \n",
      "    \"in\", \"a\", \"teacup\", \",\", \"\\\"\", \"he\", \"said\", \".\"\n",
      "]\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "1. Punctuation marks (`,`, `\"`, `.`) are treated as separate tokens.\n",
      "2. Words are split based on whitespace and punctuation boundaries.\n",
      "3. Contractions (e.g., \"he's\") would typically be split further (e.g., [\"he\", \"'s\"]), but none appear in this sentence.\n",
      "4. \"Washington\" is a likely named entity (location), and \"Syrians\" could be a nationality or demonym entity.\n",
      "\n",
      "Let me know if you'd like a different tokenization approach (e.g., splitting possessives, hyphenated words, etc.).\n",
      "Bot: [('The', 'O'), ('Syrians', 'B-MISC'), ('are', 'O'), ('confused', 'O'), (',', 'O'), ('they', 'O'), ('are', 'O'), ('definitely', 'O'), ('tense', 'O'), (',', 'O'), ('but', 'O'), ('the', 'O'), ('general', 'O'), ('assessment', 'O'), ('here', 'O'), ('in', 'O'), ('Washington', 'B-LOC'), ('is', 'O'), ('that', 'O'), ('this', 'O'), ('is', 'O'), ('essentially', 'O'), ('a', 'O'), ('storm', 'O'), ('in', 'O'), ('a', 'O'), ('teacup', 'O'), (',', 'O'), ('\"', 'O'), ('he', 'O'), ('said', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "def send_prompt_conversation(prompt):\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\", \n",
    "        messages=messages\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    print(\"Bot:\", reply)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "for sentence in dataset['train']['tokens'][:100]:\n",
    "    prompt = f\"Take as input the sentence:   {' '.join(sentence)}    \\n \\\n",
    "        --Break it down into its tokens and return it as a list. \\\n",
    "        The tokens will be used for a named entity recognition task.\\n\"\n",
    "\n",
    "    send_prompt_conversation(prompt)\n",
    "\n",
    "    prompt_2 = f\"Given the entity label set: {list(ner_dict.keys())},\\n \\\n",
    "                based on the tokens you found in the previous step, \\\n",
    "                please recognize the named entities in the given text and return a list of tuples with each token and its label. \\\n",
    "                Return only the list in the format: [('\\\"','O'),('In','O'), ('America','I-LOC'), ('is','O'), ('cold','O'), ...]  \\\n",
    "                Do not return any explanation or additional text.\\n.\"\n",
    "    \n",
    "    send_prompt_conversation(prompt_2)\n",
    "\n",
    "    with open(\"divided_inputs.txt\", \"a\") as file:\n",
    "        file.write(f\"{messages[3][\"content\"]}\\n\")\n",
    "    \n",
    "    messages = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1de51",
   "metadata": {},
   "source": [
    "### Self validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "326e8af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: [('EU', 'B-ORG'), ('rejects', 'O'), ('German', 'B-MISC'), ('call', 'O'), ('to', 'O'), ('boycott', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('.', 'O')]\n",
      "Bot: [('EU', 'B-ORG'), ('rejects', 'O'), ('German', 'B-MISC'), ('call', 'O'), ('to', 'O'), ('boycott', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('.', 'O')]\n",
      "Bot: [('Peter', 'B-PER'), ('Blackburn', 'I-PER')]\n",
      "Bot: [('Peter', 'B-PER'), ('Blackburn', 'I-PER')]\n",
      "Bot: [('BRUSSELS', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('BRUSSELS', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('The', 'O'), ('European', 'B-ORG'), ('Commission', 'I-ORG'), ('said', 'O'), ('on', 'O'), ('Thursday', 'O'), ('it', 'O'), ('disagreed', 'O'), ('with', 'O'), ('German', 'B-MISC'), ('advice', 'O'), ('to', 'O'), ('consumers', 'O'), ('to', 'O'), ('shun', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('until', 'O'), ('scientists', 'O'), ('determine', 'O'), ('whether', 'O'), ('mad', 'O'), ('cow', 'O'), ('disease', 'O'), ('can', 'O'), ('be', 'O'), ('transmitted', 'O'), ('to', 'O'), ('sheep', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('European', 'B-ORG'), ('Commission', 'I-ORG'), ('said', 'O'), ('on', 'O'), ('Thursday', 'O'), ('it', 'O'), ('disagreed', 'O'), ('with', 'O'), ('German', 'B-MISC'), ('advice', 'O'), ('to', 'O'), ('consumers', 'O'), ('to', 'O'), ('shun', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('until', 'O'), ('scientists', 'O'), ('determine', 'O'), ('whether', 'O'), ('mad', 'O'), ('cow', 'O'), ('disease', 'O'), ('can', 'O'), ('be', 'O'), ('transmitted', 'O'), ('to', 'O'), ('sheep', 'O'), ('.', 'O')]\n",
      "Bot: [('Germany', 'B-LOC'), ('\\'s', 'O'), ('representative', 'O'), ('to', 'O'), ('the', 'O'), ('European', 'B-ORG'), ('Union', 'I-ORG'), ('\\'s', 'O'), ('veterinary', 'O'), ('committee', 'O'), ('Werner', 'B-PER'), ('Zwingmann', 'I-PER'), ('said', 'O'), ('on', 'O'), ('Wednesday', 'O'), ('consumers', 'O'), ('should', 'O'), ('buy', 'O'), ('sheepmeat', 'O'), ('from', 'O'), ('countries', 'O'), ('other', 'O'), ('than', 'O'), ('Britain', 'B-LOC'), ('until', 'O'), ('the', 'O'), ('scientific', 'O'), ('advice', 'O'), ('was', 'O'), ('clearer', 'O'), ('.', 'O')]\n",
      "Bot: [('Germany', 'B-LOC'), ('\\'s', 'O'), ('representative', 'O'), ('to', 'O'), ('the', 'O'), ('European', 'B-ORG'), ('Union', 'I-ORG'), ('\\'s', 'O'), ('veterinary', 'O'), ('committee', 'O'), ('Werner', 'B-PER'), ('Zwingmann', 'I-PER'), ('said', 'O'), ('on', 'O'), ('Wednesday', 'O'), ('consumers', 'O'), ('should', 'O'), ('buy', 'O'), ('sheepmeat', 'O'), ('from', 'O'), ('countries', 'O'), ('other', 'O'), ('than', 'O'), ('Britain', 'B-LOC'), ('until', 'O'), ('the', 'O'), ('scientific', 'O'), ('advice', 'O'), ('was', 'O'), ('clearer', 'O'), ('.', 'O')]\n",
      "Bot: [('We', 'O'), ('do', 'O'), (\"n't\", 'O'), ('support', 'O'), ('any', 'O'), ('such', 'O'), ('recommendation', 'O'), ('because', 'O'), ('we', 'O'), ('do', 'O'), (\"n't\", 'O'), ('see', 'O'), ('any', 'O'), ('grounds', 'O'), ('for', 'O'), ('it', 'O'), (',', 'O'), ('\"', 'O'), ('the', 'O'), ('Commission', 'B-ORG'), (\"'s\", 'I-ORG'), ('chief', 'I-ORG'), ('spokesman', 'I-ORG'), ('Nikolaus', 'B-PER'), ('van', 'I-PER'), ('der', 'I-PER'), ('Pas', 'I-PER'), ('told', 'O'), ('a', 'O'), ('news', 'O'), ('briefing', 'O'), ('.', 'O')]\n",
      "Bot: [('We', 'O'), ('do', 'O'), (\"n't\", 'O'), ('support', 'O'), ('any', 'O'), ('such', 'O'), ('recommendation', 'O'), ('because', 'O'), ('we', 'O'), ('do', 'O'), (\"n't\", 'O'), ('see', 'O'), ('any', 'O'), ('grounds', 'O'), ('for', 'O'), ('it', 'O'), (',', 'O'), ('\"', 'O'), ('the', 'O'), ('Commission', 'B-ORG'), (\"'s\", 'I-ORG'), ('chief', 'I-ORG'), ('spokesman', 'I-ORG'), ('Nikolaus', 'B-PER'), ('van', 'I-PER'), ('der', 'I-PER'), ('Pas', 'I-PER'), ('told', 'O'), ('a', 'O'), ('news', 'O'), ('briefing', 'O'), ('.', 'O')]\n",
      "Bot: [('He', 'O'), ('said', 'O'), ('further', 'O'), ('scientific', 'O'), ('study', 'O'), ('was', 'O'), ('required', 'O'), ('and', 'O'), ('if', 'O'), ('it', 'O'), ('was', 'O'), ('found', 'O'), ('that', 'O'), ('action', 'O'), ('was', 'O'), ('needed', 'O'), ('it', 'O'), ('should', 'O'), ('be', 'O'), ('taken', 'O'), ('by', 'O'), ('the', 'O'), ('European', 'B-ORG'), ('Union', 'I-ORG'), ('.', 'O')]\n",
      "Bot: [('He', 'O'), ('said', 'O'), ('further', 'O'), ('scientific', 'O'), ('study', 'O'), ('was', 'O'), ('required', 'O'), ('and', 'O'), ('if', 'O'), ('it', 'O'), ('was', 'O'), ('found', 'O'), ('that', 'O'), ('action', 'O'), ('was', 'O'), ('needed', 'O'), ('it', 'O'), ('should', 'O'), ('be', 'O'), ('taken', 'O'), ('by', 'O'), ('the', 'O'), ('European', 'B-ORG'), ('Union', 'I-ORG'), ('.', 'O')]\n",
      "Bot: [('He', 'O'), ('said', 'O'), ('a', 'O'), ('proposal', 'O'), ('last', 'O'), ('month', 'O'), ('by', 'O'), ('EU', 'B-ORG'), ('Farm', 'I-ORG'), ('Commissioner', 'I-ORG'), ('Franz', 'B-PER'), ('Fischler', 'I-PER'), ('to', 'O'), ('ban', 'O'), ('sheep', 'O'), ('brains', 'O'), (',', 'O'), ('spleens', 'O'), ('and', 'O'), ('spinal', 'O'), ('cords', 'O'), ('from', 'O'), ('the', 'O'), ('human', 'O'), ('and', 'O'), ('animal', 'O'), ('food', 'O'), ('chains', 'O'), ('was', 'O'), ('a', 'O'), ('highly', 'O'), ('specific', 'O'), ('and', 'O'), ('precautionary', 'O'), ('move', 'O'), ('to', 'O'), ('protect', 'O'), ('human', 'O'), ('health', 'O'), ('.', 'O')]\n",
      "Bot: [('He', 'O'), ('said', 'O'), ('a', 'O'), ('proposal', 'O'), ('last', 'O'), ('month', 'O'), ('by', 'O'), ('EU', 'B-ORG'), ('Farm', 'I-ORG'), ('Commissioner', 'I-ORG'), ('Franz', 'B-PER'), ('Fischler', 'I-PER'), ('to', 'O'), ('ban', 'O'), ('sheep', 'O'), ('brains', 'O'), (',', 'O'), ('spleens', 'O'), ('and', 'O'), ('spinal', 'O'), ('cords', 'O'), ('from', 'O'), ('the', 'O'), ('human', 'O'), ('and', 'O'), ('animal', 'O'), ('food', 'O'), ('chains', 'O'), ('was', 'O'), ('a', 'O'), ('highly', 'O'), ('specific', 'O'), ('and', 'O'), ('precautionary', 'O'), ('move', 'O'), ('to', 'O'), ('protect', 'O'), ('human', 'O'), ('health', 'O'), ('.', 'O')]\n",
      "Bot: [('Fischler', 'B-PER'), ('proposed', 'O'), ('EU-wide', 'B-ORG'), ('measures', 'O'), ('after', 'O'), ('reports', 'O'), ('from', 'O'), ('Britain', 'B-LOC'), ('and', 'O'), ('France', 'B-LOC'), ('that', 'O'), ('under', 'O'), ('laboratory', 'O'), ('conditions', 'O'), ('sheep', 'O'), ('could', 'O'), ('contract', 'O'), ('Bovine', 'B-MISC'), ('Spongiform', 'I-MISC'), ('Encephalopathy', 'I-MISC'), ('(', 'O'), ('BSE', 'B-MISC'), (')', 'O'), ('--', 'O'), ('mad', 'O'), ('cow', 'O'), ('disease', 'O'), ('.', 'O')]\n",
      "Bot: [('Fischler', 'B-PER'), ('proposed', 'O'), ('EU-wide', 'B-ORG'), ('measures', 'O'), ('after', 'O'), ('reports', 'O'), ('from', 'O'), ('Britain', 'B-LOC'), ('and', 'O'), ('France', 'B-LOC'), ('that', 'O'), ('under', 'O'), ('laboratory', 'O'), ('conditions', 'O'), ('sheep', 'O'), ('could', 'O'), ('contract', 'O'), ('Bovine', 'B-MISC'), ('Spongiform', 'I-MISC'), ('Encephalopathy', 'I-MISC'), ('(', 'O'), ('BSE', 'B-MISC'), (')', 'O'), ('--', 'O'), ('mad', 'O'), ('cow', 'O'), ('disease', 'O'), ('.', 'O')]\n",
      "Bot: [('But', 'O'), ('Fischler', 'B-PER'), ('agreed', 'O'), ('to', 'O'), ('review', 'O'), ('his', 'O'), ('proposal', 'O'), ('after', 'O'), ('the', 'O'), ('EU', 'B-ORG'), (\"'s\", 'O'), ('standing', 'O'), ('veterinary', 'O'), ('committee', 'O'), (',', 'O'), ('national', 'O'), ('animal', 'O'), ('health', 'O'), ('officials', 'O'), (',', 'O'), ('questioned', 'O'), ('if', 'O'), ('such', 'O'), ('action', 'O'), ('was', 'O'), ('justified', 'O'), ('as', 'O'), ('there', 'O'), ('was', 'O'), ('only', 'O'), ('a', 'O'), ('slight', 'O'), ('risk', 'O'), ('to', 'O'), ('human', 'O'), ('health', 'O'), ('.', 'O')]\n",
      "Bot: [('But', 'O'), ('Fischler', 'B-PER'), ('agreed', 'O'), ('to', 'O'), ('review', 'O'), ('his', 'O'), ('proposal', 'O'), ('after', 'O'), ('the', 'O'), ('EU', 'B-ORG'), (\"'s\", 'O'), ('standing', 'O'), ('veterinary', 'O'), ('committee', 'O'), (',', 'O'), ('national', 'O'), ('animal', 'O'), ('health', 'O'), ('officials', 'O'), (',', 'O'), ('questioned', 'O'), ('if', 'O'), ('such', 'O'), ('action', 'O'), ('was', 'O'), ('justified', 'O'), ('as', 'O'), ('there', 'O'), ('was', 'O'), ('only', 'O'), ('a', 'O'), ('slight', 'O'), ('risk', 'O'), ('to', 'O'), ('human', 'O'), ('health', 'O'), ('.', 'O')]\n",
      "Bot: [('Spanish', 'B-MISC'), ('Farm', 'I-MISC'), ('Minister', 'I-MISC'), ('Loyola', 'B-PER'), ('de', 'I-PER'), ('Palacio', 'I-PER'), ('had', 'O'), ('earlier', 'O'), ('accused', 'O'), ('Fischler', 'B-PER'), ('at', 'O'), ('an', 'O'), ('EU', 'B-ORG'), ('farm', 'O'), ('ministers', 'O'), (\"'\", 'O'), ('meeting', 'O'), ('of', 'O'), ('causing', 'O'), ('unjustified', 'O'), ('alarm', 'O'), ('through', 'O'), ('\"', 'O'), ('dangerous', 'O'), ('generalisation', 'O'), ('.', 'O'), ('\"', 'O')]\n",
      "Bot: [('Spanish', 'B-MISC'), ('Farm', 'I-MISC'), ('Minister', 'I-MISC'), ('Loyola', 'B-PER'), ('de', 'I-PER'), ('Palacio', 'I-PER'), ('had', 'O'), ('earlier', 'O'), ('accused', 'O'), ('Fischler', 'B-PER'), ('at', 'O'), ('an', 'O'), ('EU', 'B-ORG'), ('farm', 'O'), ('ministers', 'O'), (\"'\", 'O'), ('meeting', 'O'), ('of', 'O'), ('causing', 'O'), ('unjustified', 'O'), ('alarm', 'O'), ('through', 'O'), ('\"', 'O'), ('dangerous', 'O'), ('generalisation', 'O'), ('.', 'O'), ('\"', 'O')]\n",
      "Bot: [('.', 'O')]\n",
      "Bot: [('.', 'O')]\n",
      "Bot: [('Only', 'O'), ('France', 'B-LOC'), ('and', 'O'), ('Britain', 'B-LOC'), ('backed', 'O'), ('Fischler', 'B-PER'), (\"'s\", 'O'), ('proposal', 'O'), ('.', 'O')]\n",
      "Bot: [('Only', 'O'), ('France', 'B-LOC'), ('and', 'O'), ('Britain', 'B-LOC'), ('backed', 'O'), ('Fischler', 'B-PER'), (\"'s\", 'O'), ('proposal', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('EU', 'B-ORG'), (\"'s\", 'O'), ('scientific', 'O'), ('veterinary', 'O'), ('and', 'O'), ('multidisciplinary', 'O'), ('committees', 'O'), ('are', 'O'), ('due', 'O'), ('to', 'O'), ('re-examine', 'O'), ('the', 'O'), ('issue', 'O'), ('early', 'O'), ('next', 'O'), ('month', 'O'), ('and', 'O'), ('make', 'O'), ('recommendations', 'O'), ('to', 'O'), ('the', 'O'), ('senior', 'O'), ('veterinary', 'O'), ('officials', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('EU', 'B-ORG'), (\"'s\", 'O'), ('scientific', 'O'), ('veterinary', 'O'), ('and', 'O'), ('multidisciplinary', 'O'), ('committees', 'O'), ('are', 'O'), ('due', 'O'), ('to', 'O'), ('re-examine', 'O'), ('the', 'O'), ('issue', 'O'), ('early', 'O'), ('next', 'O'), ('month', 'O'), ('and', 'O'), ('make', 'O'), ('recommendations', 'O'), ('to', 'O'), ('the', 'O'), ('senior', 'O'), ('veterinary', 'O'), ('officials', 'O'), ('.', 'O')]\n",
      "Bot: [('Sheep', 'O'), ('have', 'O'), ('long', 'O'), ('been', 'O'), ('known', 'O'), ('to', 'O'), ('contract', 'O'), ('scrapie', 'O'), (',', 'O'), ('a', 'O'), ('brain-wasting', 'O'), ('disease', 'O'), ('similar', 'O'), ('to', 'O'), ('BSE', 'B-MISC'), ('which', 'O'), ('is', 'O'), ('believed', 'O'), ('to', 'O'), ('have', 'O'), ('been', 'O'), ('transferred', 'O'), ('to', 'O'), ('cattle', 'O'), ('through', 'O'), ('feed', 'O'), ('containing', 'O'), ('animal', 'O'), ('waste', 'O'), ('.', 'O')]\n",
      "Bot: [('Sheep', 'O'), ('have', 'O'), ('long', 'O'), ('been', 'O'), ('known', 'O'), ('to', 'O'), ('contract', 'O'), ('scrapie', 'O'), (',', 'O'), ('a', 'O'), ('brain-wasting', 'O'), ('disease', 'O'), ('similar', 'O'), ('to', 'O'), ('BSE', 'B-MISC'), ('which', 'O'), ('is', 'O'), ('believed', 'O'), ('to', 'O'), ('have', 'O'), ('been', 'O'), ('transferred', 'O'), ('to', 'O'), ('cattle', 'O'), ('through', 'O'), ('feed', 'O'), ('containing', 'O'), ('animal', 'O'), ('waste', 'O'), ('.', 'O')]\n",
      "Bot: [('British', 'B-MISC'), ('farmers', 'O'), ('denied', 'O'), ('on', 'O'), ('Thursday', 'O'), ('there', 'O'), ('was', 'O'), ('any', 'O'), ('danger', 'O'), ('to', 'O'), ('human', 'O'), ('health', 'O'), ('from', 'O'), ('their', 'O'), ('sheep', 'O'), (',', 'O'), ('but', 'O'), ('expressed', 'O'), ('concern', 'O'), ('that', 'O'), ('German', 'B-MISC'), ('government', 'O'), ('advice', 'O'), ('to', 'O'), ('consumers', 'O'), ('to', 'O'), ('avoid', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('might', 'O'), ('influence', 'O'), ('consumers', 'O'), ('across', 'O'), ('Europe', 'B-LOC'), ('.', 'O')]\n",
      "Bot: [('British', 'B-MISC'), ('farmers', 'O'), ('denied', 'O'), ('on', 'O'), ('Thursday', 'O'), ('there', 'O'), ('was', 'O'), ('any', 'O'), ('danger', 'O'), ('to', 'O'), ('human', 'O'), ('health', 'O'), ('from', 'O'), ('their', 'O'), ('sheep', 'O'), (',', 'O'), ('but', 'O'), ('expressed', 'O'), ('concern', 'O'), ('that', 'O'), ('German', 'B-MISC'), ('government', 'O'), ('advice', 'O'), ('to', 'O'), ('consumers', 'O'), ('to', 'O'), ('avoid', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('might', 'O'), ('influence', 'O'), ('consumers', 'O'), ('across', 'O'), ('Europe', 'B-LOC'), ('.', 'O')]\n",
      "Bot: [('What', 'O'), ('we', 'O'), ('have', 'O'), ('to', 'O'), ('be', 'O'), ('extremely', 'O'), ('careful', 'O'), ('of', 'O'), ('is', 'O'), ('how', 'O'), ('other', 'O'), ('countries', 'O'), ('are', 'O'), ('going', 'O'), ('to', 'O'), ('take', 'O'), ('Germany', 'B-LOC'), (\"'s\", 'O'), ('lead', 'O'), (',', 'O'), ('\"', 'O'), ('Welsh', 'B-ORG'), ('National', 'I-ORG'), ('Farmers', 'I-ORG'), (\"'\", 'O'), ('Union', 'I-ORG'), ('(', 'O'), ('NFU', 'B-ORG'), (')', 'O'), ('chairman', 'O'), ('John', 'B-PER'), ('Lloyd', 'I-PER'), ('Jones', 'I-PER'), ('said', 'O'), ('on', 'O'), ('BBC', 'B-ORG'), ('radio', 'O'), ('.', 'O')]\n",
      "Bot: [('What', 'O'), ('we', 'O'), ('have', 'O'), ('to', 'O'), ('be', 'O'), ('extremely', 'O'), ('careful', 'O'), ('of', 'O'), ('is', 'O'), ('how', 'O'), ('other', 'O'), ('countries', 'O'), ('are', 'O'), ('going', 'O'), ('to', 'O'), ('take', 'O'), ('Germany', 'B-LOC'), (\"'s\", 'O'), ('lead', 'O'), (',', 'O'), ('\"', 'O'), ('Welsh', 'B-MISC'), ('National', 'B-ORG'), ('Farmers', 'I-ORG'), (\"'\", 'O'), ('Union', 'I-ORG'), ('(', 'O'), ('NFU', 'B-ORG'), (')', 'O'), ('chairman', 'O'), ('John', 'B-PER'), ('Lloyd', 'I-PER'), ('Jones', 'I-PER'), ('said', 'O'), ('on', 'O'), ('BBC', 'B-ORG'), ('radio', 'O'), ('.', 'O')]\n",
      "Bot: [('Bonn', 'B-LOC'), ('has', 'O'), ('led', 'O'), ('efforts', 'O'), ('to', 'O'), ('protect', 'O'), ('public', 'O'), ('health', 'O'), ('after', 'O'), ('consumer', 'O'), ('confidence', 'O'), ('collapsed', 'O'), ('in', 'O'), ('March', 'B-MISC'), ('after', 'O'), ('a', 'O'), ('British', 'B-MISC'), ('report', 'O'), ('suggested', 'O'), ('humans', 'O'), ('could', 'O'), ('contract', 'O'), ('an', 'O'), ('illness', 'O'), ('similar', 'O'), ('to', 'O'), ('mad', 'O'), ('cow', 'O'), ('disease', 'O'), ('by', 'O'), ('eating', 'O'), ('contaminated', 'O'), ('beef', 'O'), ('.', 'O')]\n",
      "Bot: [('Bonn', 'B-LOC'), ('has', 'O'), ('led', 'O'), ('efforts', 'O'), ('to', 'O'), ('protect', 'O'), ('public', 'O'), ('health', 'O'), ('after', 'O'), ('consumer', 'O'), ('confidence', 'O'), ('collapsed', 'O'), ('in', 'O'), ('March', 'B-MISC'), ('after', 'O'), ('a', 'O'), ('British', 'B-MISC'), ('report', 'O'), ('suggested', 'O'), ('humans', 'O'), ('could', 'O'), ('contract', 'O'), ('an', 'O'), ('illness', 'O'), ('similar', 'O'), ('to', 'O'), ('mad', 'B-MISC'), ('cow', 'I-MISC'), ('disease', 'O'), ('by', 'O'), ('eating', 'O'), ('contaminated', 'O'), ('beef', 'O'), ('.', 'O')]\n",
      "Bot: [('Germany', 'B-LOC'), ('imported', 'O'), ('47,600', 'O'), ('sheep', 'O'), ('from', 'O'), ('Britain', 'B-LOC'), ('last', 'O'), ('year', 'O'), (',', 'O'), ('nearly', 'O'), ('half', 'O'), ('of', 'O'), ('total', 'O'), ('imports', 'O'), ('.', 'O')]\n",
      "Bot: [('Germany', 'B-LOC'), ('imported', 'O'), ('47,600', 'O'), ('sheep', 'O'), ('from', 'O'), ('Britain', 'B-LOC'), ('last', 'O'), ('year', 'O'), (',', 'O'), ('nearly', 'O'), ('half', 'O'), ('of', 'O'), ('total', 'O'), ('imports', 'O'), ('.', 'O')]\n",
      "Bot: [('It', 'O'), ('brought', 'O'), ('in', 'O'), ('4,275', 'O'), ('tonnes', 'O'), ('of', 'O'), ('British', 'B-MISC'), ('mutton', 'O'), (',', 'O'), ('some', 'O'), ('10', 'O'), ('percent', 'O'), ('of', 'O'), ('overall', 'O'), ('imports', 'O'), ('.', 'O')]\n",
      "Bot: [('It', 'O'), ('brought', 'O'), ('in', 'O'), ('4,275', 'O'), ('tonnes', 'O'), ('of', 'O'), ('British', 'B-MISC'), ('mutton', 'O'), (',', 'O'), ('some', 'O'), ('10', 'O'), ('percent', 'O'), ('of', 'O'), ('overall', 'O'), ('imports', 'O'), ('.', 'O')]\n",
      "Bot: [('Rare', 'O'), ('Hendrix', 'B-PER'), ('song', 'O'), ('draft', 'O'), ('sells', 'O'), ('for', 'O'), ('almost', 'O'), ('$', 'O'), ('17,000', 'O'), ('.', 'O')]\n",
      "Bot: [('Rare', 'O'), ('Hendrix', 'B-PER'), ('song', 'O'), ('draft', 'O'), ('sells', 'O'), ('for', 'O'), ('almost', 'O'), ('$', 'O'), ('17,000', 'O'), ('.', 'O')]\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('A', 'O'), ('rare', 'O'), ('early', 'O'), ('handwritten', 'O'), ('draft', 'O'), ('of', 'O'), ('a', 'O'), ('song', 'O'), ('by', 'O'), ('U.S.', 'B-ORG'), ('guitar', 'O'), ('legend', 'O'), ('Jimi', 'B-PER'), ('Hendrix', 'I-PER'), ('was', 'O'), ('sold', 'O'), ('for', 'O'), ('almost', 'O'), ('$', 'O'), ('17,000', 'O'), ('on', 'O'), ('Thursday', 'B-MISC'), ('at', 'O'), ('an', 'O'), ('auction', 'O'), ('of', 'O'), ('some', 'O'), ('of', 'O'), ('the', 'O'), ('late', 'O'), ('musician', 'O'), (\"'s\", 'O'), ('favourite', 'O'), ('possessions', 'O'), ('.', 'O')]\n",
      "Bot: [('A', 'O'), ('rare', 'O'), ('early', 'O'), ('handwritten', 'O'), ('draft', 'O'), ('of', 'O'), ('a', 'O'), ('song', 'O'), ('by', 'O'), ('U.S.', 'B-ORG'), ('guitar', 'O'), ('legend', 'O'), ('Jimi', 'B-PER'), ('Hendrix', 'I-PER'), ('was', 'O'), ('sold', 'O'), ('for', 'O'), ('almost', 'O'), ('$', 'O'), ('17,000', 'O'), ('on', 'O'), ('Thursday', 'B-MISC'), ('at', 'O'), ('an', 'O'), ('auction', 'O'), ('of', 'O'), ('some', 'O'), ('of', 'O'), ('the', 'O'), ('late', 'O'), ('musician', 'O'), (\"'s\", 'O'), ('favourite', 'O'), ('possessions', 'O'), ('.', 'O')]\n",
      "Bot: [('A', 'O'), ('Florida', 'B-LOC'), ('restaurant', 'O'), ('paid', 'O'), ('10,925', 'O'), ('pounds', 'O'), ('(', 'O'), ('$', 'O'), ('16,935', 'O'), (')', 'O'), ('for', 'O'), ('the', 'O'), ('draft', 'O'), ('of', 'O'), ('\"', 'O'), ('Ai', 'B-MISC'), (\"n't\", 'I-MISC'), ('no', 'I-MISC'), ('telling', 'I-MISC'), ('\"', 'O'), (',', 'O'), ('which', 'O'), ('Hendrix', 'B-PER'), ('penned', 'O'), ('on', 'O'), ('a', 'O'), ('piece', 'O'), ('of', 'O'), ('London', 'B-LOC'), ('hotel', 'O'), ('stationery', 'O'), ('in', 'O'), ('late', 'O'), ('1966', 'O'), ('.', 'O')]\n",
      "Bot: [('A', 'O'), ('Florida', 'B-LOC'), ('restaurant', 'O'), ('paid', 'O'), ('10,925', 'O'), ('pounds', 'O'), ('(', 'O'), ('$', 'O'), ('16,935', 'O'), (')', 'O'), ('for', 'O'), ('the', 'O'), ('draft', 'O'), ('of', 'O'), ('\"', 'O'), ('Ai', 'B-MISC'), (\"n't\", 'I-MISC'), ('no', 'I-MISC'), ('telling', 'I-MISC'), ('\"', 'O'), (',', 'O'), ('which', 'O'), ('Hendrix', 'B-PER'), ('penned', 'O'), ('on', 'O'), ('a', 'O'), ('piece', 'O'), ('of', 'O'), ('London', 'B-LOC'), ('hotel', 'O'), ('stationery', 'O'), ('in', 'O'), ('late', 'O'), ('1966', 'O'), ('.', 'O')]\n",
      "Bot: [('At', 'O'), ('the', 'O'), ('end', 'O'), ('of', 'O'), ('a', 'O'), ('January', 'B-MISC'), ('1967', 'I-MISC'), ('concert', 'O'), ('in', 'O'), ('the', 'O'), ('English', 'B-MISC'), ('city', 'O'), ('of', 'O'), ('Nottingham', 'B-LOC'), ('he', 'O'), ('threw', 'O'), ('the', 'O'), ('sheet', 'O'), ('of', 'O'), ('paper', 'O'), ('into', 'O'), ('the', 'O'), ('audience', 'O'), (',', 'O'), ('where', 'O'), ('it', 'O'), ('was', 'O'), ('retrieved', 'O'), ('by', 'O'), ('a', 'O'), ('fan', 'O'), ('.', 'O')]\n",
      "Bot: [('At', 'O'), ('the', 'O'), ('end', 'O'), ('of', 'O'), ('a', 'O'), ('January', 'B-MISC'), ('1967', 'O'), ('concert', 'O'), ('in', 'O'), ('the', 'O'), ('English', 'B-MISC'), ('city', 'O'), ('of', 'O'), ('Nottingham', 'B-LOC'), ('he', 'O'), ('threw', 'O'), ('the', 'O'), ('sheet', 'O'), ('of', 'O'), ('paper', 'O'), ('into', 'O'), ('the', 'O'), ('audience', 'O'), (',', 'O'), ('where', 'O'), ('it', 'O'), ('was', 'O'), ('retrieved', 'O'), ('by', 'O'), ('a', 'O'), ('fan', 'O'), ('.', 'O')]\n",
      "Bot: [('Buyers', 'O'), ('also', 'O'), ('snapped', 'O'), ('up', 'O'), ('16', 'O'), ('other', 'O'), ('items', 'O'), ('that', 'O'), ('were', 'O'), ('put', 'O'), ('up', 'O'), ('for', 'O'), ('auction', 'O'), ('by', 'O'), ('Hendrix', 'B-PER'), (\"'s\", 'O'), ('former', 'O'), ('girlfriend', 'O'), ('Kathy', 'B-PER'), ('Etchingham', 'I-PER'), (',', 'O'), ('who', 'O'), ('lived', 'O'), ('with', 'O'), ('him', 'O'), ('from', 'O'), ('1966', 'O'), ('to', 'O'), ('1969', 'O'), ('.', 'O')]\n",
      "Bot: [('Buyers', 'O'), ('also', 'O'), ('snapped', 'O'), ('up', 'O'), ('16', 'O'), ('other', 'O'), ('items', 'O'), ('that', 'O'), ('were', 'O'), ('put', 'O'), ('up', 'O'), ('for', 'O'), ('auction', 'O'), ('by', 'O'), ('Hendrix', 'B-PER'), (\"'s\", 'O'), ('former', 'O'), ('girlfriend', 'O'), ('Kathy', 'B-PER'), ('Etchingham', 'I-PER'), (',', 'O'), ('who', 'O'), ('lived', 'O'), ('with', 'O'), ('him', 'O'), ('from', 'O'), ('1966', 'O'), ('to', 'O'), ('1969', 'O'), ('.', 'O')]\n",
      "Bot: [('They', 'O'), ('included', 'O'), ('a', 'O'), ('black', 'O'), ('lacquer', 'O'), ('and', 'O'), ('mother', 'O'), ('of', 'O'), ('pearl', 'O'), ('inlaid', 'O'), ('box', 'O'), ('used', 'O'), ('by', 'O'), ('Hendrix', 'B-PER'), ('to', 'O'), ('store', 'O'), ('his', 'O'), ('drugs', 'O'), (',', 'O'), ('which', 'O'), ('an', 'O'), ('anonymous', 'O'), ('Australian', 'B-MISC'), ('purchaser', 'O'), ('bought', 'O'), ('for', 'O'), ('5,060', 'O'), ('pounds', 'O'), ('(', 'O'), ('$', 'O'), ('7,845', 'O'), (')', 'O'), ('.', 'O')]\n",
      "Bot: [('They', 'O'), ('included', 'O'), ('a', 'O'), ('black', 'O'), ('lacquer', 'O'), ('and', 'O'), ('mother', 'O'), ('of', 'O'), ('pearl', 'O'), ('inlaid', 'O'), ('box', 'O'), ('used', 'O'), ('by', 'O'), ('Hendrix', 'B-PER'), ('to', 'O'), ('store', 'O'), ('his', 'O'), ('drugs', 'O'), (',', 'O'), ('which', 'O'), ('an', 'O'), ('anonymous', 'O'), ('Australian', 'B-MISC'), ('purchaser', 'O'), ('bought', 'O'), ('for', 'O'), ('5,060', 'O'), ('pounds', 'O'), ('(', 'O'), ('$', 'O'), ('7,845', 'O'), (')', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('guitarist', 'O'), ('died', 'O'), ('of', 'O'), ('a', 'O'), ('drugs', 'O'), ('overdose', 'O'), ('in', 'O'), ('1970', 'O'), ('aged', 'O'), ('27', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('guitarist', 'O'), ('died', 'O'), ('of', 'O'), ('a', 'O'), ('drugs', 'O'), ('overdose', 'O'), ('in', 'O'), ('1970', 'O'), ('aged', 'O'), ('27', 'O'), ('.', 'O')]\n",
      "Bot: [('China', 'B-LOC'), ('says', 'O'), ('Taiwan', 'B-LOC'), ('spoils', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: [('China', 'B-LOC'), ('says', 'O'), ('Taiwan', 'B-LOC'), ('spoils', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: [('BEIJING', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('BEIJING', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('China', 'B-LOC'), ('on', 'O'), ('Thursday', 'O'), ('accused', 'O'), ('Taipei', 'B-LOC'), ('of', 'O'), ('spoiling', 'O'), ('the', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('a', 'O'), ('resumption', 'O'), ('of', 'O'), ('talks', 'O'), ('across', 'O'), ('the', 'O'), ('Taiwan', 'B-LOC'), ('Strait', 'I-LOC'), ('with', 'O'), ('a', 'O'), ('visit', 'O'), ('to', 'O'), ('Ukraine', 'B-LOC'), ('by', 'O'), ('Taiwanese', 'B-MISC'), ('Vice', 'I-MISC'), ('President', 'I-MISC'), ('Lien', 'B-PER'), ('Chan', 'I-PER'), ('this', 'O'), ('week', 'O'), ('that', 'O'), ('infuriated', 'O'), ('Beijing', 'B-LOC'), ('.', 'O')]\n",
      "Bot: [('China', 'B-LOC'), ('on', 'O'), ('Thursday', 'O'), ('accused', 'O'), ('Taipei', 'B-LOC'), ('of', 'O'), ('spoiling', 'O'), ('the', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('a', 'O'), ('resumption', 'O'), ('of', 'O'), ('talks', 'O'), ('across', 'O'), ('the', 'O'), ('Taiwan', 'B-LOC'), ('Strait', 'I-LOC'), ('with', 'O'), ('a', 'O'), ('visit', 'O'), ('to', 'O'), ('Ukraine', 'B-LOC'), ('by', 'O'), ('Taiwanese', 'B-MISC'), ('Vice', 'I-MISC'), ('President', 'I-MISC'), ('Lien', 'B-PER'), ('Chan', 'I-PER'), ('this', 'O'), ('week', 'O'), ('that', 'O'), ('infuriated', 'O'), ('Beijing', 'B-LOC'), ('.', 'O')]\n",
      "Bot: [('Speaking', 'O'), ('only', 'O'), ('hours', 'O'), ('after', 'O'), ('Chinese', 'B-LOC'), ('state', 'I-LOC'), ('media', 'I-LOC'), ('said', 'O'), ('the', 'O'), ('time', 'O'), ('was', 'O'), ('right', 'O'), ('to', 'O'), ('engage', 'O'), ('in', 'O'), ('political', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), (',', 'O'), ('Foreign', 'B-ORG'), ('Ministry', 'I-ORG'), ('spokesman', 'O'), ('Shen', 'B-PER'), ('Guofang', 'I-PER'), ('told', 'O'), ('Reuters', 'B-ORG'), (':', 'O'), ('\"', 'O'), ('The', 'O'), ('necessary', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('the', 'O'), ('opening', 'O'), ('of', 'O'), ('the', 'O'), ('talks', 'O'), ('has', 'O'), ('been', 'O'), ('disrupted', 'O'), ('by', 'O'), ('the', 'O'), ('Taiwan', 'B-LOC'), ('authorities', 'O'), ('.', 'O'), ('\"', 'O')]\n",
      "Bot: [('Speaking', 'O'), ('only', 'O'), ('hours', 'O'), ('after', 'O'), ('Chinese', 'B-LOC'), ('state', 'I-LOC'), ('media', 'I-LOC'), ('said', 'O'), ('the', 'O'), ('time', 'O'), ('was', 'O'), ('right', 'O'), ('to', 'O'), ('engage', 'O'), ('in', 'O'), ('political', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), (',', 'O'), ('Foreign', 'B-ORG'), ('Ministry', 'I-ORG'), ('spokesman', 'O'), ('Shen', 'B-PER'), ('Guofang', 'I-PER'), ('told', 'O'), ('Reuters', 'B-ORG'), (':', 'O'), ('\"', 'O'), ('The', 'O'), ('necessary', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('the', 'O'), ('opening', 'O'), ('of', 'O'), ('the', 'O'), ('talks', 'O'), ('has', 'O'), ('been', 'O'), ('disrupted', 'O'), ('by', 'O'), ('the', 'O'), ('Taiwan', 'B-LOC'), ('authorities', 'O'), ('.', 'O'), ('\"', 'O')]\n",
      "Bot: [('State', 'B-ORG'), ('media', 'I-ORG'), ('quoted', 'O'), ('China', 'B-LOC'), (\"'s\", 'O'), ('top', 'O'), ('negotiator', 'O'), ('with', 'O'), ('Taipei', 'B-LOC'), (',', 'O'), ('Tang', 'B-PER'), ('Shubei', 'I-PER'), (',', 'O'), ('as', 'O'), ('telling', 'O'), ('a', 'O'), ('visiting', 'O'), ('group', 'O'), ('from', 'O'), ('Taiwan', 'B-LOC'), ('on', 'O'), ('Wednesday', 'O'), ('that', 'O'), ('it', 'O'), ('was', 'O'), ('time', 'O'), ('for', 'O'), ('the', 'O'), ('rivals', 'O'), ('to', 'O'), ('hold', 'O'), ('political', 'O'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: [('State', 'B-ORG'), ('media', 'I-ORG'), ('quoted', 'O'), ('China', 'B-LOC'), (\"'s\", 'O'), ('top', 'O'), ('negotiator', 'O'), ('with', 'O'), ('Taipei', 'B-LOC'), (',', 'O'), ('Tang', 'B-PER'), ('Shubei', 'I-PER'), (',', 'O'), ('as', 'O'), ('telling', 'O'), ('a', 'O'), ('visiting', 'O'), ('group', 'O'), ('from', 'O'), ('Taiwan', 'B-LOC'), ('on', 'O'), ('Wednesday', 'O'), ('that', 'O'), ('it', 'O'), ('was', 'O'), ('time', 'O'), ('for', 'O'), ('the', 'O'), ('rivals', 'O'), ('to', 'O'), ('hold', 'O'), ('political', 'O'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: [('Now', 'O'), ('is', 'O'), ('the', 'O'), ('time', 'O'), ('for', 'O'), ('the', 'O'), ('two', 'O'), ('sides', 'O'), ('to', 'O'), ('engage', 'O'), ('in', 'O'), ('political', 'O'), ('talks', 'O')]\n",
      "Bot: [('Now', 'O'), ('is', 'O'), ('the', 'O'), ('time', 'O'), ('for', 'O'), ('the', 'O'), ('two', 'O'), ('sides', 'O'), ('to', 'O'), ('engage', 'O'), ('in', 'O'), ('political', 'O'), ('talks', 'O')]\n",
      "Bot: [('that', 'O'), ('is', 'O'), ('to', 'O'), ('end', 'O'), ('the', 'O'), ('state', 'O'), ('of', 'O'), ('hostility', 'O'), (',', 'O'), ('\"', 'O'), ('Thursday', 'B-MISC'), (\"'s\", 'O'), ('overseas', 'O'), ('edition', 'O'), ('of', 'O'), ('the', 'O'), ('People', 'B-ORG'), (\"'s\", 'I-ORG'), ('Daily', 'I-ORG'), ('quoted', 'O'), ('Tang', 'B-PER'), ('as', 'O'), ('saying', 'O'), ('.', 'O')]\n",
      "Bot: [('that', 'O'), ('is', 'O'), ('to', 'O'), ('end', 'O'), ('the', 'O'), ('state', 'O'), ('of', 'O'), ('hostility', 'O'), (',', 'O'), ('\"', 'O'), ('Thursday', 'B-MISC'), (\"'s\", 'O'), ('overseas', 'O'), ('edition', 'O'), ('of', 'O'), ('the', 'O'), ('People', 'B-ORG'), (\"'s\", 'I-ORG'), ('Daily', 'I-ORG'), ('quoted', 'O'), ('Tang', 'B-PER'), ('as', 'O'), ('saying', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('foreign', 'B-MISC'), ('ministry', 'I-MISC'), (\"'s\", 'O'), ('Shen', 'B-PER'), ('told', 'O'), ('Reuters', 'B-ORG'), ('Television', 'I-ORG'), ('in', 'O'), ('an', 'O'), ('interview', 'O'), ('he', 'O'), ('had', 'O'), ('read', 'O'), ('reports', 'O'), ('of', 'O'), ('Tang', 'B-PER'), (\"'s\", 'O'), ('comments', 'O'), ('but', 'O'), ('gave', 'O'), ('no', 'O'), ('details', 'O'), ('of', 'O'), ('why', 'O'), ('the', 'O'), ('negotiator', 'O'), ('had', 'O'), ('considered', 'O'), ('the', 'O'), ('time', 'O'), ('right', 'O'), ('for', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), (',', 'O'), ('which', 'O'), ('Beijing', 'B-LOC'), ('considers', 'O'), ('a', 'O'), ('renegade', 'O'), ('province', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('foreign', 'B-ORG'), ('ministry', 'I-ORG'), (\"'s\", 'O'), ('Shen', 'B-PER'), ('told', 'O'), ('Reuters', 'B-ORG'), ('Television', 'I-ORG'), ('in', 'O'), ('an', 'O'), ('interview', 'O'), ('he', 'O'), ('had', 'O'), ('read', 'O'), ('reports', 'O'), ('of', 'O'), ('Tang', 'B-PER'), (\"'s\", 'O'), ('comments', 'O'), ('but', 'O'), ('gave', 'O'), ('no', 'O'), ('details', 'O'), ('of', 'O'), ('why', 'O'), ('the', 'O'), ('negotiator', 'O'), ('had', 'O'), ('considered', 'O'), ('the', 'O'), ('time', 'O'), ('right', 'O'), ('for', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), (',', 'O'), ('which', 'O'), ('Beijing', 'B-LOC'), ('considers', 'O'), ('a', 'O'), ('renegade', 'O'), ('province', 'O'), ('.', 'O')]\n",
      "Bot: [('China', 'B-LOC'), (',', 'O'), ('which', 'O'), ('has', 'O'), ('long', 'O'), ('opposed', 'O'), ('all', 'O'), ('Taipei', 'B-LOC'), ('efforts', 'O'), ('to', 'O'), ('gain', 'O'), ('greater', 'O'), ('international', 'O'), ('recognition', 'O'), (',', 'O'), ('was', 'O'), ('infuriated', 'O'), ('by', 'O'), ('a', 'O'), ('visit', 'O'), ('to', 'O'), ('Ukraine', 'B-LOC'), ('this', 'O'), ('week', 'O'), ('by', 'O'), ('Taiwanese', 'B-MISC'), ('Vice', 'I-MISC'), ('President', 'I-MISC'), ('Lien', 'I-PER'), ('.', 'O')]\n",
      "Bot: [('China', 'B-LOC'), (',', 'O'), ('which', 'O'), ('has', 'O'), ('long', 'O'), ('opposed', 'O'), ('all', 'O'), ('Taipei', 'B-LOC'), ('efforts', 'O'), ('to', 'O'), ('gain', 'O'), ('greater', 'O'), ('international', 'O'), ('recognition', 'O'), (',', 'O'), ('was', 'O'), ('infuriated', 'O'), ('by', 'O'), ('a', 'O'), ('visit', 'O'), ('to', 'O'), ('Ukraine', 'B-LOC'), ('this', 'O'), ('week', 'O'), ('by', 'O'), ('Taiwanese', 'B-MISC'), ('Vice', 'I-MISC'), ('President', 'I-MISC'), ('Lien', 'B-PER'), ('.', 'O')]\n",
      "Bot: [('China', 'B-LOC'), ('says', 'O'), ('time', 'O'), ('right', 'O'), ('for', 'O'), ('Taiwan', 'B-LOC'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: [('China', 'B-LOC'), ('says', 'O'), ('time', 'O'), ('right', 'O'), ('for', 'O'), ('Taiwan', 'B-LOC'), ('talks', 'O'), ('.', 'O')]\n",
      "Bot: [('BEIJING', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('BEIJING', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('China', 'B-LOC'), ('has', 'O'), ('said', 'O'), ('it', 'O'), ('was', 'O'), ('time', 'O'), ('for', 'O'), ('political', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), ('and', 'O'), ('that', 'O'), ('the', 'O'), ('rival', 'O'), ('island', 'O'), ('should', 'O'), ('take', 'O'), ('practical', 'O'), ('steps', 'O'), ('towards', 'O'), ('that', 'O'), ('goal', 'O'), ('.', 'O')]\n",
      "Bot: [('China', 'B-LOC'), ('has', 'O'), ('said', 'O'), ('it', 'O'), ('was', 'O'), ('time', 'O'), ('for', 'O'), ('political', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), ('and', 'O'), ('that', 'O'), ('the', 'O'), ('rival', 'O'), ('island', 'O'), ('should', 'O'), ('take', 'O'), ('practical', 'O'), ('steps', 'O'), ('towards', 'O'), ('that', 'O'), ('goal', 'O'), ('.', 'O')]\n",
      "Bot: [('Consultations', 'O'), ('should', 'O'), ('be', 'O'), ('held', 'O'), ('to', 'O'), ('set', 'O'), ('the', 'O'), ('time', 'O'), ('and', 'O'), ('format', 'O'), ('of', 'O'), ('the', 'O'), ('talks', 'O'), (',', 'O'), ('the', 'O'), ('official', 'O'), ('Xinhua', 'B-ORG'), ('news', 'I-ORG'), ('agency', 'I-ORG'), ('quoted', 'O'), ('Tang', 'B-PER'), ('Shubei', 'I-PER'), (',', 'O'), ('executive', 'O'), ('vice', 'O'), ('chairman', 'O'), ('of', 'O'), ('the', 'O'), ('Association', 'B-ORG'), ('for', 'I-ORG'), ('Relations', 'I-ORG'), ('Across', 'I-ORG'), ('the', 'I-ORG'), ('Taiwan', 'I-ORG'), ('Straits', 'I-ORG'), (',', 'O'), ('as', 'O'), ('saying', 'O'), ('late', 'O'), ('on', 'O'), ('Wednesday', 'O'), ('.', 'O')]\n",
      "Bot: [('Consultations', 'O'), ('should', 'O'), ('be', 'O'), ('held', 'O'), ('to', 'O'), ('set', 'O'), ('the', 'O'), ('time', 'O'), ('and', 'O'), ('format', 'O'), ('of', 'O'), ('the', 'O'), ('talks', 'O'), (',', 'O'), ('the', 'O'), ('official', 'O'), ('Xinhua', 'B-ORG'), ('news', 'I-ORG'), ('agency', 'I-ORG'), ('quoted', 'O'), ('Tang', 'B-PER'), ('Shubei', 'I-PER'), (',', 'O'), ('executive', 'O'), ('vice', 'O'), ('chairman', 'O'), ('of', 'O'), ('the', 'O'), ('Association', 'B-ORG'), ('for', 'I-ORG'), ('Relations', 'I-ORG'), ('Across', 'I-ORG'), ('the', 'I-ORG'), ('Taiwan', 'B-LOC'), ('Straits', 'I-LOC'), (',', 'O'), ('as', 'O'), ('saying', 'O'), ('late', 'O'), ('on', 'O'), ('Wednesday', 'O'), ('.', 'O')]\n",
      "Bot: [('German', 'B-MISC'), ('July', 'B-MISC'), ('car', 'O'), ('registrations', 'O'), ('up', 'O'), ('14.2', 'O'), ('pct', 'O'), ('yr', 'O'), ('/', 'O'), ('yr', 'O'), ('.', 'O')]\n",
      "Bot: [('German', 'B-MISC'), ('July', 'O'), ('car', 'O'), ('registrations', 'O'), ('up', 'O'), ('14.2', 'O'), ('pct', 'O'), ('yr', 'O'), ('/', 'O'), ('yr', 'O'), ('.', 'O')]\n",
      "Bot: [('FRANKFURT', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('FRANKFURT', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('German', 'B-MISC'), ('first-time', 'O'), ('registrations', 'O'), ('of', 'O'), ('motor', 'O'), ('vehicles', 'O'), ('jumped', 'O'), ('14.2', 'O'), ('percent', 'O'), ('in', 'O'), ('July', 'O'), ('this', 'O'), ('year', 'O'), ('from', 'O'), ('the', 'O'), ('year-earlier', 'O'), ('period', 'O'), (',', 'O'), ('the', 'O'), ('Federal', 'B-ORG'), ('office', 'I-ORG'), ('for', 'I-ORG'), ('motor', 'I-ORG'), ('vehicles', 'I-ORG'), ('said', 'O'), ('on', 'O'), ('Thursday', 'O'), ('.', 'O')]\n",
      "Bot: [('German', 'B-MISC'), ('first-time', 'O'), ('registrations', 'O'), ('of', 'O'), ('motor', 'O'), ('vehicles', 'O'), ('jumped', 'O'), ('14.2', 'O'), ('percent', 'O'), ('in', 'O'), ('July', 'O'), ('this', 'O'), ('year', 'O'), ('from', 'O'), ('the', 'O'), ('year-earlier', 'O'), ('period', 'O'), (',', 'O'), ('the', 'O'), ('Federal', 'B-ORG'), ('office', 'I-ORG'), ('for', 'I-ORG'), ('motor', 'I-ORG'), ('vehicles', 'I-ORG'), ('said', 'O'), ('on', 'O'), ('Thursday', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('office', 'O'), ('said', 'O'), ('356,725', 'O'), ('new', 'O'), ('cars', 'O'), ('were', 'O'), ('registered', 'O'), ('in', 'O'), ('July', 'B-MISC'), ('1996', 'I-MISC'), ('--', 'O'), ('304,850', 'O'), ('passenger', 'O'), ('cars', 'O'), ('and', 'O'), ('15,613', 'O'), ('trucks', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('office', 'O'), ('said', 'O'), ('356,725', 'O'), ('new', 'O'), ('cars', 'O'), ('were', 'O'), ('registered', 'O'), ('in', 'O'), ('July', 'O'), ('1996', 'O'), ('--', 'O'), ('304,850', 'O'), ('passenger', 'O'), ('cars', 'O'), ('and', 'O'), ('15,613', 'O'), ('trucks', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('figures', 'O'), ('represent', 'O'), ('a', 'O'), ('13.6', 'O'), ('percent', 'O'), ('increase', 'O'), ('for', 'O'), ('passenger', 'O'), ('cars', 'O'), ('and', 'O'), ('a', 'O'), ('2.2', 'O'), ('percent', 'O'), ('decline', 'O'), ('for', 'O'), ('trucks', 'O'), ('from', 'O'), ('July', 'B-MISC'), ('1995', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('figures', 'O'), ('represent', 'O'), ('a', 'O'), ('13.6', 'O'), ('percent', 'O'), ('increase', 'O'), ('for', 'O'), ('passenger', 'O'), ('cars', 'O'), ('and', 'O'), ('a', 'O'), ('2.2', 'O'), ('percent', 'O'), ('decline', 'O'), ('for', 'O'), ('trucks', 'O'), ('from', 'O'), ('July', 'B-MISC'), ('1995', 'O'), ('.', 'O')]\n",
      "Bot: [('Motor-bike', 'O'), ('registration', 'O'), ('rose', 'O'), ('32.7', 'O'), ('percent', 'O'), ('in', 'O'), ('the', 'O'), ('period', 'O'), ('.', 'O')]\n",
      "Bot: [('Motor-bike', 'O'), ('registration', 'O'), ('rose', 'O'), ('32.7', 'O'), ('percent', 'O'), ('in', 'O'), ('the', 'O'), ('period', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('growth', 'O'), ('was', 'O'), ('partly', 'O'), ('due', 'O'), ('to', 'O'), ('an', 'O'), ('increased', 'O'), ('number', 'O'), ('of', 'O'), ('Germans', 'B-MISC'), ('buying', 'O'), ('German', 'B-MISC'), ('cars', 'O'), ('abroad', 'O'), (',', 'O'), ('while', 'O'), ('manufacturers', 'O'), ('said', 'O'), ('that', 'O'), ('domestic', 'O'), ('demand', 'O'), ('was', 'O'), ('weak', 'O'), (',', 'O'), ('the', 'O'), ('federal', 'B-ORG'), ('office', 'I-ORG'), ('said', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('growth', 'O'), ('was', 'O'), ('partly', 'O'), ('due', 'O'), ('to', 'O'), ('an', 'O'), ('increased', 'O'), ('number', 'O'), ('of', 'O'), ('Germans', 'B-MISC'), ('buying', 'O'), ('German', 'B-MISC'), ('cars', 'O'), ('abroad', 'O'), (',', 'O'), ('while', 'O'), ('manufacturers', 'O'), ('said', 'O'), ('that', 'O'), ('domestic', 'O'), ('demand', 'O'), ('was', 'O'), ('weak', 'O'), (',', 'O'), ('the', 'O'), ('federal', 'B-ORG'), ('office', 'I-ORG'), ('said', 'O'), ('.', 'O')]\n",
      "Bot: [('Almost', 'O'), ('all', 'O'), ('German', 'B-MISC'), ('car', 'O'), ('manufacturers', 'O'), ('posted', 'O'), ('gains', 'O'), ('in', 'O'), ('registration', 'O'), ('numbers', 'O'), ('in', 'O'), ('the', 'O'), ('period', 'O'), ('.', 'O')]\n",
      "Bot: [('Almost', 'O'), ('all', 'O'), ('German', 'B-MISC'), ('car', 'O'), ('manufacturers', 'O'), ('posted', 'O'), ('gains', 'O'), ('in', 'O'), ('registration', 'O'), ('numbers', 'O'), ('in', 'O'), ('the', 'O'), ('period', 'O'), ('.', 'O')]\n",
      "Bot: [('Volkswagen', 'B-ORG'), ('AG', 'I-ORG'), ('won', 'O'), ('77,719', 'O'), ('registrations', 'O'), (',', 'O'), ('slightly', 'O'), ('more', 'O'), ('than', 'O'), ('a', 'O'), ('quarter', 'O'), ('of', 'O'), ('the', 'O'), ('total', 'O'), ('.', 'O')]\n",
      "Bot: [('Volkswagen', 'B-ORG'), ('AG', 'I-ORG'), ('won', 'O'), ('77,719', 'O'), ('registrations', 'O'), (',', 'O'), ('slightly', 'O'), ('more', 'O'), ('than', 'O'), ('a', 'O'), ('quarter', 'O'), ('of', 'O'), ('the', 'O'), ('total', 'O'), ('.', 'O')]\n",
      "Bot: [('Opel', 'B-ORG'), ('AG', 'I-ORG'), ('together', 'O'), ('with', 'O'), ('General', 'B-ORG'), ('Motors', 'I-ORG'), ('came', 'O'), ('in', 'O'), ('second', 'O'), ('place', 'O'), ('with', 'O'), ('49,269', 'O'), ('registrations', 'O'), (',', 'O'), ('16.4', 'O'), ('percent', 'O'), ('of', 'O'), ('the', 'O'), ('overall', 'O'), ('figure', 'O'), ('.', 'O')]\n",
      "Bot: [('Opel', 'B-ORG'), ('AG', 'I-ORG'), ('together', 'O'), ('with', 'O'), ('General', 'B-ORG'), ('Motors', 'I-ORG'), ('came', 'O'), ('in', 'O'), ('second', 'O'), ('place', 'O'), ('with', 'O'), ('49,269', 'O'), ('registrations', 'O'), (',', 'O'), ('16.4', 'O'), ('percent', 'O'), ('of', 'O'), ('the', 'O'), ('overall', 'O'), ('figure', 'O'), ('.', 'O')]\n",
      "Bot: [('Third', 'O'), ('was', 'O'), ('Ford', 'B-ORG'), ('with', 'O'), ('35,563', 'O'), ('registrations', 'O'), (',', 'O'), ('or', 'O'), ('11.7', 'O'), ('percent', 'O'), ('.', 'O')]\n",
      "Bot: [('Third', 'O'), ('was', 'O'), ('Ford', 'B-ORG'), ('with', 'O'), ('35,563', 'O'), ('registrations', 'O'), (',', 'O'), ('or', 'O'), ('11.7', 'O'), ('percent', 'O'), ('.', 'O')]\n",
      "Bot: [('Only', 'O'), ('Seat', 'B-ORG'), ('and', 'O'), ('Porsche', 'B-ORG'), ('had', 'O'), ('fewer', 'O'), ('registrations', 'O'), ('in', 'O'), ('July', 'B-MISC'), ('1996', 'O'), ('compared', 'O'), ('to', 'O'), ('last', 'O'), ('year', 'O'), (\"'s\", 'O'), ('July', 'B-MISC'), ('.', 'O')]\n",
      "Bot: [('Only', 'O'), ('Seat', 'B-ORG'), ('and', 'O'), ('Porsche', 'B-ORG'), ('had', 'O'), ('fewer', 'O'), ('registrations', 'O'), ('in', 'O'), ('July', 'B-MISC'), ('1996', 'O'), ('compared', 'O'), ('to', 'O'), ('last', 'O'), ('year', 'O'), (\"'s\", 'O'), ('July', 'B-MISC'), ('.', 'O')]\n",
      "Bot: [('Seat', 'B-ORG'), ('posted', 'O'), ('3,420', 'O'), ('registrations', 'O'), ('compared', 'O'), ('with', 'O'), ('5522', 'O'), ('registrations', 'O'), ('in', 'O'), ('July', 'B-MISC'), ('a', 'O'), ('year', 'O'), ('earlier', 'O'), ('.', 'O')]\n",
      "Bot: [('Seat', 'B-ORG'), ('posted', 'O'), ('3,420', 'O'), ('registrations', 'O'), ('compared', 'O'), ('with', 'O'), ('5522', 'O'), ('registrations', 'O'), ('in', 'O'), ('July', 'B-MISC'), ('a', 'O'), ('year', 'O'), ('earlier', 'O'), ('.', 'O')]\n",
      "Bot: [('Porsche', 'B-ORG'), (\"'s\", 'O'), ('registrations', 'O'), ('fell', 'O'), ('to', 'O'), ('554', 'O'), ('from', 'O'), ('643', 'O'), ('.', 'O')]\n",
      "Bot: [('Porsche', 'B-ORG'), (\"'s\", 'O'), ('registrations', 'O'), ('fell', 'O'), ('to', 'O'), ('554', 'O'), ('from', 'O'), ('643', 'O'), ('.', 'O')]\n",
      "Bot: [('GREEK', 'B-MISC'), ('SOCIALISTS', 'I-MISC'), ('GIVE', 'O'), ('GREEN', 'O'), ('LIGHT', 'O'), ('TO', 'O'), ('PM', 'B-ORG'), ('FOR', 'O'), ('ELECTIONS', 'O'), ('.', 'O')]\n",
      "Bot: [('GREEK', 'B-MISC'), ('SOCIALISTS', 'I-MISC'), ('GIVE', 'O'), ('GREEN', 'O'), ('LIGHT', 'O'), ('TO', 'O'), ('PM', 'B-ORG'), ('FOR', 'O'), ('ELECTIONS', 'O'), ('.', 'O')]\n",
      "Bot: [('ATHENS', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('ATHENS', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('The', 'O'), ('Greek', 'B-MISC'), ('socialist', 'I-MISC'), ('party', 'I-MISC'), (\"'s\", 'O'), ('executive', 'O'), ('bureau', 'O'), ('gave', 'O'), ('the', 'O'), ('green', 'O'), ('light', 'O'), ('to', 'O'), ('Prime', 'B-ORG'), ('Minister', 'I-ORG'), ('Costas', 'B-PER'), ('Simitis', 'I-PER'), ('to', 'O'), ('call', 'O'), ('snap', 'O'), ('elections', 'O'), (',', 'O'), ('its', 'O'), ('general', 'O'), ('secretary', 'O'), ('Costas', 'B-PER'), ('Skandalidis', 'I-PER'), ('told', 'O'), ('reporters', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('Greek', 'B-MISC'), ('socialist', 'I-MISC'), ('party', 'I-MISC'), (\"'s\", 'O'), ('executive', 'O'), ('bureau', 'O'), ('gave', 'O'), ('the', 'O'), ('green', 'O'), ('light', 'O'), ('to', 'O'), ('Prime', 'B-ORG'), ('Minister', 'I-ORG'), ('Costas', 'B-PER'), ('Simitis', 'I-PER'), ('to', 'O'), ('call', 'O'), ('snap', 'O'), ('elections', 'O'), (',', 'O'), ('its', 'O'), ('general', 'O'), ('secretary', 'O'), ('Costas', 'B-PER'), ('Skandalidis', 'I-PER'), ('told', 'O'), ('reporters', 'O'), ('.', 'O')]\n",
      "Bot: [('Prime', 'B-MISC'), ('Minister', 'I-MISC'), ('Costas', 'B-PER'), ('Simitis', 'I-PER'), ('is', 'O'), ('going', 'O'), ('to', 'O'), ('make', 'O'), ('an', 'O'), ('official', 'O'), ('announcement', 'O'), ('after', 'O'), ('a', 'O'), ('cabinet', 'O'), ('meeting', 'O'), ('later', 'O'), ('on', 'O'), ('Thursday', 'B-MISC'), (',', 'O'), ('said', 'O'), ('Skandalidis', 'B-PER'), ('.', 'O')]\n",
      "Bot: [('Prime', 'B-ORG'), ('Minister', 'I-ORG'), ('Costas', 'B-PER'), ('Simitis', 'I-PER'), ('is', 'O'), ('going', 'O'), ('to', 'O'), ('make', 'O'), ('an', 'O'), ('official', 'O'), ('announcement', 'O'), ('after', 'O'), ('a', 'O'), ('cabinet', 'O'), ('meeting', 'O'), ('later', 'O'), ('on', 'O'), ('Thursday', 'O'), (',', 'O'), ('said', 'O'), ('Skandalidis', 'B-PER'), ('.', 'O')]\n",
      "Bot: [('--', 'O'), ('Dimitris', 'B-PER'), ('Kontogiannis', 'I-PER'), (',', 'O'), ('Athens', 'B-LOC'), ('Newsroom', 'O'), ('+301', 'O'), ('3311812-4', 'O')]\n",
      "Bot: [('--', 'O'), ('Dimitris', 'B-PER'), ('Kontogiannis', 'I-PER'), (',', 'O'), ('Athens', 'B-LOC'), ('Newsroom', 'O'), ('+301', 'O'), ('3311812-4', 'O')]\n",
      "Bot: [('BayerVB', 'B-ORG'), ('sets', 'O'), ('C$', 'O'), ('100', 'O'), ('million', 'O'), ('six-year', 'O'), ('bond', 'O'), ('.', 'O')]\n",
      "Bot: [('BayerVB', 'B-ORG'), ('sets', 'O'), ('C$', 'O'), ('100', 'O'), ('million', 'O'), ('six-year', 'O'), ('bond', 'O'), ('.', 'O')]\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('The', 'O'), ('following', 'O'), ('bond', 'O'), ('was', 'O'), ('announced', 'O'), ('by', 'O'), ('lead', 'O'), ('manager', 'O'), ('Toronto', 'B-ORG'), ('Dominion', 'I-ORG'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('following', 'O'), ('bond', 'O'), ('was', 'O'), ('announced', 'O'), ('by', 'O'), ('lead', 'O'), ('manager', 'O'), ('Toronto', 'B-ORG'), ('Dominion', 'I-ORG'), ('.', 'O')]\n",
      "Bot: [('BORROWER', 'B-ORG'), ('BAYERISCHE', 'I-ORG'), ('VEREINSBANK', 'I-ORG')]\n",
      "Bot: [('BORROWER', 'B-MISC'), ('BAYERISCHE', 'B-ORG'), ('VEREINSBANK', 'I-ORG')]\n",
      "Bot: [('AMT', 'B-ORG'), ('C$', 'I-ORG'), ('100', 'I-ORG'), ('MLN', 'I-ORG'), ('COUPON', 'I-ORG'), ('6.625', 'I-ORG'), ('MATURITY', 'I-ORG'), ('24.SEP.02', 'I-ORG')]\n",
      "Bot: [('AMT', 'B-MISC'), ('C$', 'I-MISC'), ('100', 'I-MISC'), ('MLN', 'I-MISC'), ('COUPON', 'I-MISC'), ('6.625', 'I-MISC'), ('MATURITY', 'I-MISC'), ('24.SEP.02', 'I-MISC')]\n",
      "Bot: [('TYPE', 'O'), ('STRAIGHT', 'O'), ('ISS', 'B-ORG'), ('PRICE', 'O'), ('100.92', 'O'), ('PAY', 'O'), ('DATE', 'O'), ('24.SEP.96', 'O')]\n",
      "Bot: [('TYPE', 'O'), ('STRAIGHT', 'O'), ('ISS', 'B-ORG'), ('PRICE', 'O'), ('100.92', 'O'), ('PAY', 'O'), ('DATE', 'O'), ('24.SEP.96', 'O')]\n",
      "Bot: [('FULL', 'O'), ('FEES', 'O'), ('1.875', 'O'), ('REOFFER', 'O'), ('99.32', 'O'), ('SPREAD', 'O'), ('+20', 'O'), ('BP', 'O')]\n",
      "Bot: [('FULL', 'O'), ('FEES', 'O'), ('1.875', 'O'), ('REOFFER', 'O'), ('99.32', 'O'), ('SPREAD', 'O'), ('+20', 'O'), ('BP', 'O')]\n",
      "Bot: [('MOODY', 'B-ORG'), ('AA1', 'I-ORG'), ('LISTING', 'I-ORG'), ('LUX', 'I-ORG'), ('PAY', 'I-ORG'), ('FREQ', 'I-ORG'), ('=', 'O')]\n",
      "Bot: [('MOODY', 'B-ORG'), ('AA1', 'I-ORG'), ('LISTING', 'I-ORG'), ('LUX', 'I-ORG'), ('PAY', 'I-ORG'), ('FREQ', 'I-ORG'), ('=', 'O')]\n",
      "Bot: [('S&P', 'B-ORG'), ('=', 'O'), ('DENOMS', 'B-MISC'), ('(', 'O'), ('K', 'B-MISC'), (')', 'O'), ('1-10-100', 'B-MISC'), ('SALE', 'B-MISC'), ('LIMITS', 'I-MISC'), ('US', 'B-LOC'), ('/', 'O'), ('UK', 'B-LOC'), ('/', 'O'), ('CA', 'B-LOC')]\n",
      "Bot: [('S&P', 'B-ORG'), ('=', 'O'), ('DENOMS', 'B-MISC'), ('(', 'O'), ('K', 'B-MISC'), (')', 'O'), ('1-10-100', 'B-MISC'), ('SALE', 'B-MISC'), ('LIMITS', 'I-MISC'), ('US', 'B-LOC'), ('/', 'O'), ('UK', 'B-LOC'), ('/', 'O'), ('CA', 'B-LOC')]\n",
      "Bot: [('NEG', 'O'), ('PLG', 'O'), ('NO', 'O'), ('CRS', 'O'), ('DEFLT', 'O'), ('NO', 'O'), ('FORCE', 'O'), ('MAJ', 'O'), ('=', 'O')]\n",
      "Bot: [('NEG', 'O'), ('PLG', 'O'), ('NO', 'O'), ('CRS', 'O'), ('DEFLT', 'O'), ('NO', 'O'), ('FORCE', 'O'), ('MAJ', 'O'), ('=', 'O')]\n",
      "Bot: [('GOV', 'B-ORG'), ('LAW', 'I-ORG'), ('GERMAN', 'B-MISC'), ('HOME', 'B-MISC'), ('CTRY', 'I-MISC'), ('=', 'O'), ('TAX', 'B-MISC'), ('PROVS', 'I-MISC'), ('STANDARD', 'I-MISC')]\n",
      "Bot: [('GOV', 'B-ORG'), ('LAW', 'I-ORG'), ('GERMAN', 'B-MISC'), ('HOME', 'O'), ('CTRY', 'O'), ('=', 'O'), ('TAX', 'B-MISC'), ('PROVS', 'I-MISC'), ('STANDARD', 'I-MISC')]\n",
      "Bot: [('MGT', 'B-ORG'), ('/', 'O'), ('UND', 'B-ORG'), ('0.275', 'O'), ('SELL', 'O'), ('CONC', 'B-ORG'), ('1.60', 'O'), ('PRAECIP', 'B-MISC'), ('=', 'O')]\n",
      "Bot: [('MGT', 'B-ORG'), ('/', 'O'), ('UND', 'B-ORG'), ('0.275', 'O'), ('SELL', 'O'), ('CONC', 'B-ORG'), ('1.60', 'O'), ('PRAECIP', 'B-MISC'), ('=', 'O')]\n",
      "Bot: [('UNDERLYING', 'O'), ('GOVT', 'B-ORG'), ('BOND', 'I-ORG'), ('7.0', 'O'), ('PCT', 'O'), ('SEPT', 'O'), ('2001', 'O')]\n",
      "Bot: [('UNDERLYING', 'O'), ('GOVT', 'B-ORG'), ('BOND', 'I-ORG'), ('7.0', 'O'), ('PCT', 'O'), ('SEPT', 'O'), ('2001', 'O')]\n",
      "Bot: [('NOTES', 'O'), ('BAYERISCHE', 'B-ORG'), ('VEREINSBANK', 'I-ORG'), ('IS', 'O'), ('JOINT', 'O'), ('LEAD', 'O'), ('MANAGER', 'O')]\n",
      "Bot: [('NOTES', 'O'), ('BAYERISCHE', 'B-ORG'), ('VEREINSBANK', 'I-ORG'), ('IS', 'O'), ('JOINT', 'O'), ('LEAD', 'O'), ('MANAGER', 'O')]\n",
      "Bot: [('--', 'O'), ('London', 'B-LOC'), ('Newsroom', 'B-ORG'), ('+44', 'O'), ('171', 'O'), ('542', 'O'), ('7658', 'O')]\n",
      "Bot: [('--', 'O'), ('London', 'B-LOC'), ('Newsroom', 'B-ORG'), ('+44', 'O'), ('171', 'O'), ('542', 'O'), ('7658', 'O')]\n",
      "Bot: [('Venantius', 'B-PER'), ('sets', 'O'), ('$', 'O'), ('300', 'O'), ('million', 'O'), ('January', 'B-MISC'), ('1999', 'I-MISC'), ('FRN', 'B-ORG'), ('.', 'O')]\n",
      "Bot: [('Venantius', 'B-PER'), ('sets', 'O'), ('$', 'O'), ('300', 'O'), ('million', 'O'), ('January', 'B-MISC'), ('1999', 'I-MISC'), ('FRN', 'B-ORG'), ('.', 'O')]\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('LONDON', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('The', 'O'), ('following', 'O'), ('floating-rate', 'O'), ('issue', 'O'), ('was', 'O'), ('announced', 'O'), ('by', 'O'), ('lead', 'O'), ('manager', 'O'), ('Lehman', 'B-ORG'), ('Brothers', 'I-ORG'), ('International', 'I-ORG'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('following', 'O'), ('floating-rate', 'O'), ('issue', 'O'), ('was', 'O'), ('announced', 'O'), ('by', 'O'), ('lead', 'O'), ('manager', 'O'), ('Lehman', 'B-ORG'), ('Brothers', 'I-ORG'), ('International', 'I-ORG'), ('.', 'O')]\n",
      "Bot: [('BORROWER', 'O'), ('VENANTIUS', 'B-PER'), ('AB', 'I-PER'), ('(', 'O'), ('SWEDISH', 'B-ORG'), ('NATIONAL', 'I-ORG'), ('MORTGAGE', 'I-ORG'), ('AGENCY', 'I-ORG'), (')', 'O')]\n",
      "Bot: [('BORROWER', 'O'), ('VENANTIUS', 'B-PER'), ('AB', 'I-PER'), ('(', 'O'), ('SWEDISH', 'B-ORG'), ('NATIONAL', 'I-ORG'), ('MORTGAGE', 'I-ORG'), ('AGENCY', 'I-ORG'), (')', 'O')]\n",
      "Bot: [('AMT', 'O'), ('$', 'O'), ('300', 'O'), ('MLN', 'O'), ('SPREAD', 'O'), ('-', 'O'), ('12.5', 'O'), ('BP', 'O'), ('MATURITY', 'O'), ('21.JAN.99', 'O')]\n",
      "Bot: [('AMT', 'O'), ('$', 'O'), ('300', 'O'), ('MLN', 'O'), ('SPREAD', 'O'), ('-', 'O'), ('12.5', 'O'), ('BP', 'O'), ('MATURITY', 'O'), ('21.JAN.99', 'O')]\n",
      "Bot: [('TYPE', 'O'), ('FRN', 'O'), ('BASE', 'O'), ('3M', 'O'), ('LIBOR', 'O'), ('PAY', 'O'), ('DATE', 'O'), ('S23.SEP.96', 'O')]\n",
      "Bot: [('TYPE', 'O'), ('FRN', 'O'), ('BASE', 'O'), ('3M', 'O'), ('LIBOR', 'O'), ('PAY', 'O'), ('DATE', 'O'), ('S23.SEP.96', 'O')]\n",
      "Bot: [('LAST', 'O'), ('MOODY', 'B-ORG'), ('AA3', 'I-ORG'), ('ISS', 'I-ORG'), ('PRICE', 'O'), ('99.956', 'O'), ('FULL', 'O'), ('FEES', 'O'), ('10', 'O'), ('BP', 'O')]\n",
      "Bot: [('LAST', 'O'), ('MOODY', 'B-ORG'), ('AA3', 'I-ORG'), ('ISS', 'I-ORG'), ('PRICE', 'O'), ('99.956', 'O'), ('FULL', 'O'), ('FEES', 'O'), ('10', 'O'), ('BP', 'O')]\n",
      "Bot: [('LAST', 'O'), ('S&P', 'B-ORG'), ('AA+', 'O'), ('REOFFER', 'O'), ('=', 'O')]\n",
      "Bot: [('LAST', 'O'), ('S&P', 'B-ORG'), ('AA+', 'O'), ('REOFFER', 'O'), ('=', 'O')]\n",
      "Bot: [('NOTES', 'B-MISC'), ('S', 'I-MISC'), ('SHORT', 'I-MISC'), ('FIRST', 'I-MISC'), ('COUPON', 'I-MISC')]\n",
      "Bot: [('NOTES', 'O'), ('S', 'O'), ('SHORT', 'O'), ('FIRST', 'O'), ('COUPON', 'O')]\n",
      "Bot: [('LISTING', 'O'), ('LONDON', 'B-LOC'), ('DENOMS', 'O'), ('(', 'O'), ('K', 'O'), (')', 'O'), ('1-10-100', 'O'), ('SALE', 'O'), ('LIMITS', 'O'), ('US', 'B-LOC'), ('/', 'O'), ('UK', 'B-LOC'), ('/', 'O'), ('JP', 'B-LOC'), ('/', 'O'), ('FR', 'B-LOC')]\n",
      "Bot: [('LISTING', 'O'), ('LONDON', 'B-LOC'), ('DENOMS', 'O'), ('(', 'O'), ('K', 'O'), (')', 'O'), ('1-10-100', 'O'), ('SALE', 'O'), ('LIMITS', 'O'), ('US', 'B-LOC'), ('/', 'O'), ('UK', 'B-LOC'), ('/', 'O'), ('JP', 'B-LOC'), ('/', 'O'), ('FR', 'B-LOC')]\n",
      "Bot: [('NEG', 'O'), ('PLG', 'O'), ('YES', 'O'), ('CRS', 'O'), ('DEFLT', 'O'), ('NO', 'O'), ('FORCE', 'O'), ('MAJ', 'O'), ('IPMA', 'O'), ('2', 'O')]\n",
      "Bot: [('NEG', 'O'), ('PLG', 'O'), ('YES', 'O'), ('CRS', 'O'), ('DEFLT', 'O'), ('NO', 'O'), ('FORCE', 'O'), ('MAJ', 'O'), ('IPMA', 'O'), ('2', 'O')]\n",
      "Bot: [('GOV', 'B-ORG'), ('LAW', 'I-ORG'), ('ENGLISH', 'I-ORG'), ('HOME', 'I-ORG'), ('CTRY', 'I-ORG'), ('SWEDEN', 'B-LOC'), ('TAX', 'B-MISC'), ('PROVS', 'I-MISC'), ('STANDARD', 'I-MISC')]\n",
      "Bot: [('GOV', 'B-ORG'), ('LAW', 'I-ORG'), ('ENGLISH', 'I-ORG'), ('HOME', 'I-ORG'), ('CTRY', 'I-ORG'), ('SWEDEN', 'B-LOC'), ('TAX', 'B-MISC'), ('PROVS', 'I-MISC'), ('STANDARD', 'I-MISC')]\n",
      "Bot: [('MGT', 'B-ORG'), ('/', 'O'), ('UND', 'B-ORG'), ('5', 'O'), ('BP', 'B-ORG'), ('SELL', 'O'), ('CONC', 'O'), ('5', 'O'), ('BP', 'B-ORG'), ('PRAECIP', 'O'), ('=', 'O')]\n",
      "Bot: [('MGT', 'B-ORG'), ('/', 'O'), ('UND', 'B-ORG'), ('5', 'O'), ('BP', 'B-ORG'), ('SELL', 'O'), ('CONC', 'O'), ('5', 'O'), ('BP', 'B-ORG'), ('PRAECIP', 'O'), ('=', 'O')]\n",
      "Bot: [('NOTES', 'B-MISC'), ('ISSUED', 'O'), ('OFF', 'O'), ('EMTN', 'B-ORG'), ('PROGRAMME', 'I-ORG')]\n",
      "Bot: [('NOTES', 'B-MISC'), ('ISSUED', 'O'), ('OFF', 'O'), ('EMTN', 'B-ORG'), ('PROGRAMME', 'I-ORG')]\n",
      "Bot: [('--', 'O'), ('London', 'B-LOC'), ('Newsroom', 'B-ORG'), ('+44', 'O'), ('171', 'O'), ('542', 'O'), ('8863', 'O')]\n",
      "Bot: [('--', 'O'), ('London', 'B-LOC'), ('Newsroom', 'B-ORG'), ('+44', 'O'), ('171', 'O'), ('542', 'O'), ('8863', 'O')]\n",
      "Bot: [('Port', 'B-LOC'), ('conditions', 'O'), ('update', 'O'), ('-', 'O'), ('Syria', 'B-LOC'), ('-', 'O'), ('Lloyds', 'B-ORG'), ('Shipping', 'I-ORG'), ('.', 'O')]\n",
      "Bot: [('Port', 'O'), ('conditions', 'O'), ('update', 'O'), ('-', 'O'), ('Syria', 'B-LOC'), ('-', 'O'), ('Lloyds', 'B-ORG'), ('Shipping', 'I-ORG'), ('.', 'O')]\n",
      "Bot: [('Port', 'B-LOC'), ('conditions', 'O'), ('from', 'O'), ('Lloyds', 'B-ORG'), ('Shipping', 'I-ORG'), ('Intelligence', 'I-ORG'), ('Service', 'I-ORG'), ('--', 'O')]\n",
      "Bot: [('Port', 'B-LOC'), ('conditions', 'O'), ('from', 'O'), ('Lloyds', 'B-ORG'), ('Shipping', 'I-ORG'), ('Intelligence', 'I-ORG'), ('Service', 'I-ORG'), ('--', 'O')]\n",
      "Bot: [('LATTAKIA', 'B-LOC'), (',', 'O'), ('Aug', 'O'), ('10', 'O'), ('-', 'O'), ('waiting', 'O'), ('time', 'O'), ('at', 'O'), ('Lattakia', 'B-LOC'), ('and', 'O'), ('Tartous', 'B-LOC'), ('presently', 'O'), ('24', 'O'), ('hours', 'O'), ('.', 'O')]\n",
      "Bot: [('LATTAKIA', 'B-LOC'), (',', 'O'), ('Aug', 'O'), ('10', 'O'), ('-', 'O'), ('waiting', 'O'), ('time', 'O'), ('at', 'O'), ('Lattakia', 'B-LOC'), ('and', 'O'), ('Tartous', 'B-LOC'), ('presently', 'O'), ('24', 'O'), ('hours', 'O'), ('.', 'O')]\n",
      "Bot: [('Israel', 'B-LOC'), ('plays', 'O'), ('down', 'O'), ('fears', 'O'), ('of', 'O'), ('war', 'O'), ('with', 'O'), ('Syria', 'B-LOC'), ('.', 'O')]\n",
      "Bot: [('Israel', 'B-LOC'), ('plays', 'O'), ('down', 'O'), ('fears', 'O'), ('of', 'O'), ('war', 'O'), ('with', 'O'), ('Syria', 'B-LOC'), ('.', 'O')]\n",
      "Bot: [('Colleen', 'B-PER'), ('Siegel', 'I-PER')]\n",
      "Bot: [('Colleen', 'B-PER'), ('Siegel', 'I-PER')]\n",
      "Bot: [('JERUSALEM', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('JERUSALEM', 'B-LOC'), ('1996-08-22', 'O')]\n",
      "Bot: [('Israel', 'B-LOC'), (\"'s\", 'O'), ('outgoing', 'O'), ('peace', 'O'), ('negotiator', 'O'), ('with', 'O'), ('Syria', 'B-LOC'), ('said', 'O'), ('on', 'O'), ('Thursday', 'B-MISC'), ('current', 'O'), ('tensions', 'O'), ('between', 'O'), ('the', 'O'), ('two', 'O'), ('countries', 'O'), ('appeared', 'O'), ('to', 'O'), ('be', 'O'), ('a', 'O'), ('storm', 'O'), ('in', 'O'), ('a', 'O'), ('teacup', 'O'), ('.', 'O')]\n",
      "Bot: [('Israel', 'B-LOC'), (\"'s\", 'O'), ('outgoing', 'O'), ('peace', 'O'), ('negotiator', 'O'), ('with', 'O'), ('Syria', 'B-LOC'), ('said', 'O'), ('on', 'O'), ('Thursday', 'B-MISC'), ('current', 'O'), ('tensions', 'O'), ('between', 'O'), ('the', 'O'), ('two', 'O'), ('countries', 'O'), ('appeared', 'O'), ('to', 'O'), ('be', 'O'), ('a', 'O'), ('storm', 'O'), ('in', 'O'), ('a', 'O'), ('teacup', 'O'), ('.', 'O')]\n",
      "Bot: [('Itamar', 'B-PER'), ('Rabinovich', 'I-PER'), (',', 'O'), ('who', 'O'), ('as', 'O'), ('Israel', 'B-LOC'), (\"'s\", 'O'), ('ambassador', 'O'), ('to', 'O'), ('Washington', 'B-LOC'), ('conducted', 'O'), ('unfruitful', 'O'), ('negotiations', 'O'), ('with', 'O'), ('Syria', 'B-LOC'), (',', 'O'), ('told', 'O'), ('Israel', 'B-LOC'), ('Radio', 'B-ORG'), ('it', 'O'), ('looked', 'O'), ('like', 'O'), ('Damascus', 'B-LOC'), ('wanted', 'O'), ('to', 'O'), ('talk', 'O'), ('rather', 'O'), ('than', 'O'), ('fight', 'O'), ('.', 'O')]\n",
      "Bot: [('Itamar', 'B-PER'), ('Rabinovich', 'I-PER'), (',', 'O'), ('who', 'O'), ('as', 'O'), ('Israel', 'B-LOC'), (\"'s\", 'O'), ('ambassador', 'O'), ('to', 'O'), ('Washington', 'B-LOC'), ('conducted', 'O'), ('unfruitful', 'O'), ('negotiations', 'O'), ('with', 'O'), ('Syria', 'B-LOC'), (',', 'O'), ('told', 'O'), ('Israel', 'B-LOC'), ('Radio', 'B-ORG'), ('it', 'O'), ('looked', 'O'), ('like', 'O'), ('Damascus', 'B-LOC'), ('wanted', 'O'), ('to', 'O'), ('talk', 'O'), ('rather', 'O'), ('than', 'O'), ('fight', 'O'), ('.', 'O')]\n",
      "Bot: [('It', 'O'), ('appears', 'O'), ('to', 'O'), ('me', 'O'), ('the', 'O'), ('Syrian', 'B-MISC'), ('priority', 'O'), ('is', 'O'), ('still', 'O'), ('to', 'O'), ('negotiate', 'O'), ('.', 'O')]\n",
      "Bot: [('It', 'O'), ('appears', 'O'), ('to', 'O'), ('me', 'O'), ('the', 'O'), ('Syrian', 'B-MISC'), ('priority', 'O'), ('is', 'O'), ('still', 'O'), ('to', 'O'), ('negotiate', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('Syrians', 'B-MISC'), ('are', 'O'), ('confused', 'O'), (',', 'O'), ('they', 'O'), ('are', 'O'), ('definitely', 'O'), ('tense', 'O'), (',', 'O'), ('but', 'O'), ('the', 'O'), ('general', 'O'), ('assessment', 'O'), ('here', 'O'), ('in', 'O'), ('Washington', 'B-LOC'), ('is', 'O'), ('that', 'O'), ('this', 'O'), ('is', 'O'), ('essentially', 'O'), ('a', 'O'), ('storm', 'O'), ('in', 'O'), ('a', 'O'), ('teacup', 'O'), (',', 'O'), ('\"', 'O'), ('he', 'O'), ('said', 'O'), ('.', 'O')]\n",
      "Bot: [('The', 'O'), ('Syrians', 'B-MISC'), ('are', 'O'), ('confused', 'O'), (',', 'O'), ('they', 'O'), ('are', 'O'), ('definitely', 'O'), ('tense', 'O'), (',', 'O'), ('but', 'O'), ('the', 'O'), ('general', 'O'), ('assessment', 'O'), ('here', 'O'), ('in', 'O'), ('Washington', 'B-LOC'), ('is', 'O'), ('that', 'O'), ('this', 'O'), ('is', 'O'), ('essentially', 'O'), ('a', 'O'), ('storm', 'O'), ('in', 'O'), ('a', 'O'), ('teacup', 'O'), (',', 'O'), ('\"', 'O'), ('he', 'O'), ('said', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "for sentence in dataset['train']['tokens'][:100]:\n",
    "    prompt = f\"Given the entity label set: {list(ner_dict.keys())}.\\n \\\n",
    "        Based on the given entity label set, please recognize the named entities in the given text.\\n \\\n",
    "        Return only a list of tuples with each token and its label without explenation. Your output must be in the format: [('In','O'), ('America','I-LOC'), ('is','O'), ('cold','O'), ...]; nothing else\\n \\\n",
    "        Text: {\" \".join(sentence)}\"\n",
    "\n",
    "    send_prompt_conversation(prompt)\n",
    "\n",
    "    prompt_2 = f\"Please carefully review your previous answer and correct any possible mistakes. \\\n",
    "    Keep the same output structure: a list of tuples in this format: \\\n",
    "    [('\\\"','O'),('In','O'), ('America','I-LOC'), ('is','O'), ('cold','O'), ...] \\\n",
    "    Do not return any explanation or additional text.\\n.\"   \n",
    "    send_prompt_conversation(prompt_2)\n",
    "\n",
    "    with open(\"self_validating.txt\", \"a\") as file:\n",
    "        file.write(f\"{messages[3][\"content\"]}\\n\")\n",
    "    \n",
    "    messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa9b71-7193-477c-8e47-4ca2f43f1217",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e243af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of results\n",
    "import ast\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def parse_prediction(pred_str, ner_dict):\n",
    "    pred_list = ast.literal_eval(pred_str)\n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "    for token, label in pred_list:\n",
    "        tokens.append(token)\n",
    "        label_ids.append(ner_dict.get(label, 0))\n",
    "    return tokens, label_ids\n",
    "        \n",
    "def f1Score(file_path, dataset, dict=ner_dict):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score for the predictions in the file.\n",
    "    \"\"\"\n",
    "\n",
    "    label_list = list(set(dict.values()))\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    predictions = re.findall(r'(\\[.*?\\])', content, re.DOTALL)\n",
    "    # predictions = []\n",
    "    # for pred in predictions_with_duplicates:\n",
    "    #     if pred not in predictions:\n",
    "    #         predictions.append(pred)\n",
    "    y_pred = []\n",
    "    y_true = dataset['train']['ner_tags'][:len(predictions)]\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "\n",
    "        pred_tokens, pred_labels = parse_prediction(pred.strip(), ner_dict)\n",
    "        # print(f\"Pred tokens: {pred_tokens}\")\n",
    "        # print(f\"Pred labels: {pred_labels}\")\n",
    "        true_tokens = dataset['train']['tokens'][i]\n",
    "        true_labels = dataset['train']['ner_tags'][i]\n",
    "        # print(f\"True tokens: {true_tokens}\")\n",
    "        # print(f\"True labels: {true_labels}\")\n",
    "\n",
    "        aligned_preds = []\n",
    "        pred_idx = 0\n",
    "\n",
    "        if len(pred_labels) != len(true_labels):\n",
    "            for true_token in true_tokens:\n",
    "                if pred_idx < len(pred_tokens) and pred_tokens[pred_idx] == true_token:\n",
    "                    aligned_preds.append(pred_labels[pred_idx])\n",
    "                    pred_idx += 1\n",
    "                else:\n",
    "                    aligned_preds.append(-1)\n",
    "        else:\n",
    "            aligned_preds = pred_labels[:]\n",
    "    \n",
    "        y_pred.append(aligned_preds)\n",
    "\n",
    "\n",
    "    flat_true = [label for seq in y_true for label in seq]\n",
    "    flat_pred = [label for seq in y_pred for label in seq]\n",
    "    print(f\"Flat true: {flat_true}\")\n",
    "    print(f\"Flat pred: {flat_pred}\")\n",
    "    \n",
    "    precision = precision_score(flat_true, flat_pred, labels=[0,1,2,3,4,5,6,7,8], average='micro', zero_division=0)\n",
    "    recall = recall_score(flat_true, flat_pred, labels=[0,1,2,3,4,5,6,7,8], average='micro', zero_division=0)\n",
    "    f1 = f1_score(flat_true, flat_pred, labels=[0,1,2,3,4,5,6,7,8], average='micro', zero_division=0)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    return f1\n",
    "\n",
    "def f1Score_dollar(file_path, dataset, dict=ner_dict):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score for the predictions in the file,\n",
    "    extracting only the string between two '$' symbols.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_list = list(set(dict.values()))\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    predictions = re.findall(r'\\$(\\[.*?\\])\\$', content, re.DOTALL)\n",
    "    # predictions = []\n",
    "    # for pred in predictions_with_duplicates:\n",
    "    #     if pred not in predictions:\n",
    "    #         predictions.append(pred)\n",
    "    y_pred = []\n",
    "    y_true = dataset['train']['ner_tags'][:len(predictions)]\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "\n",
    "        pred_tokens, pred_labels = parse_prediction(pred.strip(), ner_dict)\n",
    "        # print(f\"Pred tokens: {pred_tokens}\")\n",
    "        # print(f\"Pred labels: {pred_labels}\")\n",
    "        true_tokens = dataset['train']['tokens'][i]\n",
    "        true_labels = dataset['train']['ner_tags'][i]\n",
    "        # print(f\"True tokens: {true_tokens}\")\n",
    "        # print(f\"True labels: {true_labels}\")\n",
    "\n",
    "        aligned_preds = []\n",
    "        pred_idx = 0\n",
    "\n",
    "        if len(pred_labels) != len(true_labels):\n",
    "            for true_token in true_tokens:\n",
    "                if pred_idx < len(pred_tokens) and pred_tokens[pred_idx] == true_token:\n",
    "                    aligned_preds.append(pred_labels[pred_idx])\n",
    "                    pred_idx += 1\n",
    "                else:\n",
    "                    aligned_preds.append(-1)\n",
    "        else:\n",
    "            aligned_preds = pred_labels[:]\n",
    "    \n",
    "        y_pred.append(aligned_preds)\n",
    "\n",
    "\n",
    "    flat_true = [label for seq in y_true for label in seq]\n",
    "    flat_pred = [label for seq in y_pred for label in seq]\n",
    "    print(f\"Flat true: {flat_true}\")\n",
    "    print(f\"Flat pred: {flat_pred}\")\n",
    "    \n",
    "    precision = precision_score(flat_true, flat_pred, labels=[0,1,2,3,4,5,6,7,8], average='micro', zero_division=0)\n",
    "    recall = recall_score(flat_true, flat_pred, labels=[0,1,2,3,4,5,6,7,8], average='micro', zero_division=0)\n",
    "    f1 = f1_score(flat_true, flat_pred, labels=[0,1,2,3,4,5,6,7,8], average='micro', zero_division=0)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a05d8b-f887-4d1e-bb83-af96d171f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat true: [3, 0, 7, 0, 0, 0, 7, 0, 0, 1, 2, 5, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 1, 2, 2, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 3, 0, 0, 1, 2, 2, 0, 0, 3, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 7, 0, 0, 1, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 0, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 7, 0, 0, 1, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 3, 4, 0, 0, 3, 0, 7, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 4, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 3, 4, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 0, 0, 0, 0, 3, 4, 4, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 1, 2, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 3, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Flat pred: [3, 0, 7, 0, 0, 0, 7, 0, 0, 1, 2, 5, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 1, 2, 2, 0, 0, 0, 1, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 5, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 7, 3, 4, 4, 4, 0, 3, 0, 0, 1, 2, 2, 0, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 7, 8, 8, 1, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 0, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 3, 4, 0, 5, 0, 0, 0, 0, 5, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 3, 4, 4, 0, 1, 0, 0, 0, 0, 3, 4, 0, 1, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 7, 7, 8, 8, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 3, 4, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 5, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 3, 4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 0, 3, 0, 7, 0, 7, 0, 7, 7, 7, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 7, 7, 8, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, -1, 5, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 1, 2, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 7, 8, 8, 8, 8, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 7, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 3, 4, 0, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 0, 5, 0, 0, 3, 4, 4, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 1, 2, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 3, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.9267\n",
      "Recall:    0.9233\n",
      "F1 Score:  0.9250\n",
      "Flat true: [3, 0, 7, 0, 0, 0, 7, 0, 0, 1, 2, 5, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 1, 2, 2, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 3, 0, 0, 1, 2, 2, 0, 0, 3, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 7, 0, 0, 1, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 0, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 7, 0, 0, 1, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 3, 4, 0, 0, 3, 0, 7, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 4, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 3, 4, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 0, 0, 0, 0, 3, 4, 4, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 1, 2, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 3, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Flat pred: [3, 0, 7, 0, 0, 0, 7, 0, 0, 1, 2, 5, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 1, 2, 2, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 5, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 7, 3, 4, 0, 4, 0, 3, 0, 0, 1, 2, 2, 0, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 7, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 7, 8, 8, 1, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 0, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 3, 4, 0, 5, 0, 0, 0, 0, 5, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 3, 4, 4, 0, 1, 0, 0, 0, 0, 3, 4, 0, 1, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 7, 8, 8, 1, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 3, 0, 0, 0, 5, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 3, 4, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 5, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 7, 3, 4, 7, 8, 8, 8, 8, 8, 8, 8, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 0, 3, 0, 7, 0, 7, 0, 7, 7, 8, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 7, 0, 0, 0, 7, 8, 8, 3, 0, 3, 0, 0, 3, 0, 7, 0, 0, 3, 4, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 5, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 7, 8, 3, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 1, 2, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 5, 7, 8, 8, 3, 0, 3, 0, 3, 0, 0, 0, 3, 0, 0, 7, 0, 0, 3, 4, 0, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 0, 5, 0, 0, 3, 4, 4, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 1, 2, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 3, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.9255\n",
      "Recall:    0.9227\n",
      "F1 Score:  0.9241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9241003931055337"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1Score(\"role_play.txt\", dataset, ner_dict)\n",
    "f1Score(\"vanilla_v2.txt\", dataset, ner_dict)\n",
    "# f1Score_dollar(\"COT.txt\", dataset, ner_dict)\n",
    "# f1Score_dollar(\"role_playand_COT.txt\", dataset, ner_dict)\n",
    "#f1Score(\"divided_inputs.txt\", dataset, ner_dict)\n",
    "f1Score(\"self_validating.txt\", dataset, ner_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60d811-bc42-4796-a697-95a219c544f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
